{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8fecdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect01.mp4 to s3://jaybucket-demo1/videos/Mod02_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Intro.mp4 to s3://jaybucket-demo1/videos/Mod02_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect02.mp4 to s3://jaybucket-demo1/videos/Mod02_Sect02.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_WrapUp.mp4 to s3://jaybucket-demo1/videos/Mod02_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Intro.mp4 to s3://jaybucket-demo1/videos/Mod03_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod01_Course Overview.mp4 to s3://jaybucket-demo1/videos/Mod01_Course Overview.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect03.mp4 to s3://jaybucket-demo1/videos/Mod02_Sect03.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part2.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect02_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect05.mp4 to s3://jaybucket-demo1/videos/Mod02_Sect05.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part1.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect03_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part1.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect02_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part3.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect03_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect03_part2.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect03_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod02_Sect04.mp4 to s3://jaybucket-demo1/videos/Mod02_Sect04.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect01.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part2.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect04_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part3.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect04_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect02_part3.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect02_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part1.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect07_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect04_part1.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect04_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect05.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect05.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect06.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect06.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part2.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect07_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect07_part3.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect07_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_Sect08.mp4 to s3://jaybucket-demo1/videos/Mod03_Sect08.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod03_WrapUp.mp4 to s3://jaybucket-demo1/videos/Mod03_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Intro.mp4 to s3://jaybucket-demo1/videos/Mod04_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect01.mp4 to s3://jaybucket-demo1/videos/Mod04_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part1.mp4 to s3://jaybucket-demo1/videos/Mod04_Sect02_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part2.mp4 to s3://jaybucket-demo1/videos/Mod04_Sect02_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_WrapUp.mp4 to s3://jaybucket-demo1/videos/Mod04_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part2.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect02_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod04_Sect02_part3.mp4 to s3://jaybucket-demo1/videos/Mod04_Sect02_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Intro.mp4 to s3://jaybucket-demo1/videos/Mod05_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect02_part1_ver2.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect02_part1_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part1.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect03_part1.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_WrapUp_ver2.mp4 to s3://jaybucket-demo1/videos/Mod05_WrapUp_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_Intro.mp4 to s3://jaybucket-demo1/videos/Mod06_Intro.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect01_ver2.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect01_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part3.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect03_part3.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part4_ver2.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect03_part4_ver2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod05_Sect03_part2.mp4 to s3://jaybucket-demo1/videos/Mod05_Sect03_part2.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_WrapUp.mp4 to s3://jaybucket-demo1/videos/Mod06_WrapUp.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod07_Sect01.mp4 to s3://jaybucket-demo1/videos/Mod07_Sect01.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_Sect02.mp4 to s3://jaybucket-demo1/videos/Mod06_Sect02.mp4\n",
      "copy: s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/Mod06_Sect01.mp4 to s3://jaybucket-demo1/videos/Mod06_Sect01.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!aws s3 cp s3://aws-tc-largeobjects/CUR-TF-200-ACMNLP-1/video/ s3://jaybucket-demo1/videos/ --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f689a0cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Effect': 'Allow',\n",
       " 'Action': 'transcribe:StartTranscriptionJob',\n",
       " 'Resource': 'arn:aws:transcribe:<region>:<account-id>:transcription-job/*'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Action\": \"transcribe:StartTranscriptionJob\",\n",
    "    \"Resource\": \"arn:aws:transcribe:<region>:<account-id>:transcription-job/*\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a09c02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['demo1-job_name_1.1_1', 'demo1-job_name_1.1_2', 'demo1-job_name_1.1_3', 'demo1-job_name_1.1_4', 'demo1-job_name_1.1_5', 'demo1-job_name_1.1_6', 'demo1-job_name_1.1_7', 'demo1-job_name_1.1_8', 'demo1-job_name_1.1_9', 'demo1-job_name_1.1_10', 'demo1-job_name_1.1_11', 'demo1-job_name_1.1_12', 'demo1-job_name_1.1_13', 'demo1-job_name_1.1_14', 'demo1-job_name_1.1_15', 'demo1-job_name_1.1_16', 'demo1-job_name_1.1_17', 'demo1-job_name_1.1_18', 'demo1-job_name_1.1_19', 'demo1-job_name_1.1_20', 'demo1-job_name_1.1_21', 'demo1-job_name_1.1_22', 'demo1-job_name_1.1_23', 'demo1-job_name_1.1_24', 'demo1-job_name_1.1_25', 'demo1-job_name_1.1_26', 'demo1-job_name_1.1_27', 'demo1-job_name_1.1_28', 'demo1-job_name_1.1_29', 'demo1-job_name_1.1_30', 'demo1-job_name_1.1_31', 'demo1-job_name_1.1_32', 'demo1-job_name_1.1_33', 'demo1-job_name_1.1_34', 'demo1-job_name_1.1_35', 'demo1-job_name_1.1_36', 'demo1-job_name_1.1_37', 'demo1-job_name_1.1_38', 'demo1-job_name_1.1_39', 'demo1-job_name_1.1_40', 'demo1-job_name_1.1_41', 'demo1-job_name_1.1_42', 'demo1-job_name_1.1_43', 'demo1-job_name_1.1_44', 'demo1-job_name_1.1_45', 'demo1-job_name_1.1_46'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "with open('./combined_output.json') as f:\n",
    "    alldata = json.load(f)\n",
    "print(alldata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad8cbac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'and', 'welcome', 'to', 'Amazon', 'Academy', 'of', 'Machine', 'Learning', 'Foundations', 'in', 'this', 'module', ',', 'you', \"'ll\", 'learn', 'about', 'the', 'course', 'objectives', ',', 'various', 'job', 'roles', 'in', 'the', 'machine', 'learning', 'domain', 'and', 'where', 'you', 'can', 'go', 'to', 'learn', 'more', 'about', 'machine', 'learning', '.', 'After', 'completing', 'this', 'module', ',', 'you', 'should', 'be', 'able', 'to', 'identify', 'course', 'prerequisites', 'and', 'objectives', 'indicate', 'the', 'role', 'of', 'the', 'data', 'scientist', 'in', 'business', 'and', 'identify', 'resources', 'for', 'further', 'learning', '.', 'We', \"'re\", 'now', 'going', 'to', 'look', 'at', 'the', 'prerequisites', 'for', 'taking', 'this', 'course', '.', 'Before', 'you', 'take', 'this', 'course', ',', 'we', 'recommend', 'that', 'you', 'first', 'complete', 'Aws', 'Academy', 'Cloud', 'Foundations', '.', 'You', 'should', 'also', 'have', 'some', 'general', 'technical', 'knowledge', 'of', 'it', 'including', 'foundational', 'computer', 'literacy', 'skills', 'like', 'basic', 'computer', 'concepts', ',', 'email', 'file', 'management', 'and', 'a', 'good', 'understanding', 'of', 'the', 'internet', '.', 'We', 'also', 'recommend', 'that', 'you', 'have', 'intermediate', 'skills', 'with', 'Python', 'programming', 'and', 'a', 'general', 'knowledge', 'of', 'applied', 'statistics', '.', 'Finally', ',', 'general', 'business', 'knowledge', 'is', 'important', 'for', 'this', 'course', '.', 'This', 'includes', 'insight', 'into', 'how', 'information', 'technology', 'is', 'used', 'in', 'business', '.', 'It', \"'s\", 'also', 'important', 'to', 'have', 'business', 'related', 'skill', 'sets', 'such', 'as', 'communication', 'skills', ',', 'leadership', 'skills', ',', 'and', 'an', 'orientation', 'towards', 'customer', 'service', '.', 'In', 'this', 'course', ',', 'you', \"'ll\", 'be', 'introduced', 'to', 'the', 'key', 'concepts', 'of', 'machine', 'learning', ',', 'its', 'tools', 'and', 'its', 'uses', 'you', \"'ll\", 'also', 'be', 'introduced', 'to', 'and', 'work', 'with', 'some', 'of', 'the', 'AWS', 'services', 'for', 'machine', 'learning', '.', 'You', \"'ll\", 'learn', 'how', 'to', 'recognize', 'how', 'machine', 'learning', 'and', 'deep', 'learning', 'are', 'part', 'of', 'artificial', 'intelligence', '.', 'Describe', 'artificial', 'intelligence', 'and', 'machine', 'learning', 'terminology', '.', 'Identify', 'how', 'machine', 'learning', 'can', 'be', 'used', 'to', 'solve', 'a', 'business', 'problem', '.', 'Describe', 'the', 'machine', 'learning', 'process', '.', 'List', 'the', 'tools', 'available', 'to', 'data', 'scientists', 'and', 'identify', 'when', 'to', 'use', 'machine', 'learning', 'instead', 'of', 'traditional', 'software', 'development', 'methods', '.', 'As', 'part', 'of', 'this', 'course', ',', 'you', \"'ll\", 'also', 'learn', 'how', 'to', 'implement', 'a', 'machine', 'learning', 'pipeline', '.', 'This', 'includes', 'how', 'to', 'formulate', 'a', 'problem', 'from', 'a', 'business', 'request', ',', 'obtain', 'and', 'secure', 'data', 'for', 'machine', 'learning', ',', 'build', 'a', 'Jupiter', 'notebook', 'by', 'using', 'Amazon', 'Sagemaker', 'outline', 'the', 'process', 'for', 'evaluating', 'data', '.', 'Explain', 'why', 'data', 'needs', 'to', 'be', 'pre', 'processed', 'and', 'use', 'open', 'source', 'tools', 'to', 'examine', 'and', 'prep', 'process', 'data', '.', 'You', 'will', 'also', 'use', 'Amazon', \"'s\", 'Sage', 'maker', 'to', 'train', 'and', 'host', 'a', 'machine', 'learning', 'model', ',', 'use', 'cross', 'validation', 'to', 'test', 'the', 'performance', 'of', 'a', 'machine', 'learning', 'model', '.', 'Use', 'a', 'hosted', 'model', 'for', 'inference', ',', 'create', 'an', 'Amazon', 'Sage', 'maker', 'hyper', 'parameter', 'tuning', 'job', 'to', 'optimize', 'a', 'model', \"'s\", 'effectiveness', '.', 'And', 'finally', 'how', 'to', 'use', 'managed', 'Amazon', 'machine', 'learning', 'services', 'to', 'solve', 'specific', 'machine', 'learning', 'problems', 'in', 'forecasting', 'computer', 'vision', 'and', 'natural', 'language', 'processing', '.', 'We', \"'ll\", 'now', 'review', 'the', 'course', 'outline', 'to', 'achieve', 'the', 'course', 'objectives', '.', 'You', \"'ll\", 'complete', 'the', 'following', 'modules', 'to', 'start', 'in', 'module', 'two', ',', 'you', \"'ll\", 'get', 'an', 'introduction', 'to', 'machine', 'learning', 'in', 'module', 'three', '.', 'You', \"'ll\", 'learn', 'how', 'to', 'implement', 'a', 'machine', 'learning', 'pipeline', 'with', 'Amazon', 'Sagemaker', 'modules', '45', 'and', 'six', 'describe', 'how', 'to', 'apply', 'managed', 'Amazon', 'machine', 'learning', 'services', 'for', 'problems', 'in', 'forecasting', 'computer', 'vision', 'and', 'natural', 'language', 'processing', '.', 'Finally', ',', 'module', 'seven', 'is', 'a', 'summary', 'of', 'the', 'course', '.', 'It', 'also', 'includes', 'an', 'overview', 'of', 'steps', 'you', 'can', 'take', 'to', 'work', 'towards', 'the', 'AWS', 'certified', 'machine', 'learning', 'specialty', '.', 'The', 'next', 'five', 'slides', 'provide', 'more', 'detail', 'about', 'the', 'subtopics', 'covered', 'in', 'each', 'module', '.', 'The', 'purpose', 'of', 'module', 'two', 'is', 'to', 'introduce', 'you', 'to', 'major', 'concepts', 'for', 'understanding', 'machine', 'learning', '.', 'Section', 'one', 'describes', 'the', 'overall', 'field', 'of', 'machine', 'learning', 'and', 'how', 'machine', 'learning', 'relates', 'to', 'artificial', 'intelligence', 'and', 'deep', 'learning', '.', 'In', 'section', 'two', ',', 'you', \"'ll\", 'learn', 'about', 'some', 'of', 'the', 'most', 'common', 'business', 'problems', 'you', 'can', 'solve', 'with', 'machine', 'learning', '.', 'Section', 'three', 'describes', 'the', 'general', 'workflow', 'for', 'solving', 'machine', 'learning', 'problems', '.', 'You', \"'ll\", 'also', 'learn', 'some', 'of', 'the', 'more', 'common', 'machine', 'learning', 'terms', '.', 'In', 'section', 'four', ',', 'you', \"'ll\", 'review', 'some', 'of', 'the', 'commonly', 'used', 'tools', 'by', 'machine', 'learning', 'professionals', '.', 'And', 'lastly', 'in', 'section', 'five', ',', 'you', \"'ll\", 'get', 'an', 'overview', 'of', 'some', 'of', 'the', 'common', 'challenges', 'you', \"'ll\", 'face', 'when', 'working', 'with', 'machine', 'learning', 'problems', '.', 'In', 'module', 'three', ',', 'you', \"'ll\", 'get', 'an', 'introduction', 'to', 'Amazon', \"'s\", 'sagemaker', 'and', 'how', 'you', 'can', 'use', 'it', 'to', 'implement', 'a', 'machine', 'learning', 'pipeline', '.', 'The', 'module', 'focuses', 'on', 'the', 'application', 'of', 'machine', 'learning', 'to', 'solve', 'problems', 'with', 'several', 'public', 'domain', 'data', 'sets', '.', 'As', 'examples', 'of', 'the', 'machine', 'learning', 'pipeline', '.', 'Section', 'one', 'introduces', 'you', 'to', 'defining', 'business', 'problems', 'and', 'the', 'data', 'sets', 'we', 'will', 'use', 'during', 'this', 'module', '.', 'Section', 'two', 'through', 'eight', 'described', 'the', 'phases', 'of', 'the', 'machine', 'learning', 'pipeline', 'by', 'using', 'computer', 'vision', 'as', 'an', 'example', 'application', '.', 'In', 'section', 'two', ',', 'you', \"'ll\", 'learn', 'how', 'to', 'collect', 'and', 'secure', 'data', '.', 'Section', 'three', 'describes', 'different', 'techniques', 'for', 'evaluating', 'data', '.', 'In', 'section', 'four', ',', 'you', \"'ll\", 'learn', 'about', 'the', 'process', 'of', 'feature', 'engineering', '.', 'Section', 'five', 'described', 'the', 'steps', 'he', \"'ll\", 'take', 'to', 'train', 'a', 'model', 'with', 'Sagemaker', '.', 'In', 'section', 'six', ',', 'you', \"'ll\", 'get', 'an', 'overview', 'of', 'the', 'options', 'in', 'Sagemaker', 'for', 'hosting', 'and', 'using', 'a', 'model', '.', 'Finally', ',', 'section', 'seven', 'and', 'eight', 'cover', 'how', 'to', 'evaluate', 'and', 'tune', 'your', 'model', 'with', 'Sagemaker', '.', 'In', 'this', 'module', ',', 'you', \"'ll\", 'be', 'introduced', 'to', 'using', 'machine', 'learning', 'to', 'create', 'forecasts', 'based', 'on', 'a', 'time', 'series', 'data', '.', 'In', 'section', 'one', ',', 'you', \"'ll\", 'be', 'introduced', 'to', 'forecasting', 'in', 'some', 'of', 'its', 'common', 'applications', '.', 'Section', 'two', 'outlined', 'some', 'of', 'the', 'pitfalls', 'of', 'using', 'time', 'series', 'data', 'to', 'make', 'forecasts', '.', 'Finally', ',', 'in', 'section', 'three', ',', 'you', \"'ll\", 'get', 'an', 'overview', 'of', 'how', 'to', 'use', 'Amazon', 'forecast', 'in', 'this', 'module', ',', 'you', \"'ll\", 'learn', 'about', 'using', 'machine', 'learning', 'for', 'computer', 'vision', '.', 'Section', 'one', 'describes', 'the', 'general', 'problems', 'you', 'can', 'solve', 'with', 'computer', 'vision', '.', 'In', 'section', 'two', ',', 'you', \"'ll\", 'learn', 'about', 'the', 'process', 'for', 'analyzing', 'images', 'and', 'videos', '.', 'And', 'in', 'section', 'three', ',', 'you', \"'ll\", 'learn', 'the', 'steps', 'you', \"'ll\", 'need', 'to', 'take', 'to', 'prepare', 'data', 'sets', 'for', 'computer', 'vision', '.', 'In', 'this', 'module', ',', 'you', \"'ll\", 'be', 'introduced', 'to', 'natural', 'language', 'processing', 'with', 'machine', 'learning', '.', 'In', 'section', 'one', ',', 'you', \"'ll\", 'learn', 'about', 'the', 'general', 'set', 'of', 'problems', 'you', 'can', 'solve', 'with', 'natural', 'language', 'processing', '.', 'Section', 'two', 'reviews', 'some', 'of', 'the', 'managed', 'Amazon', 'machine', 'learning', 'services', 'you', 'can', 'use', 'to', 'address', 'natural', 'language', 'processing', 'problems', '.', 'These', 'services', 'include', 'Amazon', ',', 'transcribe', ',', 'Amazon', 'translate', ',', 'Amazon', ',', 'Lex', ',', 'Amazon', 'comprehend', 'and', 'Amazon', 'Poly', 'module', 'seven', 'is', 'the', 'final', 'module', 'of', 'the', 'course', '.', 'In', 'this', 'module', ',', 'you', \"'ll\", 'review', 'what', 'you', \"'ve\", 'learned', 'throughout', 'this', 'course', '.', 'You', \"'ll\", 'also', 'be', 'introduced', 'to', 'the', 'next', 'steps', 'you', 'should', 'take', '.', 'If', 'you', 'want', 'to', 'achieve', 'the', 'AWS', 'Certified', 'Machine', 'learning', 'specialty', 'section', '.', 'One', 'of', 'this', 'module', 'summarizes', 'the', 'topics', 'you', \"'ve\", 'covered', 'in', 'this', 'course', '.', 'In', 'section', 'two', ',', 'you', \"'ll\", 'learn', 'more', 'about', 'the', 'Aws', 'documentation', '.', 'You', \"'ll\", 'also', 'review', 'two', 'common', 'frameworks', 'for', 'applying', 'Aws', 'services', '.', 'And', 'finally', ',', 'section', 'three', 'describes', 'the', 'steps', 'you', 'should', 'take', '.', 'If', 'you', 'want', 'to', 'continue', 'working', 'towards', 'the', 'Aws', 'certified', 'machine', 'learning', 'specialty', 'in', 'this', 'section', ',', 'you', \"'ll\", 'learn', 'about', 'some', 'of', 'the', 'more', 'common', 'job', 'roles', 'for', 'machine', 'learning', 'professionals', '.', 'If', 'you', \"'re\", 'interested', 'in', 'a', 'data', 'scientist', 'role', ',', 'focus', 'on', 'developing', 'analytical', 'statistical', 'and', 'programming', 'skills', '.', 'As', 'a', 'data', 'scientist', ',', 'you', \"'ll\", 'use', 'those', 'skills', 'to', 'collect', 'analyze', 'and', 'interpret', 'large', 'data', 'sets', '.', 'Some', 'universities', 'now', 'offer', 'degrees', 'in', 'data', 'science', ',', 'but', 'data', 'scientists', 'often', 'have', 'degrees', 'in', 'related', 'fields', 'like', 'statistics', ',', 'math', ',', 'computer', 'science', 'or', 'economics', '.', 'As', 'a', 'data', 'scientist', ',', 'you', \"'ll\", 'need', 'technical', 'competencies', 'in', 'statistics', ',', 'machine', 'learning', 'programming', 'languages', 'and', 'data', 'analytics', '.', 'If', 'you', \"'d\", 'like', 'to', 'have', 'a', 'career', 'as', 'a', 'machine', 'learning', 'engineer', ',', 'the', 'skills', 'you', \"'ll\", 'need', 'will', 'be', 'similar', 'to', 'a', 'data', 'scientist', 'skill', 'set', 'like', 'data', 'scientists', ',', 'machine', 'learning', 'engineers', 'also', 'require', 'technical', 'competencies', 'in', 'statistics', 'and', 'machine', 'learning', '.', 'However', ',', 'you', \"'ll\", 'focus', 'more', 'on', 'programming', 'skills', 'and', 'software', 'architecture', 'than', 'analysis', 'and', 'interpretation', '.', 'As', 'a', 'machine', 'learning', 'engineer', ',', 'you', \"'ll\", 'apply', 'those', 'programming', 'and', 'architecture', 'skills', 'to', 'design', 'and', 'develop', 'machine', 'learning', 'systems', '.', 'Machine', 'learning', 'engineers', 'often', 'have', 'previous', 'experience', 'with', 'software', 'development', 'and', 'they', 'rely', 'more', 'heavily', 'on', 'programming', 'and', 'software', 'engineering', 'than', 'other', 'machine', 'learning', 'roles', '.', 'You', 'might', 'also', 'be', 'interested', 'in', 'a', 'career', 'in', 'science', 'where', 'you', 'can', 'apply', 'machine', 'learning', 'technology', 'to', 'your', 'field', '.', 'Machine', 'learning', 'is', 'having', 'an', 'impact', 'in', 'everything', 'from', 'astronomy', 'to', 'zoology', '.', 'So', 'there', 'are', 'many', 'different', 'paths', 'open', 'to', 'you', '.', 'As', 'an', 'applied', 'science', 'researcher', ',', 'your', 'primary', 'focus', 'will', 'be', 'on', 'the', 'type', 'of', 'science', 'you', \"'re\", 'working', 'on', '.', 'You', \"'ll\", 'need', 'some', 'of', 'the', 'same', 'skills', 'as', 'a', 'data', 'scientist', ',', 'but', 'you', \"'ll\", 'also', 'need', 'to', 'know', 'how', 'to', 'apply', 'those', 'skills', 'to', 'your', 'chosen', 'domain', '.', 'Thus', ',', 'applied', 'science', 'roles', 'also', 'require', 'technical', 'competencies', 'in', 'statistics', 'and', 'machine', 'learning', '.', 'Many', 'software', 'developers', 'are', 'now', 'integrating', 'machine', 'learning', 'into', 'their', 'applications', '.', 'If', 'you', \"'re\", 'interested', 'in', 'a', 'career', 'as', 'a', 'software', 'developer', ',', 'you', 'should', 'also', 'include', 'machine', 'learning', 'technology', 'in', 'your', 'studies', '.', 'As', 'a', 'machine', 'learning', 'developer', ',', 'your', 'primary', 'focus', 'will', 'be', 'software', 'development', 'skills', ',', 'but', 'you', \"'ll\", 'also', 'need', 'some', 'of', 'the', 'same', 'skills', 'as', 'a', 'data', 'scientist', '.', 'So', 'make', 'sure', 'you', 'take', 'course', 'work', 'in', 'statistics', 'and', 'applied', 'mathematics', '.', 'And', 'here', \"'s\", 'a', 'final', 'note', 'for', 'this', 'module', ',', 'we', 'recommend', 'reviewing', 'your', 'student', 'guide', 'in', 'your', 'student', 'guide', ',', 'you', \"'ll\", 'find', 'links', 'to', 'documentation', 'and', 'other', 'resources', 'you', \"'ll\", 'use', 'throughout', 'the', 'course', '.', 'That', \"'s\", 'it', 'for', 'this', 'introduction', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome to Amazon Academy of Machine Learning Foundations in this module , you 'll learn about the course objective , various job role in the machine learning domain and where you can go to learn more about machine learning . After completing this module , you should be able to identify course prerequisite and objective indicate the role of the data scientist in business and identify resource for further learning . We 're now going to look at the prerequisite for taking this course . Before you take this course , we recommend that you first complete Aws Academy Cloud Foundations . You should also have some general technical knowledge of it including foundational computer literacy skill like basic computer concept , email file management and a good understanding of the internet . We also recommend that you have intermediate skill with Python programming and a general knowledge of applied statistic . Finally , general business knowledge is important for this course . This includes insight into how information technology is used in business . It 's also important to have business related skill set such a communication skill , leadership skill , and an orientation towards customer service . In this course , you 'll be introduced to the key concept of machine learning , it tool and it us you 'll also be introduced to and work with some of the AWS service for machine learning . You 'll learn how to recognize how machine learning and deep learning are part of artificial intelligence . Describe artificial intelligence and machine learning terminology . Identify how machine learning can be used to solve a business problem . Describe the machine learning process . List the tool available to data scientist and identify when to use machine learning instead of traditional software development method . As part of this course , you 'll also learn how to implement a machine learning pipeline . This includes how to formulate a problem from a business request , obtain and secure data for machine learning , build a Jupiter notebook by using Amazon Sagemaker outline the process for evaluating data . Explain why data need to be pre processed and use open source tool to examine and prep process data . You will also use Amazon 's Sage maker to train and host a machine learning model , use cross validation to test the performance of a machine learning model . Use a hosted model for inference , create an Amazon Sage maker hyper parameter tuning job to optimize a model 's effectiveness . And finally how to use managed Amazon machine learning service to solve specific machine learning problem in forecasting computer vision and natural language processing . We 'll now review the course outline to achieve the course objective . You 'll complete the following module to start in module two , you 'll get an introduction to machine learning in module three . You 'll learn how to implement a machine learning pipeline with Amazon Sagemaker module 45 and six describe how to apply managed Amazon machine learning service for problem in forecasting computer vision and natural language processing . Finally , module seven is a summary of the course . It also includes an overview of step you can take to work towards the AWS certified machine learning specialty . The next five slide provide more detail about the subtopics covered in each module . The purpose of module two is to introduce you to major concept for understanding machine learning . Section one describes the overall field of machine learning and how machine learning relates to artificial intelligence and deep learning . In section two , you 'll learn about some of the most common business problem you can solve with machine learning . Section three describes the general workflow for solving machine learning problem . You 'll also learn some of the more common machine learning term . In section four , you 'll review some of the commonly used tool by machine learning professional . And lastly in section five , you 'll get an overview of some of the common challenge you 'll face when working with machine learning problem . In module three , you 'll get an introduction to Amazon 's sagemaker and how you can use it to implement a machine learning pipeline . The module focus on the application of machine learning to solve problem with several public domain data set . As example of the machine learning pipeline . Section one introduces you to defining business problem and the data set we will use during this module . Section two through eight described the phase of the machine learning pipeline by using computer vision a an example application . In section two , you 'll learn how to collect and secure data . Section three describes different technique for evaluating data . In section four , you 'll learn about the process of feature engineering . Section five described the step he 'll take to train a model with Sagemaker . In section six , you 'll get an overview of the option in Sagemaker for hosting and using a model . Finally , section seven and eight cover how to evaluate and tune your model with Sagemaker . In this module , you 'll be introduced to using machine learning to create forecast based on a time series data . In section one , you 'll be introduced to forecasting in some of it common application . Section two outlined some of the pitfall of using time series data to make forecast . Finally , in section three , you 'll get an overview of how to use Amazon forecast in this module , you 'll learn about using machine learning for computer vision . Section one describes the general problem you can solve with computer vision . In section two , you 'll learn about the process for analyzing image and video . And in section three , you 'll learn the step you 'll need to take to prepare data set for computer vision . In this module , you 'll be introduced to natural language processing with machine learning . In section one , you 'll learn about the general set of problem you can solve with natural language processing . Section two review some of the managed Amazon machine learning service you can use to address natural language processing problem . These service include Amazon , transcribe , Amazon translate , Amazon , Lex , Amazon comprehend and Amazon Poly module seven is the final module of the course . In this module , you 'll review what you 've learned throughout this course . You 'll also be introduced to the next step you should take . If you want to achieve the AWS Certified Machine learning specialty section . One of this module summarizes the topic you 've covered in this course . In section two , you 'll learn more about the Aws documentation . You 'll also review two common framework for applying Aws service . And finally , section three describes the step you should take . If you want to continue working towards the Aws certified machine learning specialty in this section , you 'll learn about some of the more common job role for machine learning professional . If you 're interested in a data scientist role , focus on developing analytical statistical and programming skill . As a data scientist , you 'll use those skill to collect analyze and interpret large data set . Some university now offer degree in data science , but data scientist often have degree in related field like statistic , math , computer science or economics . As a data scientist , you 'll need technical competency in statistic , machine learning programming language and data analytics . If you 'd like to have a career a a machine learning engineer , the skill you 'll need will be similar to a data scientist skill set like data scientist , machine learning engineer also require technical competency in statistic and machine learning . However , you 'll focus more on programming skill and software architecture than analysis and interpretation . As a machine learning engineer , you 'll apply those programming and architecture skill to design and develop machine learning system . Machine learning engineer often have previous experience with software development and they rely more heavily on programming and software engineering than other machine learning role . You might also be interested in a career in science where you can apply machine learning technology to your field . Machine learning is having an impact in everything from astronomy to zoology . So there are many different path open to you . As an applied science researcher , your primary focus will be on the type of science you 're working on . You 'll need some of the same skill a a data scientist , but you 'll also need to know how to apply those skill to your chosen domain . Thus , applied science role also require technical competency in statistic and machine learning . Many software developer are now integrating machine learning into their application . If you 're interested in a career a a software developer , you should also include machine learning technology in your study . As a machine learning developer , your primary focus will be software development skill , but you 'll also need some of the same skill a a data scientist . So make sure you take course work in statistic and applied mathematics . And here 's a final note for this module , we recommend reviewing your student guide in your student guide , you 'll find link to documentation and other resource you 'll use throughout the course . That 's it for this introduction . Thanks for watching . We 'll see you in the next video .\n",
      "['Hi', 'and', 'welcome', 'to', 'module', 'two', 'of', 'Aws', 'Academy', 'machine', 'learning', 'in', 'this', 'module', ',', 'we', \"'re\", 'going', 'to', 'introduce', 'machine', 'learning', '.', 'We', \"'ll\", 'first', 'look', 'at', 'the', 'business', 'problems', 'that', 'can', 'be', 'solved', 'by', 'machine', 'learning', '.', 'We', \"'ll\", 'then', 'talk', 'about', 'terminology', ',', 'process', 'tools', 'and', 'some', 'of', 'the', 'challenges', 'you', \"'ll\", 'face', '.', 'After', 'completing', 'this', 'module', ',', 'you', 'should', 'be', 'able', 'to', 'recognize', 'how', 'machine', 'learning', 'and', 'deep', 'learning', 'are', 'part', 'of', 'artificial', 'intelligence', '.', 'Describe', 'artificial', 'intelligence', 'and', 'machine', 'learning', 'terminology', '.', 'Identify', 'how', 'machine', 'learning', 'can', 'be', 'used', 'to', 'solve', 'a', 'business', 'problem', '.', 'Describe', 'the', 'machine', 'learning', 'process', 'list', 'the', 'tools', 'available', 'to', 'data', 'scientists', 'and', 'identify', 'when', 'to', 'use', 'machine', 'learning', 'instead', 'of', 'traditional', 'software', 'development', 'methods', '.', 'You', \"'re\", 'now', 'ready', 'to', 'get', 'started', 'with', '.', 'Section', 'one', '.', 'See', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome to module two of Aws Academy machine learning in this module , we 're going to introduce machine learning . We 'll first look at the business problem that can be solved by machine learning . We 'll then talk about terminology , process tool and some of the challenge you 'll face . After completing this module , you should be able to recognize how machine learning and deep learning are part of artificial intelligence . Describe artificial intelligence and machine learning terminology . Identify how machine learning can be used to solve a business problem . Describe the machine learning process list the tool available to data scientist and identify when to use machine learning instead of traditional software development method . You 're now ready to get started with . Section one . See you in the next video .\n",
      "['Hi', 'and', 'welcome', 'to', 'section', 'one', 'in', 'this', 'section', '.', 'We', \"'re\", 'going', 'to', 'talk', 'about', 'what', 'machine', 'learning', 'is', '.', 'This', 'course', 'is', 'an', 'introduction', 'to', 'machine', 'learning', ',', 'which', 'is', 'also', 'known', 'as', 'M', 'L.', 'But', 'first', 'we', \"'ll\", 'discuss', 'where', 'machine', 'learning', 'fits', 'into', 'the', 'larger', 'picture', '.', 'Machine', 'learning', 'is', 'a', 'subset', 'of', 'artificial', 'intelligence', 'or', 'A', 'I', '.', 'This', 'is', 'a', 'broad', 'branch', 'of', 'computer', 'science', 'that', \"'s\", 'focused', 'on', 'building', 'machines', 'that', 'can', 'do', 'human', 'tasks', '.', 'Deep', 'learning', 'is', 'a', 'subdomain', 'of', 'machine', 'learning', 'to', 'understand', 'where', 'these', 'all', 'fit', 'together', '.', 'We', \"'ll\", 'discuss', 'each', 'one', '.', 'As', 'we', 'just', 'mentioned', ',', 'machine', 'learning', 'is', 'a', 'subset', 'of', 'a', 'broader', 'computer', 'science', 'field', 'known', 'as', 'artificial', 'intelligence', '.', 'A', 'I', 'focuses', 'on', 'building', 'machines', 'that', 'can', 'perform', 'tasks', '.', 'A', 'human', 'would', 'typically', 'perform', 'in', 'contemporary', 'popular', 'culture', '.', 'You', \"'ve\", 'probably', 'seen', 'A', 'I', 'S', 'in', 'movies', ',', 'television', 'or', 'works', 'of', 'fiction', '.', 'For', 'example', ',', 'you', 'might', 'have', 'seen', 'A', 'I', 'S', 'that', 'control', 'the', 'world', 'around', 'them', 'or', 'that', 'start', 'acting', 'on', 'their', 'own', 'initiative', '.', 'These', 'A', 'I', 'S', 'started', 'as', 'computer', 'agents', 'that', 'perceive', 'their', 'environments', 'and', 'take', 'actions', 'to', 'achieve', 'a', 'specific', 'goal', 'though', 'maybe', 'not', 'the', 'outcome', 'the', 'creators', 'originally', 'wished', 'for', 'other', 'fictional', 'A', 'I', 'S', 'interact', 'extensively', 'with', 'humans', 'as', 'helpers', 'or', 'workers', 'and', 'they', 'generally', 'do', 'a', 'better', 'job', 'working', 'with', 'humanity', '.', 'But', 'they', \"'re\", 'more', 'general', 'in', 'purpose', '.', 'These', 'kinds', 'of', 'A', 'I', 'S', 'are', 'examples', 'of', 'artificial', 'general', 'intelligence', 'or', 'A', 'G', 'I', '.', 'They', 'have', 'the', 'capacity', 'to', 'learn', 'or', 'understand', 'any', 'task', 'that', 'a', 'human', 'can', 'A', 'I', 'problems', 'typically', 'span', 'many', 'fields', 'of', 'research', 'such', 'as', 'natural', 'language', 'processing', ',', 'reasoning', ',', 'knowledge', ',', 'representation', ',', 'learning', ',', 'perception', 'and', 'physical', 'environment', ',', 'interaction', '.', 'A', 'I', 'is', \"n't\", 'yet', 'a', 'reality', 'for', 'us', 'unless', 'we', 'are', 'all', 'truly', 'living', 'in', 'a', 'simulation', '.', 'But', 'every', 'year', 'we', 'move', 'closer', 'to', 'it', '.', 'In', 'each', 'of', 'those', 'areas', ',', 'you', 'might', 'have', 'also', 'read', 'or', 'seen', 'commentary', 'on', 'the', 'ethics', 'of', 'creating', 'A', 'I', '.', 'Not', 'all', 'views', 'are', 'positive', ',', 'perhaps', 'partly', 'in', 'fear', 'of', 'the', 'malicious', 'fictional', 'A', 'I', 'S', 'that', 'want', 'to', 'destroy', 'humanity', 'or', 'use', 'them', 'as', 'power', 'sources', 'or', 'perhaps', 'they', \"'re\", 'concerned', 'about', 'the', 'risk', 'of', 'mass', 'unemployment', 'because', 'an', 'intelligent', 'machine', 'could', 'work', '24', '7', 'and', 'not', 'need', 'any', 'breaks', '.', 'Do', \"n't\", 'worry', 'though', 'we', \"'re\", 'not', 'going', 'to', 'build', 'the', 'next', 'ro', 'A', 'I', 'in', 'this', 'course', '.', 'Maybe', 'in', 'the', 'next', 'one', ',', 'if', 'you', 'do', 'a', 'search', ',', 'you', \"'ll\", 'probably', 'find', 'many', 'definitions', 'of', 'machine', 'learning', '.', 'There', 'is', \"n't\", 'a', 'universally', 'agreed', 'upon', 'definition', '.', 'So', 'we', \"'ll\", 'start', 'by', 'looking', 'at', 'a', 'couple', 'of', 'definitions', '.', 'For', 'example', ',', 'we', 'could', 'say', 'machine', 'learning', 'is', 'the', 'scientific', 'study', 'of', 'algorithms', 'and', 'statistical', 'models', 'to', 'perform', 'a', 'task', 'by', 'using', 'inference', 'instead', 'of', 'instructions', '.', 'This', 'is', \"n't\", 'a', 'bad', 'starting', 'point', '.', 'The', 'key', 'point', 'here', 'is', 'using', 'algorithms', 'and', 'statistical', 'models', 'instead', 'of', 'instructions', 'to', 'help', 'you', 'better', 'understand', 'this', '.', 'We', \"'ll\", 'apply', 'this', 'idea', 'to', 'a', 'concrete', 'example', '.', 'Suppose', 'you', 'need', 'to', 'write', 'an', 'application', 'that', 'determines', 'if', 'an', 'email', 'message', 'is', 'spam', 'or', 'not', '.', 'Without', 'machine', 'learning', ',', 'you', \"'d\", 'need', 'to', 'write', 'a', 'complex', 'series', 'of', 'decision', 'statements', 'using', 'if', 'and', 'statements', '.', 'You', \"'d\", 'also', 'need', 'to', 'use', 'words', 'in', 'the', 'subject', 'or', 'body', ',', 'the', 'number', 'of', 'links', 'and', 'the', 'length', 'of', 'the', 'message', 'to', 'determine', 'if', 'an', 'email', 'message', 'is', 'spam', ',', 'it', 'would', 'be', 'hard', 'and', 'labor', 'intensive', 'to', 'build', 'a', 'large', 'set', 'of', 'rules', 'covering', 'every', 'possibility', 'with', 'machine', 'learning', '.', 'However', ',', 'you', 'could', 'use', 'a', 'list', 'of', 'email', 'messages', 'that', 'were', 'marked', 'as', 'spam', 'or', 'not', 'spam', 'and', 'train', 'a', 'machine', 'learning', 'model', '.', 'The', 'model', 'would', 'then', 'learn', 'which', 'patterns', 'of', 'words', 'length', 'and', 'other', 'attributes', 'are', 'good', 'indicators', 'of', 'spam', 'messages', '.', 'If', 'you', 'presented', 'the', 'model', 'with', 'an', 'email', 'message', ',', 'it', 'had', \"n't\", 'seen', 'before', '.', 'The', 'model', 'would', 'perform', 'a', 'prediction', 'to', 'say', 'whether', 'the', 'message', 'was', 'spam', 'or', 'not', 'spam', '.', 'Deep', 'learning', 'represents', 'a', 'significant', 'leap', 'forward', 'in', 'the', 'capabilities', 'for', 'artificial', 'intelligence', 'and', 'machine', 'learning', '.', 'The', 'theory', 'behind', 'deep', 'learning', 'was', 'created', 'from', 'how', 'the', 'human', 'brain', 'works', '.', 'An', 'artificial', 'neural', 'network', 'or', 'A', 'N', 'N', 'is', 'inspired', 'by', 'the', 'biological', 'neurons', 'found', 'in', 'the', 'brain', '.', 'Although', 'the', 'implementation', 'has', 'become', 'very', 'different', ',', 'artificial', 'neurons', 'have', 'one', 'or', 'more', 'inputs', '.', 'And', 'a', 'single', 'output', '.', 'These', 'neurons', 'fire', 'or', 'activate', 'their', 'outputs', 'based', 'on', 'a', 'transformation', 'of', 'the', 'inputs', '.', 'A', 'neural', 'network', 'is', 'composed', 'of', 'layers', 'of', 'these', 'artificial', 'neurons', 'with', 'connections', 'between', 'the', 'layers', '.', 'There', 'are', 'typically', 'input', 'output', 'and', 'hidden', 'layers', 'in', 'the', 'network', '.', 'The', 'output', 'of', 'a', 'single', 'neuron', 'connects', 'to', 'the', 'inputs', 'of', 'all', 'the', 'neurons', '.', 'In', 'the', 'next', 'layer', ',', 'the', 'network', 'is', 'then', 'asked', 'to', 'solve', 'a', 'problem', '.', 'The', 'input', 'layer', 'is', 'populated', 'from', 'the', 'training', 'data', 'and', 'the', 'neurons', 'activate', 'throughout', 'the', 'layers', 'until', 'an', 'answer', 'is', 'presented', 'in', 'the', 'output', 'layer', '.', 'The', 'accuracy', 'of', 'the', 'output', 'is', 'then', 'measured', 'if', 'the', 'output', 'does', \"n't\", 'meet', 'your', 'threshold', ',', 'the', 'training', 'is', 'repeated', '.', 'But', 'with', 'slight', 'changes', 'to', 'the', 'weights', 'of', 'the', 'connections', 'between', 'neurons', ',', 'the', 'neural', 'network', 'will', 'do', 'this', 'repeatedly', 'each', 'time', 'it', 'strengthens', 'the', 'connections', 'that', 'lead', 'to', 'success', 'and', 'diminishes', 'the', 'connections', 'that', 'lead', 'to', 'failure', '.', 'As', 'you', \"'ll\", 'see', 'in', 'this', 'course', ',', 'machine', 'learning', 'practitioners', 'spend', 'a', 'lot', 'of', 'time', 'optimizing', 'the', 'ML', 'models', ',', 'selecting', 'the', 'best', 'data', 'features', 'to', 'train', 'with', 'and', 'selecting', 'the', 'models', 'with', 'the', 'best', 'results', '.', 'In', 'contrast', ',', 'deep', 'learning', 'practitioners', 'spend', 'almost', 'no', 'time', 'on', 'those', 'tasks', '.', 'Instead', ',', 'they', 'spend', 'their', 'time', 'modeling', 'data', 'with', 'different', 'A', 'N', 'N', 'architectures', '.', 'Though', 'the', 'theory', 'for', 'deep', 'learning', 'goes', 'back', 'decades', ',', 'the', 'hardware', 'needed', 'to', 'run', 'deep', 'learning', 'problems', 'was', \"n't\", 'generally', 'accessible', 'until', 'recently', '.', 'But', 'now', 'that', 'it', \"'s\", 'available', ',', 'you', 'can', 'use', 'deep', 'learning', 'to', 'address', 'problems', 'that', 'are', 'more', 'complex', 'than', 'the', 'problems', 'you', 'could', 'have', 'worked', 'on', 'before', 'mainstream', 'machine', 'learning', 'is', 'a', 'recent', 'occurrence', ',', 'rapid', 'advancements', 'in', 'machine', 'and', 'deep', 'learning', 'only', 'started', 'around', 'the', 'mid', 'two', 'thousands', '.', 'This', 'is', 'partly', 'because', 'Moore', \"'s\", 'law', 'and', 'the', 'rise', 'of', 'cloud', 'computing', 'resulted', 'in', 'easier', 'access', 'to', 'larger', ',', 'faster', 'and', 'cheaper', 'compute', 'and', 'storage', 'capabilities', '.', 'You', 'can', 'now', 'rent', 'computing', 'power', 'for', 'a', 'few', 'hours', 'for', 'pennies', 'before', 'this', ',', 'you', 'needed', 'substantial', 'investments', 'to', 'buy', 'and', 'operate', 'large', 'scale', 'compute', 'clusters', 'on', 'your', 'own', '.', 'In', '2012', 'neural', 'networks', 'started', 'to', 'be', 'used', 'in', 'the', 'image', 'net', 'large', 'scale', 'visual', 'recognition', 'challenge', 'and', 'machine', 'learning', 'competition', 'for', 'image', 'recognition', '.', 'The', 'accuracy', 'rate', 'jumped', 'up', 'to', 'about', '82', '%', 'and', 'has', 'been', 'steadily', 'climbing', 'ever', 'since', '.', 'In', 'fact', ',', 'it', 'exceeded', 'human', 'performance', 'in', '2015', '.', 'Here', 'are', 'some', 'of', 'the', 'key', 'takeaways', 'for', 'this', 'section', '.', 'First', 'artificial', 'intelligence', 'is', 'the', 'broad', 'field', 'of', 'building', 'machines', 'to', 'perform', 'human', 'tasks', '.', 'Also', ',', 'machine', 'learning', 'is', 'a', 'subset', 'of', 'A', 'I', '.', 'It', 'focuses', 'on', 'using', 'data', 'to', 'train', 'machine', 'learning', 'models', 'so', 'they', 'can', 'make', 'predictions', '.', 'Deep', 'learning', 'is', 'a', 'technique', 'inspired', 'from', 'human', 'biology', '.', 'It', 'uses', 'layers', 'of', 'artificial', 'neurons', 'to', 'build', 'networks', 'that', 'solve', 'problems', '.', 'And', 'last', 'advancements', 'in', 'technology', ',', 'cloud', 'computing', 'and', 'algorithm', 'development', 'have', 'led', 'to', 'a', 'corresponding', 'advance', 'in', 'machine', 'learning', 'capabilities', 'and', 'applications', '.', 'That', \"'s\", 'it', 'for', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome to section one in this section . We 're going to talk about what machine learning is . This course is an introduction to machine learning , which is also known a M L. But first we 'll discus where machine learning fit into the larger picture . Machine learning is a subset of artificial intelligence or A I . This is a broad branch of computer science that 's focused on building machine that can do human task . Deep learning is a subdomain of machine learning to understand where these all fit together . We 'll discus each one . As we just mentioned , machine learning is a subset of a broader computer science field known a artificial intelligence . A I focus on building machine that can perform task . A human would typically perform in contemporary popular culture . You 've probably seen A I S in movie , television or work of fiction . For example , you might have seen A I S that control the world around them or that start acting on their own initiative . These A I S started a computer agent that perceive their environment and take action to achieve a specific goal though maybe not the outcome the creator originally wished for other fictional A I S interact extensively with human a helper or worker and they generally do a better job working with humanity . But they 're more general in purpose . These kind of A I S are example of artificial general intelligence or A G I . They have the capacity to learn or understand any task that a human can A I problem typically span many field of research such a natural language processing , reasoning , knowledge , representation , learning , perception and physical environment , interaction . A I is n't yet a reality for u unless we are all truly living in a simulation . But every year we move closer to it . In each of those area , you might have also read or seen commentary on the ethic of creating A I . Not all view are positive , perhaps partly in fear of the malicious fictional A I S that want to destroy humanity or use them a power source or perhaps they 're concerned about the risk of mass unemployment because an intelligent machine could work 24 7 and not need any break . Do n't worry though we 're not going to build the next ro A I in this course . Maybe in the next one , if you do a search , you 'll probably find many definition of machine learning . There is n't a universally agreed upon definition . So we 'll start by looking at a couple of definition . For example , we could say machine learning is the scientific study of algorithm and statistical model to perform a task by using inference instead of instruction . This is n't a bad starting point . The key point here is using algorithm and statistical model instead of instruction to help you better understand this . We 'll apply this idea to a concrete example . Suppose you need to write an application that determines if an email message is spam or not . Without machine learning , you 'd need to write a complex series of decision statement using if and statement . You 'd also need to use word in the subject or body , the number of link and the length of the message to determine if an email message is spam , it would be hard and labor intensive to build a large set of rule covering every possibility with machine learning . However , you could use a list of email message that were marked a spam or not spam and train a machine learning model . The model would then learn which pattern of word length and other attribute are good indicator of spam message . If you presented the model with an email message , it had n't seen before . The model would perform a prediction to say whether the message wa spam or not spam . Deep learning represents a significant leap forward in the capability for artificial intelligence and machine learning . The theory behind deep learning wa created from how the human brain work . An artificial neural network or A N N is inspired by the biological neuron found in the brain . Although the implementation ha become very different , artificial neuron have one or more input . And a single output . These neuron fire or activate their output based on a transformation of the input . A neural network is composed of layer of these artificial neuron with connection between the layer . There are typically input output and hidden layer in the network . The output of a single neuron connects to the input of all the neuron . In the next layer , the network is then asked to solve a problem . The input layer is populated from the training data and the neuron activate throughout the layer until an answer is presented in the output layer . The accuracy of the output is then measured if the output doe n't meet your threshold , the training is repeated . But with slight change to the weight of the connection between neuron , the neural network will do this repeatedly each time it strengthens the connection that lead to success and diminishes the connection that lead to failure . As you 'll see in this course , machine learning practitioner spend a lot of time optimizing the ML model , selecting the best data feature to train with and selecting the model with the best result . In contrast , deep learning practitioner spend almost no time on those task . Instead , they spend their time modeling data with different A N N architecture . Though the theory for deep learning go back decade , the hardware needed to run deep learning problem wa n't generally accessible until recently . But now that it 's available , you can use deep learning to address problem that are more complex than the problem you could have worked on before mainstream machine learning is a recent occurrence , rapid advancement in machine and deep learning only started around the mid two thousand . This is partly because Moore 's law and the rise of cloud computing resulted in easier access to larger , faster and cheaper compute and storage capability . You can now rent computing power for a few hour for penny before this , you needed substantial investment to buy and operate large scale compute cluster on your own . In 2012 neural network started to be used in the image net large scale visual recognition challenge and machine learning competition for image recognition . The accuracy rate jumped up to about 82 % and ha been steadily climbing ever since . In fact , it exceeded human performance in 2015 . Here are some of the key takeaway for this section . First artificial intelligence is the broad field of building machine to perform human task . Also , machine learning is a subset of A I . It focus on using data to train machine learning model so they can make prediction . Deep learning is a technique inspired from human biology . It us layer of artificial neuron to build network that solve problem . And last advancement in technology , cloud computing and algorithm development have led to a corresponding advance in machine learning capability and application . That 's it for this section , we 'll see you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', 'in', 'this', 'section', '.', 'We', \"'re\", 'going', 'to', 'look', 'at', 'the', 'types', 'of', 'business', 'problems', '.', 'Machine', 'learning', 'can', 'help', 'you', 'solve', '.', 'Machine', 'learning', 'is', 'used', 'all', 'across', 'your', 'digital', 'lives', '.', 'Your', 'email', 'spam', 'filter', 'is', 'the', 'result', 'of', 'a', 'machine', 'learning', 'program', 'that', 'was', 'trained', 'with', 'examples', 'of', 'spam', 'and', 'regular', 'email', 'messages', 'based', 'on', 'books', '.', 'You', \"'re\", 'reading', 'or', 'products', 'you', 'bought', 'machine', 'learning', 'programs', 'can', 'predict', 'other', 'books', 'or', 'products', 'you', \"'re\", 'likely', 'to', 'be', 'interested', 'in', '.', 'Again', ',', 'the', 'machine', 'learning', 'program', 'was', 'trained', 'with', 'data', 'from', 'other', 'readers', 'habits', 'and', 'purchases', '.', 'When', 'detecting', 'credit', 'card', 'fraud', ',', 'the', 'machine', 'learning', 'program', 'was', 'trained', 'on', 'examples', 'of', 'transactions', 'that', 'turned', 'out', 'to', 'be', 'fraud', 'along', 'with', 'normal', 'transactions', '.', 'You', 'can', 'probably', 'think', 'of', 'many', 'more', 'examples', 'from', 'social', 'media', 'applications', 'using', 'facial', 'detection', 'to', 'group', 'your', 'photos', 'to', 'detecting', 'brain', 'tumors', 'in', 'brain', 'scans', 'or', 'finding', 'anomalies', 'in', 'x', 'rays', '.', 'There', 'are', 'three', 'main', 'types', 'of', 'machine', 'learning', '.', 'There', \"'s\", 'supervised', 'learning', 'where', 'a', 'model', 'uses', 'known', 'inputs', 'and', 'outputs', 'to', 'generalize', 'future', 'outputs', '.', 'There', \"'s\", 'unsupervised', 'learning', 'where', 'the', 'model', 'does', \"n't\", 'know', 'inputs', 'or', 'outputs', '.', 'So', 'it', 'finds', 'patterns', 'in', 'the', 'data', 'without', 'help', '.', 'And', 'there', \"'s\", 'reinforcement', 'learning', 'where', 'the', 'model', 'interacts', 'with', 'its', 'environment', 'and', 'learns', 'to', 'take', 'actions', 'that', 'will', 'maximize', 'rewards', '.', 'It', \"'s\", 'important', 'to', 'know', 'the', 'different', 'types', 'of', 'M', 'L', 'because', 'the', 'type', 'will', 'guide', 'you', 'towards', 'selecting', 'algorithms', 'that', 'make', 'sense', 'for', 'solving', 'your', 'business', 'problem', '.', 'Let', \"'s\", 'look', 'more', 'into', 'each', 'of', 'these', 'types', '.', 'Supervised', 'learning', 'is', 'a', 'popular', 'type', 'of', 'M', 'L', 'because', 'it', \"'s\", 'widely', 'applicable', ',', 'it', \"'s\", 'called', 'supervised', 'learning', 'because', 'there', 'needs', 'to', 'be', 'a', 'supervisor', ',', 'a', 'teacher', 'who', 'can', 'show', 'the', 'right', 'answers', ',', 'so', 'to', 'speak', '.', 'Like', 'any', 'student', ',', 'a', 'supervised', 'algorithm', 'needs', 'to', 'learn', '.', 'By', 'example', ',', 'essentially', 'it', 'needs', 'a', 'teacher', 'who', 'uses', 'training', 'data', 'to', 'help', 'it', 'determine', 'the', 'patterns', 'and', 'relationships', 'between', 'the', 'inputs', 'and', 'outputs', '.', 'If', 'you', 'want', 'to', 'build', 'an', 'application', 'to', 'detect', 'credit', 'card', 'fraud', ',', 'you', \"'d\", 'need', 'training', 'data', 'that', 'includes', 'examples', 'of', 'fraud', 'and', 'examples', 'of', 'normal', 'transactions', 'within', 'supervised', 'learning', '.', 'There', 'are', 'different', 'types', 'of', 'problems', ',', 'classification', 'and', 'regression', '.', 'There', 'are', 'two', 'subtypes', 'of', 'classification', 'problems', '.', 'The', 'first', 'is', 'binary', 'classification', '.', 'Think', 'back', 'to', 'the', 'example', 'with', 'identifying', 'fraudulent', 'transactions', ',', 'the', 'target', 'variable', 'in', 'this', 'example', 'is', 'limited', 'to', 'two', 'options', ',', 'fraudulent', 'or', 'not', 'fraudulent', '.', 'This', 'is', 'a', 'binary', 'classification', 'problem', '.', 'There', 'are', 'also', 'multi', 'class', 'classification', 'problems', '.', 'These', 'M', 'L', 'problems', 'classify', 'an', 'observation', 'into', 'one', 'of', 'three', 'or', 'more', 'categories', '.', 'Say', 'that', 'you', 'have', 'an', 'M', 'L', 'model', 'that', 'predicts', 'why', 'a', 'customer', 'is', 'calling', 'your', 'store', '.', 'So', 'you', 'can', 'reduce', 'the', 'number', 'of', 'transfers', 'needed', 'before', 'the', 'customer', 'gets', 'to', 'the', 'correct', 'customer', 'support', 'department', '.', 'In', 'this', 'case', ',', 'the', 'different', 'customer', 'support', 'departments', 'represent', 'the', 'variety', 'of', 'potential', 'target', 'variables', ',', 'which', 'could', 'be', 'many', 'different', 'departments', 'much', 'more', 'than', 'just', 'two', '.', 'There', 'are', 'also', 'regression', 'problems', 'in', 'a', 'regression', 'problem', '.', 'You', \"'re\", 'no', 'longer', 'mapping', 'an', 'input', 'to', 'a', 'defined', 'number', 'of', 'categories', '.', 'Instead', 'you', \"'re\", 'mapping', 'an', 'input', 'to', 'a', 'continuous', 'value', 'like', 'an', 'integer', '.', 'One', 'example', 'of', 'an', 'M', 'L', 'regression', 'problem', 'is', 'predicting', 'the', 'price', 'of', 'a', 'company', \"'s\", 'stock', '.', 'Computer', 'vision', 'is', 'a', 'good', 'example', 'of', 'supervised', 'learning', '.', 'Is', 'this', 'a', 'cat', 'or', 'a', 'dog', '?', 'Is', 'there', 'a', 'tumor', 'in', 'this', 'x-ray', 'computer', 'vision', 'is', 'often', 'built', 'with', 'deep', 'learning', 'models', '.', 'It', 'automates', 'the', 'extraction', 'analysis', ',', 'classification', 'and', 'understanding', 'of', 'useful', 'information', 'from', 'a', 'single', 'image', 'or', 'a', 'sequence', 'of', 'images', '.', 'Computer', 'vision', 'enables', 'machines', 'to', 'identify', 'people', ',', 'places', 'and', 'things', 'and', 'images', 'with', 'accuracy', 'at', 'or', 'above', 'human', 'levels', 'and', 'with', 'greater', 'speed', 'and', 'efficiency', '.', 'The', 'image', 'data', 'can', 'take', 'many', 'forms', 'such', 'as', 'single', 'images', ',', 'video', 'sequences', ',', 'views', 'from', 'multiple', 'cameras', 'or', 'three', 'dimensional', 'data', '.', 'You', \"'ll\", 'learn', 'more', 'about', 'computer', 'vision', '.', 'Later', 'in', 'this', 'course', ',', 'we', \"'ll\", 'now', 'discuss', 'unsupervised', 'machine', 'learning', '.', 'Sometimes', 'all', 'you', 'have', 'is', 'the', 'data', '.', 'There', \"'s\", 'no', 'supervisor', 'in', 'the', 'room', 'in', 'unsupervised', 'learning', 'labels', 'are', \"n't\", 'provided', 'like', 'they', 'are', 'with', 'supervised', 'learning', '.', 'You', 'do', \"n't\", 'know', 'all', 'the', 'variables', 'and', 'patterns', 'in', 'these', 'instances', '.', 'The', 'machine', 'has', 'to', 'uncover', 'and', 'create', 'the', 'labels', 'itself', '.', 'These', 'models', 'use', 'the', 'data', 'they', \"'re\", 'presented', 'with', 'to', 'detect', 'emerging', 'properties', 'of', 'the', 'entire', 'data', 'set', '.', 'Then', 'they', 'construct', 'patterns', 'from', 'those', 'properties', '.', 'Clustering', 'is', 'a', 'common', 'subcategory', 'of', 'unsupervised', 'learning', '.', 'This', 'kind', 'of', 'algorithm', 'groups', 'data', 'into', 'different', 'clusters', 'based', 'on', 'similar', 'features', '.', 'It', 'does', 'this', 'to', 'better', 'understand', 'the', 'attributes', 'of', 'a', 'specific', 'cluster', '.', 'For', 'example', ',', 'by', 'analyzing', 'customer', 'purchasing', 'habits', ',', 'unsupervised', 'algorithms', 'can', 'identify', 'groups', 'of', 'customers', 'that', 'are', 'associated', 'with', 'the', 'size', 'tier', 'of', 'a', 'company', '.', 'The', 'advantage', 'of', 'unsupervised', 'algorithms', 'is', 'that', 'they', 'enable', 'you', 'to', 'see', 'patterns', 'in', 'the', 'data', 'that', 'you', 'were', \"n't\", 'aware', 'of', 'before', 'natural', 'language', 'processing', 'is', 'also', 'known', 'as', 'N', 'LP', '.', 'This', 'is', 'another', 'area', 'of', 'machine', 'learning', 'that', \"'s\", 'experiencing', 'growth', '.', 'If', 'you', \"'ve\", 'ever', 'used', 'Alexa', 'or', 'any', 'other', 'voice', 'assistant', ',', 'they', \"'ll\", 'use', 'N', 'LP', 'to', 'try', 'and', 'answer', 'your', 'question', '.', 'N', 'LP', 'is', \"n't\", 'just', 'about', 'speech', ',', 'it', \"'s\", 'also', 'about', 'written', 'text', '.', 'N', 'LP', 'shows', 'up', 'in', 'many', 'applications', '.', 'For', 'example', ',', 'N', 'LP', 'is', 'used', 'with', 'chat', 'or', 'call', 'center', 'bots', 'which', 'are', 'automated', 'systems', 'that', 'help', 'you', 'get', 'your', 'bank', 'balance', 'or', 'order', 'food', 'from', 'a', 'restaurant', '.', 'You', 'can', 'use', 'N', 'LP', 'and', 'translation', 'tools', 'which', 'convert', 'text', 'between', 'languages', '.', 'For', 'example', ',', 'you', 'might', 'use', 'applications', 'that', 'translate', 'menus', 'in', 'real', 'time', '.', 'N', 'LP', 'is', 'also', 'used', 'in', 'voice', 'to', 'text', 'translations', 'which', 'converts', 'spoken', 'words', 'into', 'text', '.', 'And', 'finally', 'NLP', 'can', 'be', 'used', 'in', 'sentiment', 'analysis', 'which', 'you', 'can', 'use', 'to', 'analyze', 'the', 'sentiment', 'of', 'comments', 'and', 'reviews', 'of', 'products', ',', 'music', 'and', 'movies', '.', 'These', 'sentiments', 'could', 'be', 'used', 'to', 'give', 'the', 'movie', 'an', 'audience', 'rating', ',', 'you', \"'ll\", 'learn', 'more', 'about', 'N', 'LP', 'later', 'in', 'this', 'course', '.', 'Another', 'kind', 'of', 'machine', 'learning', 'that', \"'s\", 'been', 'gaining', 'popularity', 'recently', 'is', 'reinforcement', 'learning', '.', 'Unlike', 'other', 'machine', 'learning', 'reinforcement', 'learning', 'continuously', 'improves', 'its', 'model', 'by', 'mining', 'feedback', 'from', 'previous', 'iterations', '.', 'In', 'reinforcement', 'learning', ',', 'an', 'agent', 'continuously', 'learns', 'through', 'trial', 'and', 'error', 'as', 'it', 'interacts', 'in', 'an', 'environment', '.', 'Reinforcement', 'learning', 'is', 'broadly', 'useful', 'when', 'the', 'reward', 'of', 'a', 'desired', 'outcome', 'is', 'known', '.', 'But', 'the', 'path', 'to', 'achieving', 'it', 'is', \"n't\", '.', 'And', 'that', 'path', 'requires', 'a', 'lot', 'of', 'trial', 'and', 'error', 'to', 'discover', '.', 'Take', 'the', 'example', 'of', 'Aws', 'Deep', 'Racer', 'in', 'the', 'Aws', 'Deep', 'Racer', 'simulator', '.', 'The', 'agent', 'is', 'the', 'virtual', 'car', '.', 'The', 'environment', 'is', 'a', 'virtual', 'race', 'track', '.', 'The', 'actions', 'are', 'throttle', 'and', 'steering', 'inputs', 'to', 'the', 'car', '.', 'And', 'the', 'goal', 'is', 'completing', 'the', 'race', 'track', 'as', 'quickly', 'as', 'possible', '.', 'Without', 'deviating', 'from', 'the', 'track', '.', 'The', 'car', 'needs', 'to', 'learn', 'the', 'desired', 'driving', 'behavior', 'to', 'reach', 'the', 'goal', 'of', 'completing', 'the', 'track', 'for', 'the', 'car', 'to', 'learn', 'this', '.', 'Aws', 'Deep', 'racer', 'teams', 'use', 'rewards', 'to', 'incentivize', 'their', 'model', 'to', 'learn', 'the', 'desired', 'driving', 'behavior', '.', 'In', 'reinforcement', 'learning', 'the', 'thing', 'driving', ',', 'the', 'learning', 'is', 'called', 'the', 'agent', '.', 'In', 'this', 'case', ',', 'it', \"'s\", 'the', 'Aws', 'deep', 'racer', 'car', '.', 'The', 'environment', 'is', 'the', 'place', 'where', 'the', 'agent', 'learns', ',', 'which', 'in', 'this', 'example', 'would', 'be', 'the', 'marked', 'racetrack', '.', 'When', 'the', 'agent', 'does', 'something', 'in', 'the', 'environment', 'that', 'provokes', 'a', 'response', 'such', 'as', 'crossing', 'a', 'boundary', ',', 'it', 'should', \"n't\", 'cross', ',', 'that', \"'s\", 'called', 'an', 'action', ',', 'that', 'response', 'is', 'called', 'a', 'reward', 'or', 'penalty', '.', 'Depending', 'on', 'whether', 'the', 'agent', 'did', 'something', 'to', 'be', 'reinforced', 'or', 'discouraged', 'in', 'the', 'model', '.', 'As', 'the', 'agent', 'moves', 'within', 'the', 'environment', ',', 'its', 'action', 'should', 'start', 'receiving', 'more', 'rewards', 'and', 'fewer', 'penalties', 'until', 'it', 'meets', 'the', 'desired', 'business', 'outcome', '.', 'Self', 'driving', 'vehicles', ',', 'bring', 'together', 'many', 'machine', 'and', 'deep', 'learning', 'algorithms', 'and', 'models', 'to', 'solve', 'the', 'problem', 'of', 'driving', 'from', 'point', 'A', 'to', 'point', 'B', '.', 'Two', 'of', 'its', 'main', 'tasks', 'are', 'the', 'continuous', 'detection', 'of', 'the', 'environment', 'and', 'forecasting', 'changes', '.', 'These', 'involve', 'detecting', 'objects', 'and', 'localizing', 'and', 'predicting', 'the', 'movement', 'of', 'the', 'detected', 'objects', '.', 'The', 'outputs', 'of', 'these', 'findings', 'act', 'as', 'inputs', 'to', 'other', 'systems', 'that', 'make', 'decisions', 'on', 'what', 'they', 'should', 'do', 'with', 'the', 'vehicle', \"'s\", 'various', 'controls', '.', 'There', 'are', 'use', 'cases', 'in', 'self', 'driving', 'vehicles', 'that', 'require', 'real', 'time', 'responses', 'to', 'the', 'environment', '.', 'For', 'example', ',', 'if', 'a', 'previously', 'hidden', 'pedestrian', 'walks', 'out', 'from', 'behind', 'an', 'obstacle', ',', 'the', 'vehicle', 'brakes', 'need', 'to', 'be', 'applied', 'immediately', ',', 'there', 'can', 'be', 'no', 'latency', 'or', 'room', 'for', 'error', 'with', 'these', 'actions', '.', 'Not', 'every', 'problem', 'should', 'be', 'solved', 'with', 'machine', 'learning', '.', 'Sometimes', 'regular', 'programming', 'will', 'work', 'well', 'for', 'your', 'needs', '.', 'If', 'you', \"'re\", 'interested', 'in', 'exploring', 'a', 'potential', 'machine', 'learning', 'solution', ',', 'look', 'for', 'the', 'existence', 'of', 'large', 'data', 'sets', 'and', 'a', 'large', 'number', 'of', 'variables', '.', 'Machine', 'learning', 'is', 'often', 'the', 'best', 'choice', 'if', 'you', \"'re\", 'uncertain', 'of', 'the', 'business', 'logic', 'or', 'procedures', 'needed', 'to', 'obtain', 'an', 'answer', 'or', 'accomplish', 'a', 'task', ',', 'machine', 'learning', 'systems', 'can', 'be', 'complex', '.', 'The', 'supporting', 'infrastructure', 'management', 'support', 'and', 'technical', 'expertise', 'need', 'to', 'be', 'in', 'place', 'to', 'help', 'ensure', 'the', 'project', \"'s\", 'success', '.', 'Here', 'are', 'the', 'key', 'takeaways', 'for', 'this', 'section', 'where', 'we', 'explored', 'some', 'machine', 'learning', 'applications', 'that', 'are', 'already', 'part', 'of', 'everyday', 'life', '.', 'First', ',', 'machine', 'learning', 'problems', 'can', 'be', 'grouped', 'into', 'three', 'categories', '.', 'Supervised', 'learning', 'is', 'where', 'you', 'have', 'training', 'data', 'where', 'you', 'already', 'know', 'the', 'answer', ',', 'unsupervised', 'learning', 'is', 'where', 'you', 'have', 'data', 'but', 'are', 'looking', 'for', 'insights', 'within', 'the', 'data', 'reinforcement', 'learning', 'is', 'where', 'the', 'model', 'learns', 'based', 'on', 'experience', 'and', 'feedback', '.', 'Most', 'business', 'problems', 'are', 'supervised', 'learning', 'problems', '.', 'Well', ',', 'that', \"'s\", 'it', 'for', 'this', 'section', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome back in this section . We 're going to look at the type of business problem . Machine learning can help you solve . Machine learning is used all across your digital life . Your email spam filter is the result of a machine learning program that wa trained with example of spam and regular email message based on book . You 're reading or product you bought machine learning program can predict other book or product you 're likely to be interested in . Again , the machine learning program wa trained with data from other reader habit and purchase . When detecting credit card fraud , the machine learning program wa trained on example of transaction that turned out to be fraud along with normal transaction . You can probably think of many more example from social medium application using facial detection to group your photo to detecting brain tumor in brain scan or finding anomaly in x ray . There are three main type of machine learning . There 's supervised learning where a model us known input and output to generalize future output . There 's unsupervised learning where the model doe n't know input or output . So it find pattern in the data without help . And there 's reinforcement learning where the model interacts with it environment and learns to take action that will maximize reward . It 's important to know the different type of M L because the type will guide you towards selecting algorithm that make sense for solving your business problem . Let 's look more into each of these type . Supervised learning is a popular type of M L because it 's widely applicable , it 's called supervised learning because there need to be a supervisor , a teacher who can show the right answer , so to speak . Like any student , a supervised algorithm need to learn . By example , essentially it need a teacher who us training data to help it determine the pattern and relationship between the input and output . If you want to build an application to detect credit card fraud , you 'd need training data that includes example of fraud and example of normal transaction within supervised learning . There are different type of problem , classification and regression . There are two subtypes of classification problem . The first is binary classification . Think back to the example with identifying fraudulent transaction , the target variable in this example is limited to two option , fraudulent or not fraudulent . This is a binary classification problem . There are also multi class classification problem . These M L problem classify an observation into one of three or more category . Say that you have an M L model that predicts why a customer is calling your store . So you can reduce the number of transfer needed before the customer get to the correct customer support department . In this case , the different customer support department represent the variety of potential target variable , which could be many different department much more than just two . There are also regression problem in a regression problem . You 're no longer mapping an input to a defined number of category . Instead you 're mapping an input to a continuous value like an integer . One example of an M L regression problem is predicting the price of a company 's stock . Computer vision is a good example of supervised learning . Is this a cat or a dog ? Is there a tumor in this x-ray computer vision is often built with deep learning model . It automates the extraction analysis , classification and understanding of useful information from a single image or a sequence of image . Computer vision enables machine to identify people , place and thing and image with accuracy at or above human level and with greater speed and efficiency . The image data can take many form such a single image , video sequence , view from multiple camera or three dimensional data . You 'll learn more about computer vision . Later in this course , we 'll now discus unsupervised machine learning . Sometimes all you have is the data . There 's no supervisor in the room in unsupervised learning label are n't provided like they are with supervised learning . You do n't know all the variable and pattern in these instance . The machine ha to uncover and create the label itself . These model use the data they 're presented with to detect emerging property of the entire data set . Then they construct pattern from those property . Clustering is a common subcategory of unsupervised learning . This kind of algorithm group data into different cluster based on similar feature . It doe this to better understand the attribute of a specific cluster . For example , by analyzing customer purchasing habit , unsupervised algorithm can identify group of customer that are associated with the size tier of a company . The advantage of unsupervised algorithm is that they enable you to see pattern in the data that you were n't aware of before natural language processing is also known a N LP . This is another area of machine learning that 's experiencing growth . If you 've ever used Alexa or any other voice assistant , they 'll use N LP to try and answer your question . N LP is n't just about speech , it 's also about written text . N LP show up in many application . For example , N LP is used with chat or call center bot which are automated system that help you get your bank balance or order food from a restaurant . You can use N LP and translation tool which convert text between language . For example , you might use application that translate menu in real time . N LP is also used in voice to text translation which convert spoken word into text . And finally NLP can be used in sentiment analysis which you can use to analyze the sentiment of comment and review of product , music and movie . These sentiment could be used to give the movie an audience rating , you 'll learn more about N LP later in this course . Another kind of machine learning that 's been gaining popularity recently is reinforcement learning . Unlike other machine learning reinforcement learning continuously improves it model by mining feedback from previous iteration . In reinforcement learning , an agent continuously learns through trial and error a it interacts in an environment . Reinforcement learning is broadly useful when the reward of a desired outcome is known . But the path to achieving it is n't . And that path requires a lot of trial and error to discover . Take the example of Aws Deep Racer in the Aws Deep Racer simulator . The agent is the virtual car . The environment is a virtual race track . The action are throttle and steering input to the car . And the goal is completing the race track a quickly a possible . Without deviating from the track . The car need to learn the desired driving behavior to reach the goal of completing the track for the car to learn this . Aws Deep racer team use reward to incentivize their model to learn the desired driving behavior . In reinforcement learning the thing driving , the learning is called the agent . In this case , it 's the Aws deep racer car . The environment is the place where the agent learns , which in this example would be the marked racetrack . When the agent doe something in the environment that provokes a response such a crossing a boundary , it should n't cross , that 's called an action , that response is called a reward or penalty . Depending on whether the agent did something to be reinforced or discouraged in the model . As the agent move within the environment , it action should start receiving more reward and fewer penalty until it meet the desired business outcome . Self driving vehicle , bring together many machine and deep learning algorithm and model to solve the problem of driving from point A to point B . Two of it main task are the continuous detection of the environment and forecasting change . These involve detecting object and localizing and predicting the movement of the detected object . The output of these finding act a input to other system that make decision on what they should do with the vehicle 's various control . There are use case in self driving vehicle that require real time response to the environment . For example , if a previously hidden pedestrian walk out from behind an obstacle , the vehicle brake need to be applied immediately , there can be no latency or room for error with these action . Not every problem should be solved with machine learning . Sometimes regular programming will work well for your need . If you 're interested in exploring a potential machine learning solution , look for the existence of large data set and a large number of variable . Machine learning is often the best choice if you 're uncertain of the business logic or procedure needed to obtain an answer or accomplish a task , machine learning system can be complex . The supporting infrastructure management support and technical expertise need to be in place to help ensure the project 's success . Here are the key takeaway for this section where we explored some machine learning application that are already part of everyday life . First , machine learning problem can be grouped into three category . Supervised learning is where you have training data where you already know the answer , unsupervised learning is where you have data but are looking for insight within the data reinforcement learning is where the model learns based on experience and feedback . Most business problem are supervised learning problem . Well , that 's it for this section . We 'll see you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', '.', 'This', 'is', 'section', 'three', 'and', 'we', \"'re\", 'going', 'to', 'give', 'you', 'a', 'quick', 'high', 'level', 'overview', 'of', 'machine', 'learning', 'terminology', 'and', 'a', 'typical', 'workflow', '.', 'We', 'will', 'cover', 'these', 'topics', 'in', 'more', 'detail', 'later', 'in', 'this', 'course', '.', 'But', 'for', 'now', ',', 'we', \"'ll\", 'focus', 'on', 'the', 'larger', 'picture', '.', 'So', 'to', 'begin', ',', 'you', 'should', 'always', 'start', 'with', 'the', 'business', 'problem', '.', 'You', 'or', 'your', 'team', 'believe', 'could', 'benefit', 'from', 'machine', 'learning', 'from', 'there', '.', 'You', 'want', 'to', 'do', 'some', 'problem', 'formulation', 'in', 'this', 'phase', '.', 'One', 'task', 'is', 'to', 'articulate', 'your', 'business', 'problem', 'and', 'convert', 'it', 'to', 'an', 'M', 'L', 'problem', '.', 'After', 'you', \"'ve\", 'formulated', 'the', 'problem', ',', 'you', 'move', 'on', 'to', 'the', 'data', 'preparation', 'and', 'preprocessing', 'phase', '.', 'You', \"'ll\", 'pull', 'data', 'from', 'one', 'or', 'more', 'data', 'sources', '.', 'These', 'data', 'sources', 'might', 'have', 'differences', 'in', 'data', 'or', 'types', 'that', 'need', 'to', 'be', 'reconciled', '.', 'So', 'you', 'can', 'form', 'a', 'single', 'cohesive', 'view', 'of', 'your', 'data', '.', 'You', \"'ll\", 'need', 'to', 'visualize', 'your', 'data', 'and', 'use', 'statistics', 'to', 'determine', 'if', 'the', 'data', 'is', 'consistent', 'and', 'can', 'be', 'used', 'for', 'machine', 'learning', '.', 'We', \"'ll\", 'look', 'at', 'some', 'of', 'the', 'data', 'sources', 'later', 'in', 'the', 'course', '.', 'This', 'example', 'data', 'has', 'four', 'columns', 'containing', 'data', 'from', 'three', 'different', 'data', 'sources', '.', 'The', 'sources', 'had', 'slightly', 'different', 'ways', 'of', 'representing', 'data', 'and', 'the', 'results', 'are', 'shown', 'in', 'the', 'table', '.', 'In', 'ML', 'problems', ',', 'columns', 'represent', 'features', 'and', 'rows', 'represent', 'instances', '.', 'There', 'are', 'some', 'issues', 'here', 'with', 'the', 'data', '.', 'In', 'some', 'of', 'the', 'instances', '.', 'In', 'some', 'cases', ',', 'you', \"'ll\", 'need', 'a', 'subject', 'matter', 'expert', 'or', 'a', 'functional', 'expert', 'to', 'understand', 'the', 'authenticity', 'of', 'the', 'data', '.', 'For', 'example', ',', 'the', 'date', 'that', \"'s\", 'represented', 'as', '11', 'to', '1969', 'could', 'be', 'November', '2nd', 'or', 'February', '11th', 'in', 'the', 'year', '1969', 'someone', 'who', 'owns', 'or', 'manages', 'the', 'data', 'pool', 'would', 'be', 'able', 'to', 'clarify', 'this', 'ambiguity', '.', 'Also', 'the', 'word', 'mail', 'can', 'probably', 'be', 'attributed', 'to', 'an', 'import', 'issue', 'where', 'cells', 'shifted', 'position', ',', 'but', 'there', 'could', 'be', 'an', 'outside', 'chance', 'where', 'it', \"'s\", 'the', 'actual', 'location', ',', 'male', ',', 'a', 'city', 'that', \"'s\", 'the', 'capital', 'of', 'the', 'Republic', 'of', 'Maldives', 'at', 'times', '.', 'This', 'error', 'identification', 'is', \"n't\", 'as', 'simple', 'and', 'you', \"'ll\", 'need', 'an', 'sme', 'to', 'review', 'the', 'data', '.', 'You', \"'ll\", 'learn', 'about', 'the', 'role', 'of', 'experts', 'later', 'in', 'this', 'course', '.', 'Remember', 'that', 'one', 'of', 'the', 'largest', 'impacts', 'you', 'can', 'have', 'on', 'the', 'success', 'of', 'a', 'machine', 'learning', 'project', 'is', 'to', 'have', 'consistent', 'and', 'correct', 'data', '.', 'After', 'your', 'data', 'is', 'in', 'good', 'shape', ',', 'it', \"'s\", 'time', 'to', 'train', 'your', 'model', '.', 'This', 'is', 'where', 'the', 'process', 'gets', 'very', 'iterative', 'and', 'fluid', '.', 'You', \"'ll\", 'likely', 'go', 'through', 'many', 'multiple', 'passes', 'of', 'feature', 'engineering', 'training', ',', 'evaluating', 'and', 'tuning', 'before', 'you', 'find', 'a', 'model', 'that', 'meets', 'your', 'business', 'goals', '.', 'Feature', 'engineering', 'is', 'the', 'process', 'of', 'selecting', 'or', 'creating', 'the', 'features', '.', 'Your', 'model', 'will', 'be', 'trained', 'with', 'features', 'are', 'the', 'columns', 'of', 'data', 'you', 'have', 'in', 'your', 'data', 'set', '.', 'The', 'goal', 'of', 'the', 'model', 'is', 'to', 'correctly', 'estimate', 'the', 'target', 'value', 'for', 'the', 'new', 'data', '.', 'The', 'M', 'L', 'algorithm', 'will', 'use', 'the', 'features', 'to', 'predict', 'the', 'target', '.', 'In', 'this', 'example', ',', 'the', 'target', 'data', 'is', 'the', 'average', 'number', 'of', 'steps', 'taken', 'in', 'a', 'week', '.', 'Selecting', 'the', 'correct', 'features', 'can', 'involve', 'adding', ',', 'removing', 'or', 'calculating', 'new', 'features', '.', 'You', 'might', 'want', 'to', 'make', 'the', 'data', 'formats', 'consistent', ',', 'the', 'consistent', 'formats', 'could', 'be', 'later', 'used', 'in', 'the', 'model', 'or', 'you', 'can', 'make', 'these', 'changes', 'for', 'cosmetic', 'reasons', '.', 'Depending', 'on', 'the', 'problem', ',', 'you', 'want', 'to', 'solve', 'this', 'data', ',', 'you', 'might', 'not', 'even', 'need', 'to', 'include', 'the', 'name', 'feature', 'in', 'the', 'example', 'data', '.', 'What', 'about', 'the', 'country', 'feature', '?', 'If', 'this', 'were', 'a', 'traditional', 'database', ',', 'you', 'might', 'want', 'to', 'move', 'country', 'to', 'a', 'look', 'up', 'table', ',', 'then', 'reference', 'it', '.', 'Most', 'M', 'L', 'algorithms', 'want', 'the', 'data', 'for', 'an', 'instance', 'in', 'a', 'single', 'row', 'M', 'L', 'algorithms', 'need', 'numerical', 'data', 'to', 'process', '.', 'You', 'could', 'consider', 'turning', 'the', 'country', 'text', 'into', 'the', 'country', \"'s\", 'iso', 'code', '.', 'However', ',', 'the', 'model', 'might', 'interpret', 'the', 'numerical', 'value', 'as', 'having', 'meaning', '.', 'So', 'the', 'UK', 'S', 'iso', 'code', 'value', 'of', '44', 'would', 'be', 'more', 'significant', 'than', 'the', 'iso', 'code', 'value', 'of', 'the', 'US', ',', 'which', 'is', '01', '.', 'In', 'this', 'case', ',', 'splitting', 'the', 'data', 'into', 'multiple', 'columns', 'is', 'fine', '.', 'This', 'is', 'known', 'as', 'categorical', 'encoding', 'and', 'you', \"'ll\", 'learn', 'about', 'this', 'later', 'in', 'the', 'course', '.', 'For', 'other', 'types', 'of', 'data', ',', 'you', 'could', 'convert', 'the', 'text', 'value', 'into', 'a', 'numerical', 'value', '.', 'For', 'example', ',', 'you', 'could', 'use', 'zero', 'or', 'one', 'to', 'represent', 'male', 'or', 'female', '.', 'These', 'numeric', 'values', 'can', 'be', 'used', 'more', 'easily', 'by', 'the', 'model', '.', 'What', 'about', 'the', 'remaining', 'features', 'like', 'age', ',', 'birth', 'month', ',', 'which', 'is', 'shown', 'as', 'B', 'M', 'in', 'the', 'table', 'or', 'day', 'of', 'week', ',', 'which', 'is', 'shown', 'as', 'D', 'O', 'W', 'extracting', 'the', 'age', 'birth', 'month', 'and', 'day', 'of', 'week', 'might', 'be', 'appropriate', 'depending', 'on', 'the', 'problem', 'you', \"'re\", 'trying', 'to', 'solve', '.', 'Does', 'age', 'impact', 'the', 'target', 'variable', '?', 'What', 'about', 'which', 'day', 'of', 'the', 'week', 'they', 'were', 'born', 'on', '?', 'Do', \"n't\", 'worry', 'if', 'this', 'sounds', 'complicated', ',', 'you', \"'ll\", 'learn', 'more', 'about', 'feature', 'engineering', 'later', 'in', 'this', 'course', 'after', 'your', 'data', 'is', 'cleaned', 'and', 'you', \"'ve\", 'identified', 'the', 'features', 'you', 'want', 'to', 'use', '.', 'It', \"'s\", 'time', 'to', 'train', 'a', 'model', '.', 'You', 'wo', \"n't\", 'use', 'all', 'the', 'data', 'to', 'train', 'your', 'model', '.', 'In', 'fact', ',', 'you', 'need', 'to', 'hold', 'some', 'of', 'the', 'data', '.', 'So', 'you', 'have', 'some', 'data', 'to', 'test', 'with', '.', 'Typically', 'you', \"'ll\", 'use', 'about', '80', '%', 'of', 'the', 'data', 'to', 'train', 'with', 'and', 'you', \"'ll\", 'save', 'the', 'rest', 'of', 'the', 'data', 'for', 'testing', 'next', '.', 'You', \"'ll\", 'train', 'a', 'model', 'with', 'training', 'data', 'in', 'the', 'diagram', '.', 'The', 'model', 'uses', 'the', 'X', 'G', 'boost', 'algorithm', '.', 'The', 'model', 'itself', 'has', 'some', 'parameters', '.', 'You', 'can', 'set', 'these', 'parameters', 'will', 'alter', 'how', 'the', 'algorithm', 'works', 'and', 'they', \"'re\", 'known', 'as', 'hyper', 'parameters', '.', 'The', 'output', 'of', 'the', 'training', 'job', 'will', 'be', 'a', 'trained', 'model', 'with', 'the', 'trained', 'model', '.', 'You', 'can', 'use', 'some', 'of', 'the', 'test', 'data', 'to', 'see', 'how', 'well', 'the', 'model', 'performs', '.', 'You', \"'ll\", 'take', 'an', 'instance', ',', 'the', 'model', 'has', \"n't\", 'seen', 'and', 'use', 'it', 'to', 'perform', 'a', 'prediction', 'because', 'you', 'already', 'know', 'the', 'target', 'in', 'your', 'test', 'data', '.', 'You', 'can', 'compare', 'the', 'two', 'values', 'from', 'these', 'comparisons', '.', 'You', 'can', 'calculate', 'metrics', 'which', 'give', 'you', 'data', 'on', 'how', 'well', 'the', 'model', 'is', 'performing', '.', 'You', \"'ll\", 'then', 'make', 'changes', 'to', 'the', 'model', \"'s\", 'data', 'features', 'or', 'hyper', 'parameters', 'until', 'you', 'find', 'the', 'model', 'that', 'yields', 'the', 'best', 'results', '.', 'When', 'training', 'your', 'model', ',', 'there', \"'s\", 'a', 'real', 'danger', 'of', 'overfitting', 'or', 'under', 'fitting', 'the', 'model', ',', 'your', 'model', 'is', 'overfitting', 'your', 'training', 'data', '.', 'When', 'you', 'see', 'the', 'model', 'performs', 'well', 'on', 'the', 'training', 'data', 'but', 'does', \"n't\", 'perform', 'well', 'on', 'the', 'evaluation', 'data', '.', 'This', 'is', 'because', 'the', 'model', 'is', 'memorizing', 'the', 'data', 'it', 'saw', 'and', 'ca', \"n't\", 'generalize', 'to', 'unseen', 'examples', '.', 'Your', 'model', 'is', 'under', 'fitting', 'the', 'training', 'data', 'when', 'the', 'model', 'performs', 'poorly', 'on', 'the', 'training', 'data', '.', 'This', 'is', 'because', 'the', 'model', 'ca', \"n't\", 'capture', 'the', 'relationship', 'between', 'the', 'input', 'examples', ',', 'which', 'are', 'often', 'called', 'X', 'and', 'the', 'target', 'values', ',', 'which', 'are', 'often', 'called', 'Y', '.', 'Understanding', 'model', 'fit', 'is', 'important', 'for', 'understanding', 'the', 'root', 'cause', 'of', 'poor', 'model', 'accuracy', '.', 'This', 'understanding', 'will', 'guide', 'you', 'to', 'take', 'corrective', 'steps', '.', 'You', 'can', 'determine', 'whether', 'a', 'predictive', 'model', 'is', 'under', 'fitting', 'or', 'overfitting', 'the', 'training', 'data', 'by', 'looking', 'at', 'the', 'prediction', 'error', 'on', 'the', 'training', 'data', 'and', 'the', 'evaluation', 'data', ',', 'we', \"'ll\", 'show', 'you', 'steps', 'you', 'can', 'take', 'to', 'avoid', 'this', 'later', 'in', 'this', 'course', '.', 'After', 'you', \"'ve\", 'retrained', 'the', 'model', 'and', 'you', \"'re\", 'satisfied', 'with', 'the', 'results', ',', 'you', 'deploy', 'your', 'model', 'to', 'deliver', 'the', 'best', 'possible', 'predictions', '.', 'Later', 'in', 'this', 'course', ',', 'we', \"'ll\", 'walk', 'you', 'through', 'these', 'different', 'phases', 'and', 'give', 'you', 'hands', 'on', 'experience', 'with', 'each', 'of', 'them', '.', 'But', 'knowing', 'the', 'process', 'is', 'also', 'useful', 'when', 'using', 'the', 'managed', 'services', 'will', 'also', 'explore', 'later', 'where', 'certain', 'Amazon', 'M', 'L', 'services', 'will', 'do', 'the', 'bulk', 'of', 'the', 'work', 'for', 'you', '.', 'Here', 'are', 'some', 'of', 'the', 'key', 'takeaways', 'for', 'this', 'section', '.', 'First', ',', 'we', 'looked', 'at', 'how', 'the', 'machine', 'learning', 'pipeline', 'process', 'can', 'guide', 'you', 'through', 'the', 'process', 'of', 'training', 'and', 'evaluating', 'a', 'model', '.', 'The', 'iterative', 'process', 'can', 'be', 'broken', 'into', 'three', 'broad', 'steps', ',', 'data', 'processing', 'model', 'training', 'and', 'model', 'evaluation', '.', 'That', \"'s\", 'it', 'for', 'this', 'video', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'one', '.']\n",
      "Hi and welcome back . This is section three and we 're going to give you a quick high level overview of machine learning terminology and a typical workflow . We will cover these topic in more detail later in this course . But for now , we 'll focus on the larger picture . So to begin , you should always start with the business problem . You or your team believe could benefit from machine learning from there . You want to do some problem formulation in this phase . One task is to articulate your business problem and convert it to an M L problem . After you 've formulated the problem , you move on to the data preparation and preprocessing phase . You 'll pull data from one or more data source . These data source might have difference in data or type that need to be reconciled . So you can form a single cohesive view of your data . You 'll need to visualize your data and use statistic to determine if the data is consistent and can be used for machine learning . We 'll look at some of the data source later in the course . This example data ha four column containing data from three different data source . The source had slightly different way of representing data and the result are shown in the table . In ML problem , column represent feature and row represent instance . There are some issue here with the data . In some of the instance . In some case , you 'll need a subject matter expert or a functional expert to understand the authenticity of the data . For example , the date that 's represented a 11 to 1969 could be November 2nd or February 11th in the year 1969 someone who owns or manages the data pool would be able to clarify this ambiguity . Also the word mail can probably be attributed to an import issue where cell shifted position , but there could be an outside chance where it 's the actual location , male , a city that 's the capital of the Republic of Maldives at time . This error identification is n't a simple and you 'll need an sme to review the data . You 'll learn about the role of expert later in this course . Remember that one of the largest impact you can have on the success of a machine learning project is to have consistent and correct data . After your data is in good shape , it 's time to train your model . This is where the process get very iterative and fluid . You 'll likely go through many multiple pass of feature engineering training , evaluating and tuning before you find a model that meet your business goal . Feature engineering is the process of selecting or creating the feature . Your model will be trained with feature are the column of data you have in your data set . The goal of the model is to correctly estimate the target value for the new data . The M L algorithm will use the feature to predict the target . In this example , the target data is the average number of step taken in a week . Selecting the correct feature can involve adding , removing or calculating new feature . You might want to make the data format consistent , the consistent format could be later used in the model or you can make these change for cosmetic reason . Depending on the problem , you want to solve this data , you might not even need to include the name feature in the example data . What about the country feature ? If this were a traditional database , you might want to move country to a look up table , then reference it . Most M L algorithm want the data for an instance in a single row M L algorithm need numerical data to process . You could consider turning the country text into the country 's iso code . However , the model might interpret the numerical value a having meaning . So the UK S iso code value of 44 would be more significant than the iso code value of the US , which is 01 . In this case , splitting the data into multiple column is fine . This is known a categorical encoding and you 'll learn about this later in the course . For other type of data , you could convert the text value into a numerical value . For example , you could use zero or one to represent male or female . These numeric value can be used more easily by the model . What about the remaining feature like age , birth month , which is shown a B M in the table or day of week , which is shown a D O W extracting the age birth month and day of week might be appropriate depending on the problem you 're trying to solve . Does age impact the target variable ? What about which day of the week they were born on ? Do n't worry if this sound complicated , you 'll learn more about feature engineering later in this course after your data is cleaned and you 've identified the feature you want to use . It 's time to train a model . You wo n't use all the data to train your model . In fact , you need to hold some of the data . So you have some data to test with . Typically you 'll use about 80 % of the data to train with and you 'll save the rest of the data for testing next . You 'll train a model with training data in the diagram . The model us the X G boost algorithm . The model itself ha some parameter . You can set these parameter will alter how the algorithm work and they 're known a hyper parameter . The output of the training job will be a trained model with the trained model . You can use some of the test data to see how well the model performs . You 'll take an instance , the model ha n't seen and use it to perform a prediction because you already know the target in your test data . You can compare the two value from these comparison . You can calculate metric which give you data on how well the model is performing . You 'll then make change to the model 's data feature or hyper parameter until you find the model that yield the best result . When training your model , there 's a real danger of overfitting or under fitting the model , your model is overfitting your training data . When you see the model performs well on the training data but doe n't perform well on the evaluation data . This is because the model is memorizing the data it saw and ca n't generalize to unseen example . Your model is under fitting the training data when the model performs poorly on the training data . This is because the model ca n't capture the relationship between the input example , which are often called X and the target value , which are often called Y . Understanding model fit is important for understanding the root cause of poor model accuracy . This understanding will guide you to take corrective step . You can determine whether a predictive model is under fitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data , we 'll show you step you can take to avoid this later in this course . After you 've retrained the model and you 're satisfied with the result , you deploy your model to deliver the best possible prediction . Later in this course , we 'll walk you through these different phase and give you hand on experience with each of them . But knowing the process is also useful when using the managed service will also explore later where certain Amazon M L service will do the bulk of the work for you . Here are some of the key takeaway for this section . First , we looked at how the machine learning pipeline process can guide you through the process of training and evaluating a model . The iterative process can be broken into three broad step , data processing model training and model evaluation . That 's it for this video . We 'll see you in the next one .\n",
      "['Welcome', 'back', 'in', 'this', 'section', '.', 'We', \"'ll\", 'look', 'at', 'some', 'of', 'the', 'tools', 'you', \"'ll\", 'be', 'using', 'throughout', 'the', 'rest', 'of', 'this', 'course', '.', 'Before', 'we', 'start', 'this', 'list', 'is', \"n't\", 'an', 'exhaustive', 'list', 'of', 'all', 'the', 'tools', 'available', 'today', '.', 'We', \"'re\", 'really', 'going', 'to', 'cover', 'them', 'at', 'a', 'high', 'level', ',', 'but', 'it', \"'s\", 'a', 'good', 'place', 'to', 'get', 'started', '.', 'First', ',', 'there', \"'s\", 'the', 'Jupiter', 'notebook', '.', 'The', 'Jupiter', 'notebook', 'is', 'an', 'open', 'source', 'web', 'application', 'you', 'can', 'use', 'to', 'create', 'and', 'share', 'documents', 'that', 'contain', 'live', 'code', 'equations', ',', 'visualizations', 'and', 'narrative', 'text', 'uses', 'include', 'data', 'cleaning', 'and', 'transformation', ',', 'numerical', 'simulation', ',', 'statistical', 'modeling', ',', 'data', 'visualization', ',', 'machine', 'learning', '.', 'And', 'much', 'more', '.', 'Jupiter', 'Lab', 'is', 'a', 'web', 'based', 'interactive', 'development', 'environment', 'for', 'Jupiter', 'notebooks', 'code', 'and', 'data', '.', 'Jupiter', 'lab', 'is', 'flexible', '.', 'You', 'can', 'use', 'it', 'to', 'configure', 'and', 'arrange', 'the', 'user', 'interface', 'to', 'support', 'a', 'wide', 'range', 'of', 'workflows', 'in', 'data', 'science', ',', 'scientific', 'computing', 'and', 'machine', 'learning', '.', 'Jupiter', 'lab', 'is', 'extensible', 'and', 'modular', '.', 'You', 'can', 'write', 'plug', 'ins', 'that', 'add', 'new', 'components', 'and', 'integrate', 'with', 'existing', 'ones', 'later', 'in', 'this', 'course', ',', 'you', \"'ll\", 'use', 'Amazon', 'Sagemaker', 'which', 'hosts', 'both', 'Jupiter', 'notebooks', 'and', 'Jupiter', 'Lab', '.', 'Pandas', 'is', 'an', 'open', 'source', 'Python', 'library', '.', 'It', \"'s\", 'used', 'for', 'data', 'handling', 'and', 'analysis', '.', 'Pandas', 'represents', 'data', 'in', 'a', 'table', 'similar', 'to', 'a', 'spreadsheet', '.', 'This', 'table', 'is', 'known', 'as', 'a', 'pandas', 'data', 'frame', 'mat', 'plot', 'lib', 'is', 'a', 'Python', 'library', 'for', 'creating', 'scientific', 'static', 'animated', 'and', 'interactive', 'visualizations', '.', 'In', 'Python', ',', 'you', \"'ll\", 'use', 'it', 'to', 'generate', 'plots', 'of', 'your', 'data', 'later', 'in', 'this', 'course', '.', 'Seaborne', 'is', 'another', 'data', 'visualization', 'library', 'for', 'Python', 'that', \"'s\", 'built', 'on', 'map', 'plot', 'lib', '.', 'It', 'provides', 'a', 'high', 'level', 'interface', 'for', 'drawing', 'attractive', 'and', 'informative', 'statistical', 'graphs', '.', 'Numb', 'Pi', 'is', 'one', 'of', 'the', 'fundamental', 'scientific', 'computing', 'packages', 'in', 'Python', '.', 'It', 'contains', 'functions', 'for', 'n', 'dimensional', 'array', 'objects', '.', 'It', 'also', 'has', 'useful', 'math', 'functions', 'such', 'as', 'linear', 'algebra', 'four', 'year', 'transform', 'and', 'random', 'number', 'capabilities', '.', 'Side', 'kit', 'learn', 'is', 'an', 'open', 'source', 'machine', 'learning', 'library', 'that', 'supports', 'supervised', 'and', 'unsupervised', 'learning', '.', 'It', 'also', 'provides', 'various', 'tools', 'for', 'model', 'fitting', 'data', ',', 'preprocessing', 'model', 'selection', 'and', 'evaluation', 'and', 'many', 'other', 'utilities', 'Psy', 'Pit', 'Learn', 'is', 'built', 'on', 'numpy', 'sci', 'pi', 'and', 'map', 'plot', 'lib', '.', 'It', \"'s\", 'a', 'good', 'tool', 'for', 'exploring', 'machine', 'learning', '.', 'Although', 'you', \"'ll\", 'only', 'use', 'it', 'to', 'borrow', 'a', 'few', 'functions', 'in', 'this', 'course', ',', 'you', 'might', 'want', 'to', 'consider', 'exploring', 'it', 'after', 'you', 'complete', 'this', 'course', '.', 'Moving', 'up', 'from', 'individual', 'libraries', 'and', 'packages', '.', 'There', 'are', 'also', 'tools', 'that', 'contain', 'production', 'ready', 'frameworks', '.', 'We', 'already', 'mentioned', 'side', 'kit', 'learn', ',', 'which', 'is', 'a', 'good', 'library', 'for', 'machine', 'learning', '.', 'The', 'frameworks', 'supported', 'on', 'AWS', 'such', 'as', 'tensor', 'flow', 'and', 'Caris', 'also', 'include', 'libraries', 'you', 'can', 'use', 'for', 'machine', 'learning', '.', 'All', 'the', 'frameworks', 'listed', 'here', 'are', 'supported', 'on', 'AWS', 'and', 'can', 'be', 'used', 'from', 'Amazon', '.', 'Sagemaker', '.', 'Aws', 'also', 'provides', 'compute', 'instances', 'that', 'are', 'tuned', 'for', 'machine', 'learning', 'in', 'both', 'the', 'cloud', 'and', 'at', 'the', 'edge', 'compute', 'instances', 'can', 'be', 'optimized', 'for', 'learning', 'and', 'inference', '.', 'Another', 'AWS', 'resource', 'you', 'can', 'use', 'are', 'certain', 'Amazon', 'machine', 'images', 'or', 'Ami', 'S', 'we', 'offer', 'prepackaged', 'AMI', 'S', 'that', 'contain', 'many', 'of', 'the', 'popular', 'frameworks', '.', 'Finally', ',', 'there', \"'s\", 'Amazon', \"'s\", 'Sagemaker', 'which', 'is', 'an', 'AWS', 'service', 'with', 'many', 'capabilities', '.', 'First', ',', 'Sagemaker', 'can', 'deploy', 'machine', 'learning', 'instances', ',', 'running', 'Jupiter', 'notebooks', 'and', 'Jupiter', 'lab', '.', 'It', 'manages', 'the', 'deployment', 'of', 'these', 'compute', 'resources', '.', 'So', 'you', 'only', 'need', 'to', 'connect', 'to', 'the', 'Jupiter', 'environment', '.', 'Sagemaker', 'also', 'provides', 'tools', 'for', 'labeling', 'data', 'training', 'models', 'and', 'hosting', 'trained', 'models', '.', 'Aws', 'marketplace', 'also', 'provides', 'a', 'selection', 'of', 'ready', 'to', 'use', 'model', 'packages', 'and', 'algorithms', 'from', 'third', 'party', 'machine', 'learning', 'developers', '.', 'Aws', 'also', 'provides', 'a', 'set', 'of', 'managed', 'machine', 'learning', 'servicess', 'and', 'you', 'can', 'integrate', 'them', 'into', 'your', 'applications', 'even', 'if', 'you', 'do', \"n't\", 'have', 'substantial', 'machine', 'learning', 'experience', 'for', 'computer', 'vision', '.', 'Amazon', 'recognition', 'provides', 'object', 'and', 'facial', 'recognition', 'for', 'both', 'image', 'and', 'video', '.', 'Also', ',', 'Amazon', 'textt', 'can', 'extract', 'text', 'from', 'images', '.', 'Speech', 'services', 'include', 'Amazon', 'Polly', 'which', 'can', 'speak', 'text', '.', 'Another', 'speech', 'service', 'is', 'Amazon', 'transcribed', 'which', 'converts', 'spoken', 'audio', 'to', 'text', 'for', 'language', '.', 'Amazon', 'comprehend', 'uses', 'N', 'LP', 'to', 'find', 'insights', 'and', 'relationships', 'in', 'text', '.', 'Also', ',', 'Amazon', 'translate', 'can', 'translate', 'text', 'into', 'different', 'languages', 'if', 'you', 'want', 'to', 'work', 'with', 'chat', 'bots', ',', 'Amazon', 'Lex', 'helps', 'you', 'build', 'interactive', 'conversational', 'applications', 'that', 'use', 'voice', 'or', 'text', 'for', 'forecasting', '.', 'Amazon', 'forecast', 'uses', 'machine', 'learning', 'to', 'combine', 'time', 'series', 'data', 'with', 'additional', 'variables', '.', 'So', 'you', 'can', 'build', 'forecasts', '.', 'And', 'finally', ',', 'if', 'you', \"'d\", 'like', 'to', 'work', 'with', 'recommendations', ',', 'Amazon', 'personalized', 'can', 'help', 'you', 'create', 'individual', 'personalized', 'recommendations', 'for', 'customers', '.', 'These', 'managed', 'services', 'have', 'already', 'been', 'trained', 'in', 'many', 'aspects', 'of', 'the', 'problem', 'domain', '.', 'You', 'only', 'need', 'to', 'provide', 'your', 'specific', 'data', 'to', 'get', 'started', '.', 'We', \"'re\", 'going', 'to', 'look', 'at', 'many', 'of', 'these', 'managed', 'services', 'in', 'the', 'second', 'half', 'of', 'this', 'course', '.', 'After', 'you', 'learn', 'how', 'to', 'do', 'things', 'on', 'your', 'own', ',', 'the', 'key', 'takeaways', 'for', 'this', 'section', 'include', 'these', 'points', '.', 'First', ',', 'Python', 'is', 'the', 'most', 'popular', 'language', 'for', 'performing', 'machine', 'learning', 'tasks', '.', 'Jupiter', 'notebooks', 'provide', 'you', 'with', 'a', 'web', 'based', 'hosted', 'development', 'environment', 'for', 'machine', 'learning', '.', 'You', \"'ll\", 'use', 'Jupiter', 'notebooks', 'frequently', 'in', 'machine', 'learning', '.', 'There', 'are', 'a', 'large', 'number', 'of', 'open', 'source', 'tools', 'such', 'as', 'Penda', 'that', 'you', \"'ll\", 'use', 'often', 'as', 'a', 'machine', 'learning', 'practitioner', '.', 'Finally', ',', 'depending', 'upon', 'your', 'requirements', ',', 'you', 'might', 'start', 'with', 'low', 'level', 'frameworks', 'to', 'create', 'your', 'own', 'solution', '.', 'You', 'might', 'also', 'use', 'tools', 'such', 'as', 'Amazon', 'Sagemaker', 'to', 'help', 'with', 'some', 'of', 'the', 'heavy', 'lifting', 'or', 'you', 'could', 'simply', 'use', 'and', 'adapt', 'one', 'of', 'the', 'managed', 'Amazon', 'ML', 'services', 'for', 'your', 'specific', 'problem', 'domain', '.', 'That', \"'s\", 'it', 'for', 'this', 'video', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'one', '.']\n",
      "Welcome back in this section . We 'll look at some of the tool you 'll be using throughout the rest of this course . Before we start this list is n't an exhaustive list of all the tool available today . We 're really going to cover them at a high level , but it 's a good place to get started . First , there 's the Jupiter notebook . The Jupiter notebook is an open source web application you can use to create and share document that contain live code equation , visualization and narrative text us include data cleaning and transformation , numerical simulation , statistical modeling , data visualization , machine learning . And much more . Jupiter Lab is a web based interactive development environment for Jupiter notebook code and data . Jupiter lab is flexible . You can use it to configure and arrange the user interface to support a wide range of workflow in data science , scientific computing and machine learning . Jupiter lab is extensible and modular . You can write plug in that add new component and integrate with existing one later in this course , you 'll use Amazon Sagemaker which host both Jupiter notebook and Jupiter Lab . Pandas is an open source Python library . It 's used for data handling and analysis . Pandas represents data in a table similar to a spreadsheet . This table is known a a panda data frame mat plot lib is a Python library for creating scientific static animated and interactive visualization . In Python , you 'll use it to generate plot of your data later in this course . Seaborne is another data visualization library for Python that 's built on map plot lib . It provides a high level interface for drawing attractive and informative statistical graph . Numb Pi is one of the fundamental scientific computing package in Python . It contains function for n dimensional array object . It also ha useful math function such a linear algebra four year transform and random number capability . Side kit learn is an open source machine learning library that support supervised and unsupervised learning . It also provides various tool for model fitting data , preprocessing model selection and evaluation and many other utility Psy Pit Learn is built on numpy sci pi and map plot lib . It 's a good tool for exploring machine learning . Although you 'll only use it to borrow a few function in this course , you might want to consider exploring it after you complete this course . Moving up from individual library and package . There are also tool that contain production ready framework . We already mentioned side kit learn , which is a good library for machine learning . The framework supported on AWS such a tensor flow and Caris also include library you can use for machine learning . All the framework listed here are supported on AWS and can be used from Amazon . Sagemaker . Aws also provides compute instance that are tuned for machine learning in both the cloud and at the edge compute instance can be optimized for learning and inference . Another AWS resource you can use are certain Amazon machine image or Ami S we offer prepackaged AMI S that contain many of the popular framework . Finally , there 's Amazon 's Sagemaker which is an AWS service with many capability . First , Sagemaker can deploy machine learning instance , running Jupiter notebook and Jupiter lab . It manages the deployment of these compute resource . So you only need to connect to the Jupiter environment . Sagemaker also provides tool for labeling data training model and hosting trained model . Aws marketplace also provides a selection of ready to use model package and algorithm from third party machine learning developer . Aws also provides a set of managed machine learning services and you can integrate them into your application even if you do n't have substantial machine learning experience for computer vision . Amazon recognition provides object and facial recognition for both image and video . Also , Amazon textt can extract text from image . Speech service include Amazon Polly which can speak text . Another speech service is Amazon transcribed which convert spoken audio to text for language . Amazon comprehend us N LP to find insight and relationship in text . Also , Amazon translate can translate text into different language if you want to work with chat bot , Amazon Lex help you build interactive conversational application that use voice or text for forecasting . Amazon forecast us machine learning to combine time series data with additional variable . So you can build forecast . And finally , if you 'd like to work with recommendation , Amazon personalized can help you create individual personalized recommendation for customer . These managed service have already been trained in many aspect of the problem domain . You only need to provide your specific data to get started . We 're going to look at many of these managed service in the second half of this course . After you learn how to do thing on your own , the key takeaway for this section include these point . First , Python is the most popular language for performing machine learning task . Jupiter notebook provide you with a web based hosted development environment for machine learning . You 'll use Jupiter notebook frequently in machine learning . There are a large number of open source tool such a Penda that you 'll use often a a machine learning practitioner . Finally , depending upon your requirement , you might start with low level framework to create your own solution . You might also use tool such a Amazon Sagemaker to help with some of the heavy lifting or you could simply use and adapt one of the managed Amazon ML service for your specific problem domain . That 's it for this video . We 'll see you in the next one .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'This', 'is', 'section', 'five', 'and', 'we', \"'re\", 'going', 'to', 'discuss', 'challenges', 'with', 'machine', 'learning', '.', 'You', \"'ll\", 'come', 'across', 'many', 'challenges', 'in', 'machine', 'learning', '.', 'There', 'are', 'a', 'lot', 'of', 'poor', 'quality', 'and', 'inconsistent', 'data', 'available', '.', 'A', 'significant', 'portion', 'of', 'your', 'job', 'will', 'be', 'getting', 'access', 'to', 'or', 'generating', 'enough', 'good', 'data', '.', 'That', \"'s\", 'representative', 'of', 'the', 'problem', '.', 'You', 'want', 'to', 'solve', 'a', 'key', 'issue', 'to', 'watch', 'out', 'for', 'is', 'under', 'or', 'overfitting', 'the', 'model', '.', 'It', \"'s\", 'not', 'all', 'about', 'the', 'data', 'although', 'it', 'mostly', 'is', '.', 'Do', 'you', 'have', 'data', 'science', 'experience', '?', 'Is', 'staffing', 'a', 'team', 'of', 'data', 'scientists', 'cost', 'effective', '.', 'Does', 'management', 'support', 'using', 'machine', 'learning', '?', 'What', 'does', 'the', 'business', 'landscape', 'look', 'like', '?', 'Are', 'the', 'problems', 'too', 'complex', 'to', 'formulate', 'into', 'a', 'machine', 'learning', 'problem', '?', 'Can', 'the', 'resulting', 'model', 'be', 'explained', 'to', 'the', 'business', '?', 'If', 'it', 'ca', \"n't\", 'be', 'explained', ',', 'it', 'might', 'not', 'get', 'adopted', '.', 'What', \"'s\", 'the', 'cost', 'of', 'building', 'updating', 'and', 'operating', 'a', 'machine', 'learning', 'solution', '?', 'Finally', ',', 'how', 'does', 'the', 'technology', 'map', '?', 'Does', 'the', 'business', 'unit', 'have', 'access', 'to', 'the', 'data', 'that', \"'s\", 'needed', '?', 'Can', 'the', 'data', 'be', 'secured', 'to', 'meet', 'any', 'regulatory', 'requirements', '?', 'What', 'tools', 'and', 'frameworks', 'will', 'be', 'used', '?', 'How', 'will', 'the', 'solution', 'integrate', 'with', 'other', 'systems', '?', 'These', 'are', 'important', 'questions', 'to', 'be', 'successful', ',', 'you', \"'ll\", 'need', 'to', 'be', 'able', 'to', 'answer', 'and', 'address', 'them', '.', 'Many', 'machine', 'learning', 'problems', 'can', 'be', 'solved', 'today', 'by', 'using', 'existing', 'models', 'and', 'without', 'substantial', 'machine', 'learning', 'knowledge', ',', 'we', \"'ve\", 'already', 'talked', 'about', 'the', 'Aws', 'Managed', 'services', 'for', 'machine', 'learning', '.', 'You', 'can', 'add', 'sophisticated', 'machine', 'learning', 'capabilities', 'to', 'your', 'applications', '.', 'With', 'only', 'some', 'basic', 'developer', 'skills', 'for', 'calling', 'API', 'S.', 'There', 'are', 'other', 'prebuilt', 'models', 'you', 'can', 'use', 'or', 'adapt', '.', 'One', 'example', 'is', 'YOLO', ',', 'which', 'means', 'you', 'only', 'look', 'once', 'YOLO', 'is', 'a', 'popular', 'computer', 'vision', 'model', '.', 'In', 'addition', 'to', 'these', 'scenarios', ',', 'you', 'can', 'use', 'the', 'AWS', 'marketplace', 'if', 'you', \"'d\", 'prefer', 'to', 'buy', 'models', 'and', 'services', 'from', 'independent', 'software', 'vendors', 'instead', 'of', 'developing', 'your', 'own', '.', 'Here', 'are', 'the', 'key', 'takeaways', 'for', 'this', 'section', '.', 'First', ',', 'you', \"'ll\", 'face', 'many', 'machine', 'learning', 'challenges', '.', 'The', 'biggest', 'ones', 'that', 'you', 'can', 'directly', 'influence', 'are', 'related', 'to', 'data', ',', 'you', 'should', 'consider', 'managed', 'services', 'to', 'solve', 'machine', 'learning', 'problems', 'within', 'the', 'domains', 'they', 'support', 'such', 'as', 'using', 'Amazon', 'recognition', 'for', 'computer', 'vision', 'problems', '.', 'That', \"'s\", 'it', '.', 'For', 'this', 'section', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . This is section five and we 're going to discus challenge with machine learning . You 'll come across many challenge in machine learning . There are a lot of poor quality and inconsistent data available . A significant portion of your job will be getting access to or generating enough good data . That 's representative of the problem . You want to solve a key issue to watch out for is under or overfitting the model . It 's not all about the data although it mostly is . Do you have data science experience ? Is staffing a team of data scientist cost effective . Does management support using machine learning ? What doe the business landscape look like ? Are the problem too complex to formulate into a machine learning problem ? Can the resulting model be explained to the business ? If it ca n't be explained , it might not get adopted . What 's the cost of building updating and operating a machine learning solution ? Finally , how doe the technology map ? Does the business unit have access to the data that 's needed ? Can the data be secured to meet any regulatory requirement ? What tool and framework will be used ? How will the solution integrate with other system ? These are important question to be successful , you 'll need to be able to answer and address them . Many machine learning problem can be solved today by using existing model and without substantial machine learning knowledge , we 've already talked about the Aws Managed service for machine learning . You can add sophisticated machine learning capability to your application . With only some basic developer skill for calling API S. There are other prebuilt model you can use or adapt . One example is YOLO , which mean you only look once YOLO is a popular computer vision model . In addition to these scenario , you can use the AWS marketplace if you 'd prefer to buy model and service from independent software vendor instead of developing your own . Here are the key takeaway for this section . First , you 'll face many machine learning challenge . The biggest one that you can directly influence are related to data , you should consider managed service to solve machine learning problem within the domain they support such a using Amazon recognition for computer vision problem . That 's it . For this section . We 'll see you in the next video .\n",
      "['It', \"'s\", 'now', 'time', 'to', 'review', 'the', 'module', '.', 'Here', 'are', 'the', 'main', 'takeaways', 'for', 'this', 'module', '.', 'First', ',', 'we', 'looked', 'at', 'defining', 'machine', 'learning', 'and', 'how', 'it', 'fits', 'into', 'the', 'broader', 'A', 'I', 'landscape', '.', 'We', 'also', 'looked', 'at', 'the', 'types', 'of', 'problem', 'machine', 'learning', 'can', 'help', 'us', 'solve', 'and', 'how', 'machine', 'learning', 'applies', 'learning', 'algorithms', 'to', 'develop', 'models', 'from', 'large', 'data', 'sets', '.', 'We', 'then', 'looked', 'at', 'the', 'machine', 'learning', 'pipeline', 'and', 'the', 'different', 'stages', 'for', 'developing', 'a', 'machine', 'learning', 'application', '.', 'Finally', ',', 'we', 'introduce', 'some', 'of', 'the', 'tools', 'and', 'services', 'you', 'can', 'use', 'before', 'discussing', 'some', 'of', 'the', 'challenges', 'with', 'machine', 'learning', '.', 'In', 'summary', ',', 'in', 'this', 'module', ',', 'you', 'learned', 'how', 'to', 'recognize', 'how', 'machine', 'learning', 'and', 'deep', 'learning', 'are', 'part', 'of', 'artificial', 'intelligence', '.', 'Describe', 'artificial', 'intelligence', 'and', 'machine', 'learning', 'terminology', 'identify', 'how', 'machine', 'learning', 'can', 'be', 'used', 'to', 'solve', 'a', 'business', 'problem', '.', 'Describe', 'the', 'machine', 'learning', 'process', 'list', 'the', 'tools', 'available', 'to', 'data', '.', 'Scientists', 'identify', 'when', 'to', 'use', 'machine', 'learning', 'instead', 'of', 'traditional', 'software', 'development', 'methods', '.', 'Thanks', 'for', 'watching', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "It 's now time to review the module . Here are the main takeaway for this module . First , we looked at defining machine learning and how it fit into the broader A I landscape . We also looked at the type of problem machine learning can help u solve and how machine learning applies learning algorithm to develop model from large data set . We then looked at the machine learning pipeline and the different stage for developing a machine learning application . Finally , we introduce some of the tool and service you can use before discussing some of the challenge with machine learning . In summary , in this module , you learned how to recognize how machine learning and deep learning are part of artificial intelligence . Describe artificial intelligence and machine learning terminology identify how machine learning can be used to solve a business problem . Describe the machine learning process list the tool available to data . Scientists identify when to use machine learning instead of traditional software development method . Thanks for watching , we 'll see you in the next video .\n",
      "['Welcome', 'back', 'to', 'Aws', 'Academy', 'of', 'Machine', 'learning', '.', 'This', 'is', 'module', 'three', 'and', 'we', \"'re\", 'going', 'to', 'work', 'through', 'the', 'entire', 'machine', 'learning', 'pipeline', 'by', 'using', 'Amazon', 'Sage', 'Maker', '.', 'This', 'module', 'will', 'discuss', 'a', 'typical', 'process', 'for', 'handling', 'a', 'machine', 'learning', 'problem', '.', 'The', 'machine', 'learning', 'pipeline', 'can', 'be', 'applied', 'to', 'many', 'machine', 'learning', 'problems', '.', 'The', 'focus', 'is', 'on', 'supervised', 'learning', 'but', 'the', 'process', 'you', 'learn', 'in', 'this', 'module', 'can', 'be', 'adapted', 'to', 'other', 'types', 'of', 'machine', 'learning', 'as', 'well', '.', 'This', 'is', 'a', 'large', 'module', 'and', 'we', \"'ll\", 'be', 'covering', 'a', 'lot', 'of', 'material', '.', 'At', 'the', 'end', 'of', 'this', 'module', ',', 'you', \"'ll\", 'be', 'able', 'to', 'formulate', 'a', 'problem', 'from', 'a', 'business', 'request', ',', 'obtain', 'and', 'secure', 'data', 'for', 'machine', 'learning', ',', 'build', 'a', 'Jupiter', 'notebook', 'by', 'using', 'Amazon', 'Sagemaker', 'outline', 'the', 'process', 'for', 'evaluating', 'data', '.', 'Explain', 'why', 'data', 'needs', 'to', 'be', 'pre', 'processed', ',', 'use', 'open', 'source', 'tools', 'to', 'examine', 'and', 'prep', 'process', 'data', '.', 'Use', 'Amazon', 'Sagemaker', 'to', 'train', 'and', 'host', 'a', 'machine', 'learning', 'model', '.', 'Use', 'cross', 'validation', 'to', 'test', 'the', 'performance', 'of', 'a', 'machine', 'learning', 'model', ',', 'use', 'a', 'hosted', 'model', 'for', 'inference', 'and', 'finally', 'create', 'an', 'Amazon', 'Sagemaker', 'hyper', 'parameter', 'tuning', 'job', 'to', 'optimize', 'a', 'model', \"'s\", 'effectiveness', '.', 'We', \"'re\", 'ready', 'to', 'get', 'started', '.', 'See', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Welcome back to Aws Academy of Machine learning . This is module three and we 're going to work through the entire machine learning pipeline by using Amazon Sage Maker . This module will discus a typical process for handling a machine learning problem . The machine learning pipeline can be applied to many machine learning problem . The focus is on supervised learning but the process you learn in this module can be adapted to other type of machine learning a well . This is a large module and we 'll be covering a lot of material . At the end of this module , you 'll be able to formulate a problem from a business request , obtain and secure data for machine learning , build a Jupiter notebook by using Amazon Sagemaker outline the process for evaluating data . Explain why data need to be pre processed , use open source tool to examine and prep process data . Use Amazon Sagemaker to train and host a machine learning model . Use cross validation to test the performance of a machine learning model , use a hosted model for inference and finally create an Amazon Sagemaker hyper parameter tuning job to optimize a model 's effectiveness . We 're ready to get started . See you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', 'to', 'module', 'three', '.', 'This', 'is', 'section', 'one', 'and', 'we', \"'re\", 'going', 'to', 'take', 'a', 'look', 'at', 'some', 'of', 'the', 'data', 'sets', 'we', \"'ll\", 'use', 'in', 'this', 'module', '.', 'We', \"'ll\", 'also', 'look', 'at', 'guidance', 'for', 'how', 'to', 'formulate', 'a', 'business', 'problem', 'before', 'we', 'get', 'started', '.', 'Here', \"'s\", 'a', 'reminder', 'of', 'the', 'machine', 'learning', 'pipeline', 'we', 'looked', 'at', 'in', 'the', 'previous', 'module', 'and', 'how', 'that', 'maps', 'to', 'the', 'sections', 'in', 'this', 'module', '.', 'This', 'section', 'section', 'one', 'will', 'cover', 'how', 'to', 'formulate', 'a', 'problem', '.', 'It', 'will', 'also', 'cover', 'the', 'data', 'sets', 'we', \"'ll\", 'use', 'throughout', 'this', 'module', '.', 'Section', 'two', 'will', 'discuss', 'how', 'to', 'obtain', 'and', 'secure', 'data', 'for', 'your', 'machine', 'learning', 'activities', '.', 'In', 'section', 'three', ',', 'we', \"'ll\", 'show', 'you', 'tools', 'and', 'techniques', 'for', 'gaining', 'an', 'understanding', 'of', 'your', 'data', '.', 'Then', 'in', 'section', 'four', ',', 'we', \"'ll\", 'look', 'at', 'pre', 'processing', 'your', 'data', '.', 'So', 'it', \"'s\", 'ready', 'to', 'train', 'a', 'model', '.', 'Section', 'five', 'will', 'cover', 'selecting', 'and', 'training', 'an', 'appropriate', 'machine', 'learning', 'model', '.', 'Section', 'six', 'will', 'show', 'you', 'how', 'to', 'deploy', 'a', 'model', '.', 'So', 'you', 'can', 'make', 'a', 'prediction', '.', 'Section', 'seven', 'will', 'examine', 'the', 'process', 'of', 'evaluating', 'the', 'performance', 'of', 'a', 'machine', 'learning', 'module', '.', 'And', 'finally', ',', 'in', 'section', 'eight', ',', 'we', \"'ll\", 'look', 'at', 'tuning', 'the', 'model', ',', 'the', 'machine', 'learning', 'pipeline', 'is', 'an', 'iterative', 'process', '.', 'When', 'you', 'work', 'on', 'a', 'real', 'world', 'problem', ',', 'you', 'might', 'find', 'yourself', 'iterating', 'many', 'times', 'before', 'you', 'arrive', 'at', 'a', 'solution', 'that', 'meets', 'your', 'business', \"'\", 'needs', '.', 'In', 'this', 'first', 'section', ',', 'we', \"'ll\", 'examine', 'how', 'to', 'think', 'about', 'turning', 'a', 'business', 'requirement', 'into', 'a', 'machine', 'learning', 'problem', '.', 'The', 'first', 'step', 'in', 'this', 'phase', 'is', 'to', 'simply', 'define', 'the', 'problem', 'you', 'want', 'to', 'solve', '.', 'And', 'the', 'goal', 'you', 'want', 'to', 'reach', 'understanding', 'the', 'business', 'goal', 'is', 'key', 'because', 'you', \"'ll\", 'use', 'it', 'to', 'measure', 'the', 'performance', 'of', 'your', 'solution', '.', 'It', \"'s\", 'not', 'unusual', 'to', 'solidify', 'the', 'business', 'problem', 'before', 'you', 'can', 'begin', 'targeting', 'a', 'solution', '.', 'There', 'are', 'a', 'lot', 'of', 'other', 'questions', 'you', 'could', 'ask', 'to', 'develop', 'a', 'good', 'understanding', 'of', 'the', 'problem', 'with', 'more', 'information', 'about', 'the', 'problem', '.', 'You', 'can', 'begin', 'framing', 'an', 'approach', '.', 'First', ',', 'can', 'the', 'problem', 'even', 'be', 'solved', 'by', 'machine', 'learning', 'or', 'would', 'a', 'traditional', 'approach', 'make', 'more', 'sense', '?', 'Is', 'this', 'a', 'supervised', 'or', 'unsupervised', 'machine', 'learning', 'problem', '?', 'Do', 'you', 'have', 'labeled', 'data', 'to', 'train', 'a', 'supervised', 'model', '?', 'There', 'are', 'many', 'questions', 'you', 'could', 'ask', 'yourself', 'and', 'the', 'business', 'ultimately', ',', 'you', 'should', 'try', 'to', 'validate', 'the', 'use', 'of', 'machine', 'learning', 'and', 'you', 'should', 'make', 'sure', 'you', 'have', 'access', 'to', 'the', 'right', 'people', 'and', 'data', '.', 'You', 'should', 'also', 'try', 'to', 'come', 'up', 'with', 'the', 'simplest', 'solution', 'to', 'the', 'problem', '.', 'Here', \"'s\", 'an', 'example', ',', 'you', 'want', 'to', 'identify', 'fraudulent', 'credit', 'card', 'transactions', 'so', 'you', 'can', 'stop', 'the', 'transaction', 'before', 'it', 'processes', '.', 'That', \"'s\", 'your', 'problem', '.', 'Now', ',', 'what', \"'s\", 'the', 'business', 'goal', 'or', 'outcome', 'driving', 'this', 'problem', 'statement', 'in', 'this', 'case', ',', 'say', 'that', 'the', 'intended', 'outcome', 'is', 'a', 'reduction', 'in', 'the', 'number', 'of', 'customers', 'who', 'end', 'their', 'membership', 'to', 'the', 'credit', 'card', 'as', 'a', 'result', 'of', 'a', 'fraudulent', 'transaction', '.', 'From', 'a', 'business', 'perspective', ',', 'how', 'do', 'you', 'define', 'success', 'given', 'this', 'problem', 'and', 'the', 'desired', 'outcome', '?', 'This', 'is', 'the', 'stage', 'where', 'you', 'need', 'to', 'move', 'from', 'qualitative', 'statements', 'to', 'quantitative', 'statements', 'that', 'can', 'be', 'easily', 'measured', '.', 'Continuing', 'with', 'the', 'example', ',', 'a', 'metric', 'you', 'could', 'use', 'to', 'define', 'success', 'with', '.', 'This', 'problem', 'might', 'be', 'a', '10', '%', 'reduction', 'in', 'the', 'number', 'of', 'customers', 'who', 'file', 'claims', 'for', 'fraudulent', 'transactions', 'within', 'a', 'six', 'month', 'period', '.', 'So', 'now', 'you', \"'ve\", 'defined', 'the', 'business', 'side', 'of', 'your', 'problem', '.', 'It', \"'s\", 'time', 'to', 'start', 'thinking', 'about', 'this', 'in', 'terms', 'of', 'your', 'machine', 'learning', 'model', 'itself', '.', 'What', \"'s\", 'the', 'actual', 'output', 'you', 'want', 'to', 'see', 'from', 'your', 'model', '?', 'You', 'want', 'to', 'be', 'specific', 'here', '.', 'It', 'should', 'be', 'a', 'statement', 'that', 'reflects', 'what', 'an', 'M', 'L', 'model', 'could', 'actually', 'output', '.', 'An', 'example', 'might', 'be', '.', 'The', 'model', 'will', 'output', 'whether', 'or', 'not', 'a', 'credit', 'card', 'transaction', 'is', 'fraudulent', 'or', 'not', 'fraudulent', '.', 'Now', 'that', 'you', 'know', 'what', 'you', 'want', 'your', 'M', 'L', 'model', 'to', 'actually', 'achieve', ',', 'you', 'can', 'use', 'this', 'information', 'to', 'determine', 'the', 'type', 'of', 'M', 'L', 'you', \"'re\", 'working', 'with', '.', 'If', 'you', 'have', 'historical', 'data', ',', 'where', 'customers', 'filed', 'reports', 'for', 'fraud', 'transactions', ',', 'you', 'can', 'use', 'this', 'data', 'for', 'your', 'machine', 'learning', 'purpose', '.', 'This', 'historical', 'data', 'falls', 'under', 'the', 'supervised', 'learning', 'approach', 'where', 'the', 'labels', 'are', 'already', 'defined', '.', 'Recall', 'from', 'earlier', 'in', 'this', 'course', 'that', 'supervised', 'ML', 'types', 'are', 'categorized', 'into', 'two', 'groups', ',', 'classification', 'and', 'regression', '.', 'In', 'the', 'credit', 'card', 'example', ',', 'the', 'desired', 'output', 'of', 'categorizing', 'a', 'transaction', 'is', 'fraud', 'or', 'not', 'fraud', '.', 'So', 'you', 'can', 'see', 'that', 'you', \"'re\", 'dealing', 'with', 'a', 'binary', 'classification', 'problem', 'throughout', 'this', 'module', '.', 'You', \"'ll\", 'see', 'several', 'data', 'sets', 'being', 'used', '.', 'You', 'can', 'access', 'these', 'data', 'sets', 'and', 'many', 'more', 'from', 'the', 'UC', 'Irvine', 'machine', 'learning', 'repository', '.', 'The', 'first', 'data', 'set', 'contains', 'numerical', 'information', 'about', 'the', 'composition', 'of', 'wine', 'along', 'with', 'the', 'quality', 'of', 'the', 'wine', '.', 'The', 'question', 'you', 'might', 'want', 'to', 'ask', 'on', 'this', 'data', 'set', 'is', 'based', 'on', 'the', 'composition', 'of', 'the', 'wine', '.', 'Could', 'we', 'predict', 'the', 'quality', 'and', 'therefore', 'the', 'price', '.', 'In', 'addition', 'to', 'that', 'question', ',', 'we', \"'ll\", 'also', 'use', 'this', 'data', 'set', 'to', 'view', 'statistics', 'deal', 'with', 'outliers', 'and', 'scale', 'numerical', 'data', '.', 'The', 'second', 'data', 'set', 'is', 'a', 'car', 'evaluation', 'database', '.', 'This', 'data', 'set', 'is', 'heavily', 'text', 'based', '.', 'This', 'enables', 'you', 'to', 'explore', 'the', 'encoding', 'categorical', 'data', 'which', 'converts', 'the', 'text', 'values', 'into', 'numbers', 'that', 'can', 'be', 'processed', 'by', 'machine', 'learning', '.', 'The', 'third', 'data', 'set', 'is', 'a', 'biomedical', 'data', 'set', 'which', 'you', \"'ll\", 'also', 'use', 'in', 'the', 'labs', '.', 'The', 'question', 'to', 'answer', 'for', 'this', 'data', 'set', 'is', 'based', 'on', 'the', 'biomechanical', 'features', '.', 'Can', 'you', 'predict', 'if', 'a', 'patient', 'has', 'an', 'abnormality', '?', 'This', 'data', 'set', 'will', 'take', 'you', 'through', 'the', 'entire', 'end', 'to', 'end', 'process', '.', 'You', \"'ll\", 'end', 'with', 'a', 'trained', 'model', 'that', \"'s\", 'been', 'tuned', 'and', 'that', 'you', 'can', 'use', 'to', 'make', 'a', 'prediction', '.', 'In', 'this', 'section', ',', 'we', 'looked', 'at', 'how', 'business', 'problems', 'need', 'to', 'be', 'converted', 'into', 'an', 'M', 'L', 'problem', '.', 'We', 'also', 'looked', 'at', 'some', 'of', 'the', 'key', 'questions', 'to', 'ask', ',', 'which', 'are', 'what', 'is', 'defining', 'success', '?', 'Can', 'you', 'measure', 'the', 'outcome', 'or', 'impact', 'if', 'your', 'solution', 'is', 'implemented', '?', 'Most', 'business', 'problems', 'fall', 'into', 'one', 'of', 'two', 'categories', '.', 'The', 'first', 'category', 'is', 'classification', 'which', 'can', 'be', 'binary', 'or', 'multi', 'class', '.', 'Ask', 'yourself', 'does', 'the', 'target', 'belong', 'to', 'a', 'class', '?', 'And', 'the', 'second', 'category', 'is', 'regression', 'for', 'this', '.', 'Ask', 'yourself', ',', 'can', 'you', 'predict', 'a', 'numerical', 'value', '?', 'That', \"'s\", 'it', '?', 'For', 'section', 'one', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome back to module three . This is section one and we 're going to take a look at some of the data set we 'll use in this module . We 'll also look at guidance for how to formulate a business problem before we get started . Here 's a reminder of the machine learning pipeline we looked at in the previous module and how that map to the section in this module . This section section one will cover how to formulate a problem . It will also cover the data set we 'll use throughout this module . Section two will discus how to obtain and secure data for your machine learning activity . In section three , we 'll show you tool and technique for gaining an understanding of your data . Then in section four , we 'll look at pre processing your data . So it 's ready to train a model . Section five will cover selecting and training an appropriate machine learning model . Section six will show you how to deploy a model . So you can make a prediction . Section seven will examine the process of evaluating the performance of a machine learning module . And finally , in section eight , we 'll look at tuning the model , the machine learning pipeline is an iterative process . When you work on a real world problem , you might find yourself iterating many time before you arrive at a solution that meet your business ' need . In this first section , we 'll examine how to think about turning a business requirement into a machine learning problem . The first step in this phase is to simply define the problem you want to solve . And the goal you want to reach understanding the business goal is key because you 'll use it to measure the performance of your solution . It 's not unusual to solidify the business problem before you can begin targeting a solution . There are a lot of other question you could ask to develop a good understanding of the problem with more information about the problem . You can begin framing an approach . First , can the problem even be solved by machine learning or would a traditional approach make more sense ? Is this a supervised or unsupervised machine learning problem ? Do you have labeled data to train a supervised model ? There are many question you could ask yourself and the business ultimately , you should try to validate the use of machine learning and you should make sure you have access to the right people and data . You should also try to come up with the simplest solution to the problem . Here 's an example , you want to identify fraudulent credit card transaction so you can stop the transaction before it process . That 's your problem . Now , what 's the business goal or outcome driving this problem statement in this case , say that the intended outcome is a reduction in the number of customer who end their membership to the credit card a a result of a fraudulent transaction . From a business perspective , how do you define success given this problem and the desired outcome ? This is the stage where you need to move from qualitative statement to quantitative statement that can be easily measured . Continuing with the example , a metric you could use to define success with . This problem might be a 10 % reduction in the number of customer who file claim for fraudulent transaction within a six month period . So now you 've defined the business side of your problem . It 's time to start thinking about this in term of your machine learning model itself . What 's the actual output you want to see from your model ? You want to be specific here . It should be a statement that reflects what an M L model could actually output . An example might be . The model will output whether or not a credit card transaction is fraudulent or not fraudulent . Now that you know what you want your M L model to actually achieve , you can use this information to determine the type of M L you 're working with . If you have historical data , where customer filed report for fraud transaction , you can use this data for your machine learning purpose . This historical data fall under the supervised learning approach where the label are already defined . Recall from earlier in this course that supervised ML type are categorized into two group , classification and regression . In the credit card example , the desired output of categorizing a transaction is fraud or not fraud . So you can see that you 're dealing with a binary classification problem throughout this module . You 'll see several data set being used . You can access these data set and many more from the UC Irvine machine learning repository . The first data set contains numerical information about the composition of wine along with the quality of the wine . The question you might want to ask on this data set is based on the composition of the wine . Could we predict the quality and therefore the price . In addition to that question , we 'll also use this data set to view statistic deal with outlier and scale numerical data . The second data set is a car evaluation database . This data set is heavily text based . This enables you to explore the encoding categorical data which convert the text value into number that can be processed by machine learning . The third data set is a biomedical data set which you 'll also use in the lab . The question to answer for this data set is based on the biomechanical feature . Can you predict if a patient ha an abnormality ? This data set will take you through the entire end to end process . You 'll end with a trained model that 's been tuned and that you can use to make a prediction . In this section , we looked at how business problem need to be converted into an M L problem . We also looked at some of the key question to ask , which are what is defining success ? Can you measure the outcome or impact if your solution is implemented ? Most business problem fall into one of two category . The first category is classification which can be binary or multi class . Ask yourself doe the target belong to a class ? And the second category is regression for this . Ask yourself , can you predict a numerical value ? That 's it ? For section one , we 'll see you in the next video .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'re\", 'now', 'going', 'to', 'look', 'at', 'a', 'few', 'ways', 'you', 'can', 'collect', 'and', 'secure', 'data', 'in', 'this', 'section', '.', 'We', \"'ll\", 'explore', 'some', 'of', 'the', 'techniques', 'and', 'challenges', 'associated', 'with', 'collecting', 'and', 'securing', 'the', 'data', 'that', \"'s\", 'needed', 'for', 'machine', 'learning', '.', 'Consider', 'again', 'the', 'original', 'example', 'about', 'predicting', 'credit', 'card', 'fraud', ',', 'you', \"'ve\", 'further', 'formulated', 'the', 'problem', '.', 'But', 'what', 'data', 'do', 'you', 'need', 'to', 'actually', 'train', 'your', 'model', 'so', 'you', 'can', 'get', 'the', 'desired', 'output', 'and', 'subsequently', 'achieve', 'your', 'intended', 'business', 'outcome', '.', 'Do', 'you', 'have', 'access', 'to', 'the', 'data', '?', 'If', 'so', ',', 'how', 'much', 'data', 'do', 'you', 'have', '?', 'And', 'where', 'is', 'it', '?', 'What', 'solution', 'can', 'you', 'use', 'to', 'bring', 'all', 'this', 'data', 'into', 'one', 'centralized', 'repository', '?', 'The', 'answers', 'to', 'these', 'questions', 'are', 'essential', 'at', 'this', 'stage', '.', 'The', 'good', 'news', 'for', 'a', 'budding', 'data', 'scientist', 'is', 'that', 'there', 'are', 'many', 'places', 'where', 'you', 'can', 'obtain', 'data', ',', 'private', 'data', 'from', 'you', 'or', 'your', 'existing', 'customer', 'already', 'exists', ',', 'including', 'everything', 'from', 'log', 'files', 'to', 'customer', 'invoice', 'databases', ',', 'private', 'data', 'can', 'be', 'useful', 'depending', 'on', 'the', 'problem', 'you', \"'re\", 'trying', 'to', 'solve', '.', 'In', 'many', 'cases', ',', 'private', 'data', 'is', 'found', 'in', 'many', 'different', 'systems', '.', 'We', \"'ll\", 'look', 'at', 'how', 'to', 'bring', 'these', 'sources', 'together', 'shortly', '.', 'Sometimes', 'you', 'want', 'to', 'use', 'data', 'that', 'was', 'collected', 'and', 'made', 'available', 'by', 'a', 'commercial', 'organization', 'companies', 'such', 'as', 'Reuters', ',', 'Change', 'Healthcare', ',', 'Dun', 'and', 'Bradstreet', 'and', 'Foursquare', 'Maintain', 'Databases', '.', 'You', 'can', 'subscribe', 'to', '.', 'They', 'include', 'curated', 'news', 'stories', ',', 'anonymized', 'health', 'care', 'transactions', ',', 'global', 'business', 'records', 'and', 'location', 'data', '.', 'If', 'you', 'supplement', 'your', 'own', 'data', 'with', 'commercial', 'data', ',', 'you', 'can', 'get', 'useful', 'insights', 'you', 'would', \"n't\", 'have', 'gotten', '.', 'Otherwise', ',', 'there', 'are', 'also', 'many', 'open', 'source', 'data', 'sets', 'ranging', 'from', 'wine', 'quality', 'to', 'movie', 'reviews', '.', 'These', 'data', 'sets', 'are', 'made', 'available', 'for', 'use', 'in', 'research', 'or', 'for', 'teaching', 'purposes', ',', 'Aws', 'Cale', 'and', 'the', 'U', 'C', 'I', 'machine', 'learning', 'repositories', 'are', 'good', 'places', 'to', 'find', 'open', 'source', 'data', 'sets', '.', 'Government', 'and', 'health', 'organizations', 'are', 'other', 'sources', 'of', 'data', 'that', 'could', 'be', 'useful', 'supervised', 'machine', 'learning', 'problems', 'need', 'a', 'lot', 'of', 'data', '.', 'These', 'are', 'also', 'called', 'observations', 'and', 'you', 'already', 'need', 'to', 'know', 'the', 'target', 'answer', 'or', 'prediction', 'for', 'that', 'data', '.', 'This', 'kind', 'of', 'data', 'where', 'you', 'already', 'know', 'the', 'target', 'answer', 'or', 'prediction', 'is', 'called', 'labeled', 'data', '.', 'Each', 'observation', 'in', 'your', 'data', 'is', 'made', 'up', 'of', 'two', 'elements', ',', 'the', 'target', 'and', 'the', 'features', ',', 'the', 'target', 'is', 'the', 'answer', 'you', 'want', 'to', 'predict', '.', 'So', 'in', 'the', 'credit', 'card', 'transaction', 'example', ',', 'the', 'target', 'of', 'any', 'given', 'observation', 'is', 'either', 'fraud', 'or', 'not', 'fraud', '.', 'A', 'feature', 'is', 'an', 'attribute', 'of', 'the', 'example', 'that', 'you', 'can', 'use', 'to', 'identify', 'patterns', 'for', 'predicting', 'the', 'target', 'answer', '.', 'A', 'feature', 'in', 'the', 'credit', 'card', 'example', 'could', 'be', 'the', 'date', 'of', 'the', 'transaction', ',', 'the', 'vendor', 'or', 'the', 'amount', 'in', 'dollars', 'of', 'the', 'transaction', '.', 'You', 'might', 'wonder', 'if', 'the', 'source', 'of', 'the', 'target', 'is', 'fraud', 'or', 'not', 'fraud', '.', 'Typically', ',', 'this', 'information', 'is', 'discovered', 'only', 'after', 'the', 'transaction', 'is', 'complete', 'and', 'the', 'actual', 'card', 'owner', 'notices', 'a', 'fraudulent', 'transaction', 'on', 'their', 'statement', '.', 'This', 'information', 'would', 'be', 'recorded', 'with', 'the', 'transaction', 'for', 'exactly', 'the', 'purpose', 'of', 'using', 'it', 'to', 'train', 'a', 'future', 'model', '.', 'So', 'given', 'what', 'you', 'know', 'about', 'the', 'elements', 'of', 'an', 'ML', 'data', 'set', ',', 'we', \"'ll\", 'return', 'to', 'one', 'of', 'the', 'original', 'questions', '.', 'What', 'data', 'do', 'you', 'need', 'to', 'actually', 'train', 'your', 'model', 'to', 'reach', 'the', 'desired', 'output', 'and', 'subsequently', 'your', 'intended', 'business', 'outcome', '.', 'This', 'is', 'an', 'example', 'of', 'a', 'stage', 'in', 'the', 'M', 'L', 'pipeline', '.', 'When', 'it', \"'s\", 'crucial', 'to', 'get', 'domain', 'expertise', 'to', 'help', 'you', 'answer', 'this', 'question', 'with', 'domain', 'knowledge', ',', 'you', 'can', 'start', 'determining', 'the', 'features', 'and', 'target', 'data', '.', 'Your', 'model', 'will', 'need', 'to', 'make', 'accurate', 'predictions', '.', 'Your', 'data', 'should', 'be', 'representative', 'of', 'the', 'data', 'you', \"'ll\", 'have', 'when', 'you', \"'re\", 'using', 'the', 'model', 'to', 'make', 'a', 'prediction', '.', 'For', 'example', ',', 'if', 'you', 'want', 'to', 'predict', 'credit', 'card', 'fraud', ',', 'you', 'need', 'to', 'collect', 'data', 'for', 'positive', 'or', 'fraudulent', 'transactions', '.', 'You', 'also', 'need', 'to', 'collect', 'data', 'for', 'negative', 'or', 'non', 'fraudulent', 'transactions', '.', 'You', 'need', 'both', 'types', 'of', 'data', '.', 'So', 'the', 'machine', 'learning', 'algorithm', 'can', 'find', 'patterns', 'that', 'will', 'distinguish', 'between', 'the', 'two', 'types', '.', 'Suppose', 'your', 'average', 'amount', 'of', 'fraudulent', 'transactions', 'is', 'actually', '3', '%', '.', 'But', 'your', 'training', 'data', 'set', 'only', 'includes', 'a', 'very', 'small', 'fraction', 'of', 'fraudulent', 'observations', 'say', '0.4', '%', '.', 'In', 'this', 'case', ',', 'it', 'will', 'be', 'difficult', 'for', 'your', 'model', 'to', 'truly', 'learn', 'patterns', 'related', 'to', 'fraudulent', 'transactions', 'that', 'it', 'might', 'encounter', 'in', 'production', '.', 'There', 'are', 'many', 'different', 'services', 'in', 'Aws', 'where', 'you', 'could', 'find', 'or', 'store', 'your', 'data', '.', 'Here', 'are', 'some', 'key', 'services', 'you', 'might', 'use', '.', 'Amazon', \"'s\", 'simple', 'storage', 'service', 'is', 'also', 'known', 'as', 'Amazon', 'S3', '.', 'It', 'provides', 'object', 'level', 'storage', 'with', 'S3', '.', 'You', 'can', 'store', 'as', 'much', 'data', 'as', 'you', 'want', 'in', 'the', 'form', 'of', 'objects', 'which', 'you', 'can', 'think', 'of', 'as', 'files', '.', 'They', 'could', 'be', 'CS', 'V', 'files', 'or', 'files', 'of', 'other', 'formats', 'you', 'need', 'S3', 'can', 'be', 'accessed', 'through', 'the', 'web', 'based', 'AWS', 'management', 'console', '.', 'You', 'can', 'also', 'access', 'S3', 'programmatically', 'through', 'the', 'API', 'and', 'S', 'D', 'K', 'S', 'or', 'with', 'third', 'party', 'solutions', ',', 'which', 'also', 'use', 'the', 'API', 'and', 'S', 'D', 'K', 'S.', 'If', 'your', 'training', 'data', 'is', 'already', 'in', 'S3', 'and', 'you', \"'re\", 'planning', 'to', 'run', 'training', 'jobs', 'several', 'times', 'with', 'different', 'algorithms', 'and', 'parameters', ',', 'you', 'could', 'use', 'Amazon', 'FSX', 'for', 'Luster', '.', 'It', \"'s\", 'a', 'file', 'system', 'service', 'that', 'speeds', 'up', 'your', 'training', 'jobs', 'by', 'serving', 'your', 'S3', 'data', 'to', 'Amazon', 'Sagemaker', 'at', 'high', 'speeds', '.', 'The', 'first', 'time', 'you', 'run', 'a', 'training', 'job', 'FSX', 'four', 'Luster', 'automatically', 'copies', 'data', 'from', 'S3', 'and', 'makes', 'it', 'available', 'to', 'sagemaker', '.', 'You', 'can', 'use', 'the', 'same', 'Amazon', 'FSX', 'file', 'system', 'for', 'subsequent', 'iterations', 'of', 'training', 'jobs', 'which', 'prevents', 'repeated', 'downloads', 'of', 'common', 'S3', 'objects', '.', 'Alternatively', ',', 'your', 'training', 'data', 'might', 'already', 'be', 'in', 'Amazon', 'Elastic', 'file', 'system', 'or', 'Amazon', 'Efs', '.', 'If', 'so', 'we', 'recommend', 'using', 'EFS', 'as', 'your', 'data', 'source', 'for', 'training', 'data', ',', 'it', 'can', 'launch', 'your', 'training', 'jobs', 'directly', 'from', 'the', 'service', 'without', 'needing', 'data', 'movement', ',', 'which', 'results', 'in', 'faster', 'training', 'start', 'times', '.', 'This', 'is', 'often', 'the', 'case', 'in', 'environments', 'where', 'data', 'scientists', 'have', 'home', 'directories', 'in', 'Amazon', 'EFS', ',', 'they', 'can', 'quickly', 'iterate', 'on', 'their', 'models', 'by', 'bringing', 'in', 'new', 'data', 'sharing', 'data', 'with', 'colleagues', 'and', 'experimenting', 'with', 'different', 'fields', 'or', 'labels', 'in', 'their', 'data', 'set', '.', 'For', 'example', ',', 'a', 'data', 'scientist', 'can', 'use', 'a', 'Jupiter', 'notebook', 'to', 'do', 'an', 'initial', 'cleansing', 'on', 'a', 'training', 'set', 'and', 'launch', 'a', 'training', 'job', 'from', 'Amazon', 'sagemaker', '.', 'They', 'could', 'then', 'use', 'their', 'Jupiter', 'notebook', 'to', 'drop', 'a', 'column', 'and', 'relaunch', 'the', 'training', 'job', 'and', 'finally', 'compare', 'the', 'resulting', 'models', 'to', 'see', 'which', 'one', 'works', 'better', '.', 'There', 'are', 'many', 'other', 'AWS', 'services', 'and', 'resources', 'where', 'you', 'might', 'find', 'data', '.', 'For', 'example', ',', 'you', 'could', 'use', 'Amazon', 'relational', 'database', 'service', 'or', 'Amazon', 'rds', ',', 'a', 'manage', 'relational', 'database', 'service', '.', 'You', 'could', 'also', 'use', 'Amazon', 'Redshift', 'which', 'is', 'a', 'managed', 'data', 'warehouse', 'service', '.', 'Another', 'option', 'is', 'Amazon', 'time', 'stream', ',', 'a', 'managed', 'time', 'series', 'database', 'designed', 'specifically', 'to', 'handle', 'large', 'amounts', 'of', 'data', 'from', 'the', 'internet', 'of', 'things', 'or', 'I', 'O', 'T.', 'You', 'could', 'even', 'spin', 'up', 'your', 'own', 'instances', 'on', 'Amazon', 'Elastic', 'compute', 'cloud', ',', 'which', 'is', 'also', 'known', 'as', 'Amazon', 'EC2', 'and', 'post', 'your', 'own', 'database', 'on', 'these', 'instances', '.', 'When', 'you', 'have', 'data', 'sources', ',', 'you', \"'ll\", 'need', 'to', 'extract', 'useful', 'data', 'from', 'these', 'sources', '.', 'When', 'assembling', 'your', 'data', 'for', 'machine', 'learning', '.', 'We', \"'ll\", 'look', 'at', 'this', 'next', ',', 'that', \"'s\", 'it', '.', 'For', 'part', 'one', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'two', 'where', 'we', \"'ll\", 'review', 'how', 'to', 'extract', ',', 'transform', 'and', 'load', 'data', '.']\n",
      "Hi , welcome back . We 're now going to look at a few way you can collect and secure data in this section . We 'll explore some of the technique and challenge associated with collecting and securing the data that 's needed for machine learning . Consider again the original example about predicting credit card fraud , you 've further formulated the problem . But what data do you need to actually train your model so you can get the desired output and subsequently achieve your intended business outcome . Do you have access to the data ? If so , how much data do you have ? And where is it ? What solution can you use to bring all this data into one centralized repository ? The answer to these question are essential at this stage . The good news for a budding data scientist is that there are many place where you can obtain data , private data from you or your existing customer already exists , including everything from log file to customer invoice database , private data can be useful depending on the problem you 're trying to solve . In many case , private data is found in many different system . We 'll look at how to bring these source together shortly . Sometimes you want to use data that wa collected and made available by a commercial organization company such a Reuters , Change Healthcare , Dun and Bradstreet and Foursquare Maintain Databases . You can subscribe to . They include curated news story , anonymized health care transaction , global business record and location data . If you supplement your own data with commercial data , you can get useful insight you would n't have gotten . Otherwise , there are also many open source data set ranging from wine quality to movie review . These data set are made available for use in research or for teaching purpose , Aws Cale and the U C I machine learning repository are good place to find open source data set . Government and health organization are other source of data that could be useful supervised machine learning problem need a lot of data . These are also called observation and you already need to know the target answer or prediction for that data . This kind of data where you already know the target answer or prediction is called labeled data . Each observation in your data is made up of two element , the target and the feature , the target is the answer you want to predict . So in the credit card transaction example , the target of any given observation is either fraud or not fraud . A feature is an attribute of the example that you can use to identify pattern for predicting the target answer . A feature in the credit card example could be the date of the transaction , the vendor or the amount in dollar of the transaction . You might wonder if the source of the target is fraud or not fraud . Typically , this information is discovered only after the transaction is complete and the actual card owner notice a fraudulent transaction on their statement . This information would be recorded with the transaction for exactly the purpose of using it to train a future model . So given what you know about the element of an ML data set , we 'll return to one of the original question . What data do you need to actually train your model to reach the desired output and subsequently your intended business outcome . This is an example of a stage in the M L pipeline . When it 's crucial to get domain expertise to help you answer this question with domain knowledge , you can start determining the feature and target data . Your model will need to make accurate prediction . Your data should be representative of the data you 'll have when you 're using the model to make a prediction . For example , if you want to predict credit card fraud , you need to collect data for positive or fraudulent transaction . You also need to collect data for negative or non fraudulent transaction . You need both type of data . So the machine learning algorithm can find pattern that will distinguish between the two type . Suppose your average amount of fraudulent transaction is actually 3 % . But your training data set only includes a very small fraction of fraudulent observation say 0.4 % . In this case , it will be difficult for your model to truly learn pattern related to fraudulent transaction that it might encounter in production . There are many different service in Aws where you could find or store your data . Here are some key service you might use . Amazon 's simple storage service is also known a Amazon S3 . It provides object level storage with S3 . You can store a much data a you want in the form of object which you can think of a file . They could be CS V file or file of other format you need S3 can be accessed through the web based AWS management console . You can also access S3 programmatically through the API and S D K S or with third party solution , which also use the API and S D K S. If your training data is already in S3 and you 're planning to run training job several time with different algorithm and parameter , you could use Amazon FSX for Luster . It 's a file system service that speed up your training job by serving your S3 data to Amazon Sagemaker at high speed . The first time you run a training job FSX four Luster automatically copy data from S3 and make it available to sagemaker . You can use the same Amazon FSX file system for subsequent iteration of training job which prevents repeated downloads of common S3 object . Alternatively , your training data might already be in Amazon Elastic file system or Amazon Efs . If so we recommend using EFS a your data source for training data , it can launch your training job directly from the service without needing data movement , which result in faster training start time . This is often the case in environment where data scientist have home directory in Amazon EFS , they can quickly iterate on their model by bringing in new data sharing data with colleague and experimenting with different field or label in their data set . For example , a data scientist can use a Jupiter notebook to do an initial cleansing on a training set and launch a training job from Amazon sagemaker . They could then use their Jupiter notebook to drop a column and relaunch the training job and finally compare the resulting model to see which one work better . There are many other AWS service and resource where you might find data . For example , you could use Amazon relational database service or Amazon rds , a manage relational database service . You could also use Amazon Redshift which is a managed data warehouse service . Another option is Amazon time stream , a managed time series database designed specifically to handle large amount of data from the internet of thing or I O T. You could even spin up your own instance on Amazon Elastic compute cloud , which is also known a Amazon EC2 and post your own database on these instance . When you have data source , you 'll need to extract useful data from these source . When assembling your data for machine learning . We 'll look at this next , that 's it . For part one of this section , we 'll see you again for part two where we 'll review how to extract , transform and load data .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'data', 'collection', 'by', 'reviewing', 'how', 'to', 'extract', ',', 'transform', 'and', 'load', 'data', 'data', 'is', 'typically', 'spread', 'across', 'many', 'different', 'systems', 'and', 'data', 'providers', '.', 'This', 'presents', 'a', 'challenge', '.', 'You', \"'ll\", 'need', 'to', 'bring', 'all', 'these', 'data', 'sources', 'together', 'into', 'something', 'that', 'can', 'be', 'consumed', 'by', 'a', 'machine', 'learning', 'model', '.', 'You', 'can', 'do', 'this', 'through', 'extract', ',', 'transform', 'and', 'load', ',', 'which', 'is', 'also', 'known', 'as', 'E', 'T', 'L.', 'The', 'steps', 'in', 'E', 'T', 'L', 'are', 'defined', 'this', 'way', 'in', 'the', 'extract', 'step', ',', 'you', 'pull', 'the', 'data', 'from', 'the', 'sources', 'to', 'a', 'single', 'location', '.', 'During', 'extraction', ',', 'you', 'might', 'need', 'to', 'modify', 'the', 'data', ',', 'combine', 'matching', 'records', 'or', 'do', 'other', 'tasks', 'that', 'transform', 'the', 'data', '.', 'Finally', '.', 'In', 'the', 'load', 'step', ',', 'the', 'data', 'is', 'loaded', 'into', 'a', 'repository', 'such', 'as', 'Amazon', 'S3', '.', 'A', 'typical', 'L', 'framework', 'has', 'several', 'components', '.', 'As', 'an', 'example', ',', 'consider', 'the', 'diagram', '.', 'First', ',', 'the', 'crawler', ',', 'a', 'program', 'connects', 'to', 'a', 'data', 'store', 'which', 'can', 'be', 'a', 'source', 'or', 'a', 'target', '.', 'It', 'progresses', 'through', 'a', 'ranked', 'list', 'of', 'classifiers', 'to', 'determine', 'the', 'schema', 'for', 'your', 'data', '.', 'Then', 'it', 'creates', 'metadata', 'tables', 'in', 'the', 'aws', 'glue', 'data', 'catalog', '.', 'A', 'job', 'defines', 'the', 'business', 'logic', 'that', \"'s\", 'needed', 'to', 'perform', 'E', 'T', 'L', 'work', 'to', 'run', 'the', 'job', 'you', \"'ll\", 'need', 'to', 'use', 'a', 'schedule', 'or', 'event', '.', 'As', 'a', 'final', 'note', ',', 'the', 'services', 'we', 'just', 'discussed', 'exist', 'in', 'the', 'transform', 'partition', 'of', 'the', 'E', 'T', 'L', 'process', '.', 'Aws', 'Glue', 'is', 'a', 'fully', 'managed', 'E', 'T', 'L', 'service', 'that', 'makes', 'it', 'simple', 'and', 'cost', 'effective', 'to', 'categorize', 'your', 'data', ',', 'clean', 'it', ',', 'enrich', 'it', 'and', 'move', 'it', 'reliably', 'between', 'various', 'data', 'stores', '.', 'Aws', 'Glue', 'consists', 'of', 'a', 'central', 'metadata', 'repository', 'known', 'as', 'the', 'Aws', 'Glue', 'data', 'catalog', '.', 'This', 'is', 'an', 'E', 'T', 'L', 'engine', 'that', 'automatically', 'generates', 'Python', 'or', 'Scalar', 'code', '.', 'It', 'also', 'provides', 'a', 'flexible', 'scheduler', 'that', 'handles', 'dependency', 'resolution', ',', 'job', 'monitoring', 'and', 'retries', '.', 'Aws', 'GLUE', 'is', 'serverless', 'so', 'you', 'do', \"n't\", 'need', 'to', 'set', 'up', 'or', 'manage', 'any', 'infrastructure', '.', 'You', 'can', 'use', 'the', 'Aws', 'GLUE', 'console', 'to', 'discover', 'data', ',', 'transform', 'it', 'and', 'make', 'it', 'available', 'for', 'search', 'in', 'queries', '.', 'The', 'console', 'calls', 'the', 'underlying', 'services', 'to', 'orchestrate', 'the', 'work', 'needed', 'to', 'transform', 'your', 'data', '.', 'You', 'can', 'also', 'use', 'the', 'Aws', 'glue', 'API', 'operations', 'to', 'interface', 'with', 'the', 'Aws', 'glue', 'services', '.', 'This', 'way', 'you', 'can', 'edit', 'debug', 'and', 'test', 'your', 'Python', 'or', 'Scalea', 'Apache', 'spark', 'E', 'T', 'L', 'code', 'using', 'a', 'familiar', 'development', 'environment', '.', 'Aws', 'glue', 'is', 'well', 'suited', 'to', 'machine', 'learning', 'because', 'it', 'can', 'receive', 'label', 'data', 'that', 'can', 'be', 'used', 'for', 'training', '.', 'Here', \"'s\", 'an', 'example', 'say', 'that', 'you', 'provide', 'Aws', 'glue', 'with', 'training', 'data', 'that', 'teaches', 'the', 'model', 'what', 'duplicate', 'records', 'in', 'the', 'data', 'source', 'look', 'like', '.', 'Then', 'Aws', 'glue', 'can', 'identify', 'the', 'duplicates', 'and', 'present', 'them', 'for', 'further', 'analysis', 'by', 'a', 'data', 'engineer', '.', 'Aws', 'Glue', 'enables', 'the', 'orchestration', 'of', 'complex', 'E', 'T', 'L', 'jobs', '.', 'In', 'the', 'example', ',', 'Aws', 'glue', 'crawls', 'the', 'data', 'sources', 'and', 'presents', 'the', 'information', 'to', 'clients', 'as', 'a', 'data', 'catalog', '.', 'Aws', 'glue', 'can', 'run', 'your', 'E', 'T', 'L', 'jobs', 'based', 'on', 'an', 'event', 'such', 'as', 'getting', 'a', 'new', 'data', 'set', '.', 'For', 'example', ',', 'you', 'can', 'use', 'an', 'aws', 'LAMBDA', 'function', 'to', 'trigger', 'your', 'E', 'T', 'L', 'jobs', 'to', 'run', 'as', 'soon', 'as', 'new', 'data', 'becomes', 'available', 'in', 'Amazon', 'S3', ',', 'you', 'can', 'also', 'register', 'this', 'new', 'data', 'set', 'in', 'the', 'AWS', 'glue', 'data', 'catalog', 'as', 'part', 'of', 'your', 'E', 'T', 'L', 'jobs', '.', 'Although', 'manage', 'tools', 'are', 'available', 'in', 'AWS', 'to', 'manipulate', 'data', '.', 'A', 'data', 'scientist', 'will', 'also', 'write', 'scripts', 'in', 'their', 'jupiter', 'notebook', 'to', 'handle', 'the', 'data', '.', 'A', 'very', 'simple', 'extract', 'and', 'load', 'script', 'is', 'shown', 'here', '.', 'The', 'imports', 'and', 'variables', 'section', 'imports', 'the', 'libraries', 'that', 'are', 'used', '.', 'Note', 'that', 'Bodo', 'three', 'is', 'the', 'library', 'for', 'AWS', '.', 'Variables', 'are', 'also', 'set', 'here', 'for', 'the', 'zip', 'files', ',', 'web', 'location', 'and', 'a', 'local', 'folder', 'for', 'extraction', '.', 'The', 'download', 'and', 'extract', 'section', 'makes', 'a', 'web', 'request', 'saving', 'the', 'bytes', 'from', 'the', 'URL', 'as', 'a', 'stream', '.', 'This', 'stream', 'is', 'passed', 'to', 'the', 'zip', 'file', 'function', 'which', 'is', 'then', 'used', 'to', 'extract', 'the', 'data', 'with', 'the', 'extracted', 'files', 'in', 'a', 'folder', '.', 'The', 'upload', 'to', 'S3', 'section', 'enumerates', 'the', 'folders', 'files', 'and', 'uploads', 'each', 'file', 'to', 'Amazon', 'S3', '.', 'If', 'you', 'discover', 'that', 'this', 'script', 'is', 'used', ',', 'often', ',', 'it', 'should', 'be', 'migrated', 'to', 'a', 'standalone', 'function', 'that', 'can', 'be', 'imported', 'by', 'other', 'Python', 'applications', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'two', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'three', 'where', 'we', \"'ll\", 'review', 'how', 'to', 'secure', 'your', 'data', '.']\n",
      "Hi , welcome back . We 'll continue exploring data collection by reviewing how to extract , transform and load data data is typically spread across many different system and data provider . This present a challenge . You 'll need to bring all these data source together into something that can be consumed by a machine learning model . You can do this through extract , transform and load , which is also known a E T L. The step in E T L are defined this way in the extract step , you pull the data from the source to a single location . During extraction , you might need to modify the data , combine matching record or do other task that transform the data . Finally . In the load step , the data is loaded into a repository such a Amazon S3 . A typical L framework ha several component . As an example , consider the diagram . First , the crawler , a program connects to a data store which can be a source or a target . It progress through a ranked list of classifier to determine the schema for your data . Then it creates metadata table in the aws glue data catalog . A job defines the business logic that 's needed to perform E T L work to run the job you 'll need to use a schedule or event . As a final note , the service we just discussed exist in the transform partition of the E T L process . Aws Glue is a fully managed E T L service that make it simple and cost effective to categorize your data , clean it , enrich it and move it reliably between various data store . Aws Glue consists of a central metadata repository known a the Aws Glue data catalog . This is an E T L engine that automatically generates Python or Scalar code . It also provides a flexible scheduler that handle dependency resolution , job monitoring and retries . Aws GLUE is serverless so you do n't need to set up or manage any infrastructure . You can use the Aws GLUE console to discover data , transform it and make it available for search in query . The console call the underlying service to orchestrate the work needed to transform your data . You can also use the Aws glue API operation to interface with the Aws glue service . This way you can edit debug and test your Python or Scalea Apache spark E T L code using a familiar development environment . Aws glue is well suited to machine learning because it can receive label data that can be used for training . Here 's an example say that you provide Aws glue with training data that teach the model what duplicate record in the data source look like . Then Aws glue can identify the duplicate and present them for further analysis by a data engineer . Aws Glue enables the orchestration of complex E T L job . In the example , Aws glue crawl the data source and present the information to client a a data catalog . Aws glue can run your E T L job based on an event such a getting a new data set . For example , you can use an aws LAMBDA function to trigger your E T L job to run a soon a new data becomes available in Amazon S3 , you can also register this new data set in the AWS glue data catalog a part of your E T L job . Although manage tool are available in AWS to manipulate data . A data scientist will also write script in their jupiter notebook to handle the data . A very simple extract and load script is shown here . The import and variable section import the library that are used . Note that Bodo three is the library for AWS . Variables are also set here for the zip file , web location and a local folder for extraction . The download and extract section make a web request saving the byte from the URL a a stream . This stream is passed to the zip file function which is then used to extract the data with the extracted file in a folder . The upload to S3 section enumerates the folder file and uploads each file to Amazon S3 . If you discover that this script is used , often , it should be migrated to a standalone function that can be imported by other Python application . That 's it . For part two of this section , we 'll see you again for part three where we 'll review how to secure your data .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'data', 'collection', 'by', 'reviewing', 'how', 'to', 'secure', 'your', 'data', '.', 'It', \"'s\", 'important', 'to', 'consider', 'the', 'security', 'of', 'your', 'data', '.', 'Though', 'the', 'data', 'sets', 'used', 'in', 'this', 'course', 'are', 'all', 'public', 'real', 'data', 'about', 'customer', 'transactions', 'or', 'health', 'records', 'need', 'to', 'be', 'kept', 'secure', '.', 'You', 'can', 'use', 'Aws', ',', 'identity', 'and', 'access', 'management', ',', 'which', 'is', 'also', 'known', 'as', 'I', 'AM', '.', 'It', \"'s\", 'a', 'service', 'that', 'controls', 'access', 'to', 'resources', ',', 'make', 'sure', 'you', \"'re\", 'securing', 'your', 'data', 'within', 'AWS', 'correctly', 'so', 'you', 'can', 'avoid', 'data', 'breaches', '.', 'The', 'diagram', 'shows', 'a', 'simple', 'I', 'AM', 'policy', 'that', 'allows', 'only', 'read', 'access', 'to', 'a', 'specific', 'S3', 'bucket', 'for', 'the', 'listed', 'role', '.', 'In', 'addition', 'to', 'controlling', 'access', 'to', 'data', ',', 'you', 'need', 'to', 'make', 'sure', 'your', 'data', 'is', 'secure', '.', 'It', \"'s\", 'a', 'good', 'practice', 'and', 'it', 'might', 'also', 'be', 'legally', 'required', 'for', 'certain', 'data', 'types', 'such', 'as', 'financial', 'data', 'or', 'health', 'care', 'records', '.', 'Aws', 'provides', 'encryption', 'features', 'for', 'storage', 'services', '.', 'Typically', 'for', 'data', 'that', \"'s\", 'at', 'rest', 'or', 'in', 'transit', ',', 'you', 'can', 'often', 'meet', 'these', 'encryption', 'requirements', 'by', 'enabling', 'encryption', 'on', 'the', 'object', 'or', 'service', 'you', 'want', 'to', 'protect', 'for', 'data', 'in', 'transit', '.', 'You', 'must', 'use', 'secure', 'transports', 'like', 'secure', 'sockets', 'layer', 'transport', 'layer', 'security', 'or', 'SSL', 'TLS', '.', 'Another', 'aspect', 'to', 'consider', 'is', 'compliance', 'audits', '.', 'When', 'dealing', 'with', 'data', 'from', 'regulated', 'industries', ',', 'you', \"'ll\", 'often', 'need', 'to', 'audit', 'access', 'to', 'the', 'data', '.', 'Aws', 'Cloud', 'Trail', 'is', 'a', 'service', 'that', 'enables', 'governance', 'compliance', ',', 'operational', 'auditing', 'and', 'risk', 'auditing', 'of', 'your', 'Aws', 'account', 'with', 'cloud', 'trail', '.', 'You', 'can', 'log', 'continuously', 'monitor', 'and', 'retain', 'account', 'activity', 'related', 'to', 'actions', 'across', 'your', 'entire', 'Aws', 'infrastructure', '.', 'Cloud', 'trail', 'provides', 'an', 'event', 'history', 'of', 'your', 'AWS', 'account', 'activity', 'including', 'actions', 'taken', 'through', 'the', 'Aws', 'management', 'console', ',', 'Aws', 'S', 'D', 'K', 'S', 'command', 'line', 'tools', 'and', 'other', 'Aws', 'services', '.', 'This', 'event', 'in', 'history', 'simplifies', 'security', 'analysis', ',', 'resource', 'change', ',', 'tracking', 'and', 'troubleshooting', '.', 'You', 'can', 'also', 'use', 'cloud', 'trail', 'to', 'detect', 'unusual', 'activity', 'in', 'your', 'Aws', 'accounts', '.', 'All', 'these', 'features', 'can', 'help', 'you', 'simplify', 'operational', 'analysis', 'and', 'troubleshooting', '.', 'Here', 'are', 'the', 'key', 'takeaways', 'for', 'this', 'section', '.', 'We', 'looked', 'at', 'the', 'first', 'step', 'in', 'solving', 'machine', 'learning', 'problems', ',', 'obtaining', 'the', 'data', 'required', 'to', 'train', 'your', 'machine', 'learning', 'model', '.', 'We', 'also', 'reviewed', 'how', 'E', 'T', 'L', 'can', 'be', 'used', 'to', 'obtain', 'data', 'from', 'multiple', 'sources', '.', 'Services', 'like', 'AWS', 'glue', 'can', 'make', 'it', 'easy', 'to', 'obtain', 'data', 'from', 'multiple', 'data', 'stores', '.', 'Finally', 'make', 'sure', 'you', 'understand', 'your', 'security', 'requirements', '.', 'These', 'are', 'based', 'on', 'both', 'business', 'need', 'and', 'any', 'regulatory', 'requirements', '.', 'Also', 'make', 'sure', 'your', 'data', 'is', 'secure', '.', 'Only', 'authorized', 'users', 'should', 'be', 'able', 'to', 'access', 'your', 'data', 'and', 'it', 'should', 'be', 'encrypted', 'where', 'possible', '.', 'That', \"'s\", 'it', '.', 'For', 'section', 'two', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video']\n",
      "Hi , welcome back . We 'll continue exploring data collection by reviewing how to secure your data . It 's important to consider the security of your data . Though the data set used in this course are all public real data about customer transaction or health record need to be kept secure . You can use Aws , identity and access management , which is also known a I AM . It 's a service that control access to resource , make sure you 're securing your data within AWS correctly so you can avoid data breach . The diagram show a simple I AM policy that allows only read access to a specific S3 bucket for the listed role . In addition to controlling access to data , you need to make sure your data is secure . It 's a good practice and it might also be legally required for certain data type such a financial data or health care record . Aws provides encryption feature for storage service . Typically for data that 's at rest or in transit , you can often meet these encryption requirement by enabling encryption on the object or service you want to protect for data in transit . You must use secure transport like secure socket layer transport layer security or SSL TLS . Another aspect to consider is compliance audit . When dealing with data from regulated industry , you 'll often need to audit access to the data . Aws Cloud Trail is a service that enables governance compliance , operational auditing and risk auditing of your Aws account with cloud trail . You can log continuously monitor and retain account activity related to action across your entire Aws infrastructure . Cloud trail provides an event history of your AWS account activity including action taken through the Aws management console , Aws S D K S command line tool and other Aws service . This event in history simplifies security analysis , resource change , tracking and troubleshooting . You can also use cloud trail to detect unusual activity in your Aws account . All these feature can help you simplify operational analysis and troubleshooting . Here are the key takeaway for this section . We looked at the first step in solving machine learning problem , obtaining the data required to train your machine learning model . We also reviewed how E T L can be used to obtain data from multiple source . Services like AWS glue can make it easy to obtain data from multiple data store . Finally make sure you understand your security requirement . These are based on both business need and any regulatory requirement . Also make sure your data is secure . Only authorized user should be able to access your data and it should be encrypted where possible . That 's it . For section two , we 'll see you in the next video\n",
      "['Hi', 'and', 'welcome', 'back', '.', 'This', 'is', 'section', 'three', 'and', 'we', \"'re\", 'going', 'to', 'cover', 'how', 'to', 'evaluate', 'your', 'data', 'in', 'this', 'section', '.', 'We', \"'ll\", 'look', 'at', 'different', 'data', 'formats', 'and', 'types', '.', 'We', \"'ll\", 'also', 'look', 'at', 'how', 'you', 'can', 'visualize', 'and', 'analyze', 'the', 'data', 'before', 'feature', 'engineering', 'before', 'you', 'can', 'start', 'running', 'statistics', 'on', 'your', 'data', 'to', 'better', 'understand', 'what', 'you', \"'re\", 'working', 'with', '.', 'You', 'need', 'to', 'ensure', 'it', \"'s\", 'in', 'the', 'right', 'format', 'for', 'analysis', 'for', 'Amazon', \"'s\", 'Sagemaker', 'algorithms', 'support', 'training', 'with', 'data', 'in', 'CS', 'V', 'format', '.', 'Many', 'of', 'the', 'tools', 'you', \"'ll\", 'use', 'to', 'explore', 'visualize', 'and', 'analyze', 'the', 'data', 'can', 'also', 'read', 'it', 'in', 'CS', 'V', 'format', '.', 'Generally', 'speaking', ',', 'you', \"'ll\", 'need', 'to', 'have', 'at', 'least', 'some', 'domain', 'knowledge', 'for', 'the', 'problem', 'you', \"'re\", 'trying', 'to', 'solve', 'with', 'machine', 'learning', '.', 'For', 'example', ',', 'if', 'you', \"'re\", 'developing', 'a', 'model', 'to', 'predict', 'if', 'a', 'set', 'of', 'symptoms', 'indicates', 'a', 'disease', ',', 'you', \"'d\", 'need', 'to', 'know', 'the', 'relationship', 'between', 'the', 'symptoms', 'and', 'the', 'disease', 'data', 'typically', 'needs', 'to', 'be', 'in', 'numeric', 'form', '.', 'So', 'machine', 'learning', 'algorithms', 'can', 'use', 'the', 'data', 'to', 'make', 'predictions', '.', 'We', \"'ll\", 'look', 'at', 'ways', 'you', 'can', 'convert', 'text', 'data', 'in', 'the', 'next', 'section', '.', 'For', 'now', ',', 'we', \"'ll\", 'just', 'explore', 'the', 'data', 'and', 'try', 'to', 'gain', 'some', 'insights', 'into', 'the', 'overall', 'data', 'set', '.', 'One', 'popular', 'open', 'source', 'Python', 'library', 'is', 'pandas', '.', 'It', 'can', 'take', 'data', 'in', 'various', 'formats', 'reformat', 'it', 'and', 'load', 'it', 'into', 'a', 'tabular', 'representation', 'of', 'your', 'data', ',', 'presenting', 'it', 'in', 'rows', 'and', 'columns', '.', 'Some', 'of', 'the', 'formats', 'that', 'pandas', 'can', 'reformat', 'and', 'load', 'include', 'CS', 'V', ',', 'Excel', ',', 'Pickle', 'and', 'javascript', 'object', 'notation', 'or', 'json', 'pandas', 'also', 'has', 'data', 'analysis', 'and', 'manipulation', 'features', 'and', 'we', 'will', 'use', 'them', 'throughout', 'this', 'module', '.', 'Loading', 'Data', 'can', 'be', 'as', 'simple', 'as', 'the', 'example', 'which', 'pulls', 'in', 'the', 'CS', 'V', 'file', 'from', 'the', 'specified', 'L', 'when', 'you', 'load', 'data', 'into', 'pandas', ',', 'it', \"'s\", 'stored', 'as', 'a', 'pandas', 'data', 'frame', '.', 'In', 'the', 'pandas', 'documentation', '.', 'A', 'data', 'frame', 'is', 'described', 'as', 'a', 'general', 'two', 'D', 'labeled', 'size', 'mutable', 'tabular', 'structure', 'with', 'potentially', 'heterogeneously', 'typed', 'column', '.', 'A', 'more', 'helpful', 'way', 'to', 'think', 'of', 'a', 'data', 'frame', 'is', 'to', 'think', 'of', 'it', 'as', 'a', 'spreadsheet', 'or', 'a', 'SQL', 'table', 'like', 'a', 'table', 'or', 'spreadsheet', '.', 'A', 'data', 'frame', 'will', 'have', 'rows', 'which', 'are', 'also', 'known', 'as', 'instances', 'and', 'it', 'will', 'have', 'columns', 'which', 'are', 'also', 'known', 'as', 'attributes', '.', 'The', 'shape', 'property', 'of', 'a', 'data', 'frame', 'describes', 'the', 'number', 'of', 'rows', 'and', 'columns', '.', 'It', 'has', 'each', 'column', 'in', 'a', 'data', 'frame', 'is', 'a', 'series', '.', 'A', 'series', 'is', 'a', 'one', 'dimensional', 'labeled', 'array', '.', 'A', 'series', 'can', 'store', 'data', 'of', 'any', 'type', 'to', 'learn', 'more', 'about', 'data', 'structures', '.', 'In', 'pandas', '.', 'See', 'the', 'pandas', 'documentation', 'along', 'with', 'data', ',', 'you', 'can', 'load', 'a', 'data', 'frame', 'with', 'row', 'labels', 'and', 'column', 'labels', '.', 'The', 'row', 'labels', 'are', 'known', 'as', 'an', 'index', 'and', 'the', 'column', 'labels', 'are', 'known', 'as', 'columns', '.', 'If', 'you', 'loaded', 'your', 'data', 'from', 'a', 'CS', 'V', 'file', 'with', 'a', 'header', 'row', ',', 'the', 'columns', 'will', 'be', 'created', 'from', 'the', 'first', 'line', 'of', 'the', 'file', '.', 'You', 'can', 'change', 'that', 'behavior', '.', 'However', ',', 'if', 'you', 'do', \"n't\", 'have', 'column', 'names', 'in', 'the', 'source', 'file', ',', 'you', 'can', 'pass', 'them', 'as', 'a', 'parameter', '.', 'When', 'performing', 'data', 'analysis', '.', 'It', \"'s\", 'important', 'to', 'make', 'sure', 'you', \"'re\", 'using', 'the', 'correct', 'data', 'types', '.', 'In', 'many', 'cases', ',', 'pandas', 'will', 'correctly', 'infer', 'the', 'correct', 'data', 'types', 'when', 'it', 'loads', 'data', '.', 'And', 'you', 'can', 'move', 'on', '.', 'If', 'you', 'have', 'domain', 'knowledge', 'or', 'access', 'to', 'a', 'domain', 'expert', ',', 'they', 'can', 'often', 'identify', 'data', 'type', 'issues', '.', 'You', 'can', 'use', 'either', 'D', 'types', 'or', 'the', 'info', 'function', 'to', 'obtain', 'information', 'on', 'the', 'column', 'types', 'as', 'shown', 'in', 'the', 'example', '.', 'If', 'you', 'do', \"n't\", 'have', 'the', 'correct', 'data', 'types', ',', 'you', 'need', 'to', 'figure', 'out', 'why', '.', 'This', 'is', 'the', 'case', '.', 'Often', 'a', 'numeric', 'column', 'could', 'have', 'been', 'missing', 'data', 'or', 'it', 'could', 'be', 'a', 'single', 'text', 'value', '.', 'For', 'example', ',', 'in', 'the', 'car', 'data', 'set', ',', 'the', 'number', 'of', 'doors', 'can', 'be', '234', 'or', 'five', 'more', '.', 'After', 'you', \"'ve\", 'analyzed', 'the', 'data', ',', 'you', 'can', 'convert', 'a', 'column', 'to', 'the', 'correct', 'data', 'type', 'using', 'pandas', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'one', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'two', 'where', 'we', 'will', 'review', 'how', 'to', 'describe', 'your', 'data', '.']\n",
      "Hi and welcome back . This is section three and we 're going to cover how to evaluate your data in this section . We 'll look at different data format and type . We 'll also look at how you can visualize and analyze the data before feature engineering before you can start running statistic on your data to better understand what you 're working with . You need to ensure it 's in the right format for analysis for Amazon 's Sagemaker algorithm support training with data in CS V format . Many of the tool you 'll use to explore visualize and analyze the data can also read it in CS V format . Generally speaking , you 'll need to have at least some domain knowledge for the problem you 're trying to solve with machine learning . For example , if you 're developing a model to predict if a set of symptom indicates a disease , you 'd need to know the relationship between the symptom and the disease data typically need to be in numeric form . So machine learning algorithm can use the data to make prediction . We 'll look at way you can convert text data in the next section . For now , we 'll just explore the data and try to gain some insight into the overall data set . One popular open source Python library is panda . It can take data in various format reformat it and load it into a tabular representation of your data , presenting it in row and column . Some of the format that panda can reformat and load include CS V , Excel , Pickle and javascript object notation or json panda also ha data analysis and manipulation feature and we will use them throughout this module . Loading Data can be a simple a the example which pull in the CS V file from the specified L when you load data into panda , it 's stored a a panda data frame . In the panda documentation . A data frame is described a a general two D labeled size mutable tabular structure with potentially heterogeneously typed column . A more helpful way to think of a data frame is to think of it a a spreadsheet or a SQL table like a table or spreadsheet . A data frame will have row which are also known a instance and it will have column which are also known a attribute . The shape property of a data frame describes the number of row and column . It ha each column in a data frame is a series . A series is a one dimensional labeled array . A series can store data of any type to learn more about data structure . In panda . See the panda documentation along with data , you can load a data frame with row label and column label . The row label are known a an index and the column label are known a column . If you loaded your data from a CS V file with a header row , the column will be created from the first line of the file . You can change that behavior . However , if you do n't have column name in the source file , you can pas them a a parameter . When performing data analysis . It 's important to make sure you 're using the correct data type . In many case , panda will correctly infer the correct data type when it load data . And you can move on . If you have domain knowledge or access to a domain expert , they can often identify data type issue . You can use either D type or the info function to obtain information on the column type a shown in the example . If you do n't have the correct data type , you need to figure out why . This is the case . Often a numeric column could have been missing data or it could be a single text value . For example , in the car data set , the number of door can be 234 or five more . After you 've analyzed the data , you can convert a column to the correct data type using panda . That 's it . For part one of this section , we 'll see you again for part two where we will review how to describe your data .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'how', 'to', 'describe', 'your', 'data', '.', 'Now', 'that', 'your', 'data', 'is', 'in', 'a', 'readable', 'format', ',', 'you', 'can', 'perform', 'descriptive', 'statistics', 'on', 'the', 'data', 'to', 'better', 'understand', 'it', '.', 'Descriptive', 'statistics', 'help', 'you', 'gain', 'valuable', 'insights', 'into', 'your', 'data', 'so', 'that', 'you', 'can', 'effectively', 'preproce', 'the', 'data', 'and', 'prepare', 'it', 'for', 'your', 'M', 'L', 'model', '.', 'We', \"'ll\", 'look', 'at', 'how', 'you', 'can', 'do', 'that', 'and', 'discuss', 'why', 'it', \"'s\", 'so', 'important', '.', 'First', ',', 'descriptive', 'statistics', 'can', 'be', 'organized', 'into', 'a', 'few', 'different', 'categories', '.', 'Overall', 'statistics', 'include', 'the', 'number', 'of', 'rows', 'and', 'the', 'number', 'of', 'columns', 'in', 'your', 'data', 'set', '.', 'This', 'information', 'which', 'relates', 'to', 'the', 'dimensions', 'of', 'your', 'data', 'is', 'very', 'important', '.', 'For', 'example', ',', 'it', 'can', 'indicate', 'that', 'you', 'have', 'too', 'many', 'features', 'which', 'can', 'lead', 'to', 'high', 'dimensionality', 'and', 'poor', 'model', 'performance', 'attribute', 'statistics', 'are', 'another', 'type', 'of', 'descriptive', 'statistic', '.', 'Specifically', 'for', 'numeric', 'attributes', '.', 'They', \"'re\", 'used', 'to', 'get', 'a', 'better', 'sense', 'of', 'the', 'shape', 'of', 'your', 'attributes', '.', 'This', 'includes', 'properties', 'like', 'the', 'mean', 'standard', 'deviation', 'variants', 'and', 'minimum', 'and', 'maximum', 'values', '.', 'If', 'you', 'need', 'to', 'look', 'at', 'relationships', 'between', 'more', 'than', 'one', 'variable', ',', 'you', 'can', 'consider', 'Multivariate', 'statistics', ',', 'they', 'mostly', 'relate', 'to', 'the', 'correlations', 'and', 'relationships', 'between', 'your', 'attributes', 'for', 'cases', 'when', 'you', 'have', 'multiple', 'variables', 'or', 'features', ',', 'you', 'might', 'want', 'to', 'look', 'at', 'the', 'correlations', 'between', 'them', '.', 'It', \"'s\", 'important', 'to', 'identify', 'correlations', 'between', 'attributes', 'because', 'a', 'high', 'correlation', 'between', 'two', 'attributes', 'can', 'sometimes', 'lead', 'to', 'poor', 'model', 'performance', '.', 'When', 'features', 'are', 'closely', 'correlated', 'and', 'they', \"'re\", 'all', 'used', 'in', 'the', 'same', 'model', 'to', 'predict', 'the', 'response', 'variable', '.', 'There', 'could', 'be', 'problems', '.', 'For', 'example', ',', 'the', 'model', 'loss', 'might', 'not', 'converge', 'to', 'a', 'minimum', 'state', '.', 'So', 'be', 'aware', 'of', 'highly', 'correlated', 'features', 'in', 'your', 'data', 'set', 'mean', 'and', 'median', 'are', 'two', 'different', 'measures', '.', 'Describing', 'the', 'extent', 'that', 'your', 'data', 'is', 'clustered', 'around', 'some', 'value', 'or', 'position', 'mean', 'can', 'be', 'a', 'useful', 'method', 'for', 'understanding', 'your', 'data', 'when', 'the', 'data', 'is', 'symmetrical', '.', 'However', ',', 'if', 'your', 'data', 'is', 'skewed', 'or', 'contains', 'outliers', ',', 'then', 'median', 'tends', 'to', 'provide', 'the', 'better', 'metric', 'for', 'understanding', 'your', 'data', 'as', 'it', 'relates', 'to', 'central', 'tendency', '.', 'For', 'instance', ',', 'if', 'you', 'have', 'outliers', 'with', 'large', 'values', ',', 'the', 'mean', 'can', 'be', 'skewed', 'one', 'way', 'and', 'it', 'would', \"n't\", 'serve', 'as', 'an', 'accurate', 'representation', 'of', 'where', 'your', 'values', 'are', 'truly', 'centered', 'median', 'is', \"n't\", 'affected', 'by', 'outliers', '.', 'In', 'the', 'same', 'way', ',', 'we', \"'ll\", 'talk', 'more', 'about', 'outliers', 'soon', 'statistics', 'are', 'available', 'and', 'they', 'can', 'be', 'viewed', 'on', 'numerical', 'data', 'by', 'using', 'methods', 'such', 'as', 'describe', 'there', 'are', 'also', 'other', 'methods', 'to', 'calculate', 'the', 'mean', 'median', 'and', 'others', '.', 'You', 'can', 'also', 'view', 'statistics', 'on', 'single', 'or', 'multiple', 'columns', '.', 'You', 'can', 'even', 'group', 'data', 'by', 'specific', 'values', 'for', 'categorical', 'attributes', '.', 'You', 'can', 'look', 'at', 'the', 'frequency', 'of', 'attribute', 'values', 'in', 'your', 'data', 'set', '.', 'That', 'information', 'will', 'give', 'you', 'some', 'idea', 'about', 'what', 'is', 'inside', 'that', 'categorical', 'variable', '.', 'The', 'diagram', 'here', 'shows', 'the', 'car', 'data', 'set', 'which', 'is', 'made', 'up', 'of', 'several', 'categorical', 'values', '.', 'Buying', 'mat', 'lug', 'boot', 'safety', 'and', 'class', 'safety', 'can', 'be', 'either', 'low', ',', 'medium', 'or', 'high', 'from', 'the', 'described', 'function', '.', 'You', 'can', 'see', 'that', 'there', 'are', 'three', 'unique', 'values', 'with', 'low', 'being', 'the', 'most', 'frequent', 'looking', 'at', 'the', 'class', 'column', '.', 'It', 'appears', 'that', 'the', 'top', 'value', 'of', 'the', 'four', 'is', 'UN', 'AC', 'C', 'which', 'stands', 'for', 'unaccounted', '.', 'This', 'accounts', 'for', '1210', 'of', 'the', '1728', 'values', 'or', '70', '%', '.', 'This', 'might', 'suggest', 'an', 'imbalance', 'for', 'a', 'target', 'variable', '.', 'That', \"'s\", 'also', 'of', 'a', 'categorical', 'type', '.', 'You', 'can', 'look', 'at', 'the', 'class', 'distribution', 'to', 'see', 'whether', 'there', \"'s\", 'a', 'class', 'imbalance', 'in', 'your', 'data', 'set', ',', 'imbalanced', 'data', 'can', 'mark', 'a', 'disproportionate', 'ratio', 'for', 'your', 'classes', '.', 'For', 'instance', ',', 'your', 'data', 'set', 'is', 'made', 'up', 'of', 'credit', 'card', 'transactions', ',', 'but', 'only', '1/10', 'of', 'a', 'percent', 'is', 'labeled', 'as', 'fraud', '.', 'In', 'this', 'case', ',', 'your', 'algorithm', 'might', 'not', 'learn', 'well', 'enough', 'to', 'predict', 'examples', 'of', 'credit', 'card', 'fraud', 'visualization', 'could', 'help', 'you', 'gain', 'insights', 'into', 'your', 'data', 'that', 'you', 'might', 'not', 'be', 'aware', 'of', '.', 'Otherwise', ',', 'a', 'histogram', 'is', 'often', 'a', 'good', 'visualization', 'technique', 'for', 'seeing', 'the', 'overall', 'behavior', 'of', 'a', 'particular', 'feature', 'with', 'a', 'histogram', '.', 'You', 'can', 'answer', 'questions', 'like', 'is', 'the', 'feature', 'data', 'normally', 'distributed', '.', 'How', 'many', 'peaks', 'are', 'there', 'in', 'the', 'data', '.', 'Is', 'there', 'any', 'skewness', 'for', 'that', 'particular', 'feature', '?', 'When', 'using', 'histograms', 'for', 'your', 'data', 'visualization', 'values', 'are', 'binned', ',', 'the', 'taller', 'peaks', 'of', 'the', 'histogram', 'indicate', 'the', 'most', 'common', 'values', 'for', 'numerical', 'features', '.', 'You', 'can', 'use', 'density', 'plots', 'and', 'box', 'plots', '.', 'In', 'addition', 'to', 'histograms', 'to', 'get', 'an', 'idea', 'of', 'what', \"'s\", 'inside', 'that', 'particular', 'feature', 'like', 'a', 'histogram', '.', 'These', 'visualizations', 'will', 'help', 'you', 'answer', 'questions', 'like', 'what', \"'s\", 'the', 'range', 'of', 'the', 'data', ',', 'the', 'peak', 'of', 'the', 'data', '?', 'Are', 'there', 'any', 'outliers', '?', 'Are', 'there', 'any', 'special', 'features', 'answering', 'these', 'questions', '?', 'Helps', 'you', 'understand', 'your', 'data', 'better', '?', 'And', 'can', 'also', 'help', 'you', 'decide', 'if', 'you', 'need', 'to', 'do', 'more', 'specialized', 'data', '.', 'Preprocessing', '.', 'A', 'box', 'plot', 'is', 'a', 'method', 'for', 'graphically', 'depicting', 'groups', 'of', 'numerical', 'data', 'through', 'their', 'core', 'tiles', '.', 'When', 'you', 'have', 'more', 'than', 'two', 'numerical', 'variables', 'in', 'a', 'feature', 'data', 'set', ',', 'you', 'might', 'want', 'to', 'look', 'at', 'their', 'relationship', '.', 'A', 'scatter', 'plot', 'is', 'a', 'good', 'way', 'to', 'identify', 'any', 'special', 'relationships', 'among', 'those', 'variables', '.', 'In', 'this', 'case', ',', 'the', 'left', 'diagram', 'has', 'sulfates', 'and', 'alcohol', '.', 'There', 'are', 'two', 'numerical', 'variables', '.', 'Suppose', 'you', 'want', 'to', 'show', 'the', 'relationship', 'between', 'these', 'variables', '.', 'You', 'can', 'use', 'a', 'scatter', 'plot', 'to', 'help', 'you', 'visualize', 'that', 'there', 'are', 'plots', 'scattered', 'around', 'and', 'the', 'correlation', 'among', 'them', 'might', 'not', 'be', 'that', 'high', 'because', 'the', 'data', 'is', 'scattered', '.', 'However', ',', 'you', 'might', 'find', 'some', 'relatively', 'positive', 'relationships', 'between', 'the', 'two', 'variables', 'scatter', 'plot', 'matrices', 'help', 'you', 'look', 'at', 'the', 'relationship', 'between', 'multiple', 'different', 'features', 'in', 'Pandi', ',', 'you', 'can', 'easily', 'create', 'scatter', 'plot', 'matrices', 'based', 'on', 'the', 'columns', '.', 'You', 'want', 'to', 'look', 'at', 'this', 'example', ',', 'has', 'three', 'columns', 'and', 'it', 'will', 'give', 'the', 'pair', 'wise', 'scatter', 'plot', 'for', 'any', 'two', 'columns', 'with', 'a', 'scatter', 'plot', '.', 'You', 'might', 'want', 'to', 'identify', 'special', 'regions', 'that', 'a', 'particular', 'subset', 'of', 'data', 'could', 'fit', 'into', '.', 'In', 'the', 'example', ',', 'is', 'there', 'a', 'relationship', 'between', 'alcohol', 'sulfates', 'and', 'quality', '?', 'You', 'could', 'plot', 'those', 'values', 'against', 'good', 'and', 'poor', 'quality', 'wines', '.', 'Like', 'the', 'example', ',', 'plotting', 'gives', 'you', 'an', 'idea', 'of', 'how', 'useful', 'particular', 'variables', 'can', 'be', '.', 'If', 'you', \"'re\", 'using', 'them', 'for', 'a', 'classification', 'problem', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'two', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'three', 'where', 'we', \"'ll\", 'review', 'correlations', 'and', 'the', 'takeaways', 'for', 'this', 'section', '.']\n",
      "Hi , welcome back . We 'll continue exploring how to describe your data . Now that your data is in a readable format , you can perform descriptive statistic on the data to better understand it . Descriptive statistic help you gain valuable insight into your data so that you can effectively preproce the data and prepare it for your M L model . We 'll look at how you can do that and discus why it 's so important . First , descriptive statistic can be organized into a few different category . Overall statistic include the number of row and the number of column in your data set . This information which relates to the dimension of your data is very important . For example , it can indicate that you have too many feature which can lead to high dimensionality and poor model performance attribute statistic are another type of descriptive statistic . Specifically for numeric attribute . They 're used to get a better sense of the shape of your attribute . This includes property like the mean standard deviation variant and minimum and maximum value . If you need to look at relationship between more than one variable , you can consider Multivariate statistic , they mostly relate to the correlation and relationship between your attribute for case when you have multiple variable or feature , you might want to look at the correlation between them . It 's important to identify correlation between attribute because a high correlation between two attribute can sometimes lead to poor model performance . When feature are closely correlated and they 're all used in the same model to predict the response variable . There could be problem . For example , the model loss might not converge to a minimum state . So be aware of highly correlated feature in your data set mean and median are two different measure . Describing the extent that your data is clustered around some value or position mean can be a useful method for understanding your data when the data is symmetrical . However , if your data is skewed or contains outlier , then median tends to provide the better metric for understanding your data a it relates to central tendency . For instance , if you have outlier with large value , the mean can be skewed one way and it would n't serve a an accurate representation of where your value are truly centered median is n't affected by outlier . In the same way , we 'll talk more about outlier soon statistic are available and they can be viewed on numerical data by using method such a describe there are also other method to calculate the mean median and others . You can also view statistic on single or multiple column . You can even group data by specific value for categorical attribute . You can look at the frequency of attribute value in your data set . That information will give you some idea about what is inside that categorical variable . The diagram here show the car data set which is made up of several categorical value . Buying mat lug boot safety and class safety can be either low , medium or high from the described function . You can see that there are three unique value with low being the most frequent looking at the class column . It appears that the top value of the four is UN AC C which stand for unaccounted . This account for 1210 of the 1728 value or 70 % . This might suggest an imbalance for a target variable . That 's also of a categorical type . You can look at the class distribution to see whether there 's a class imbalance in your data set , imbalanced data can mark a disproportionate ratio for your class . For instance , your data set is made up of credit card transaction , but only 1/10 of a percent is labeled a fraud . In this case , your algorithm might not learn well enough to predict example of credit card fraud visualization could help you gain insight into your data that you might not be aware of . Otherwise , a histogram is often a good visualization technique for seeing the overall behavior of a particular feature with a histogram . You can answer question like is the feature data normally distributed . How many peak are there in the data . Is there any skewness for that particular feature ? When using histogram for your data visualization value are binned , the taller peak of the histogram indicate the most common value for numerical feature . You can use density plot and box plot . In addition to histogram to get an idea of what 's inside that particular feature like a histogram . These visualization will help you answer question like what 's the range of the data , the peak of the data ? Are there any outlier ? Are there any special feature answering these question ? Helps you understand your data better ? And can also help you decide if you need to do more specialized data . Preprocessing . A box plot is a method for graphically depicting group of numerical data through their core tile . When you have more than two numerical variable in a feature data set , you might want to look at their relationship . A scatter plot is a good way to identify any special relationship among those variable . In this case , the left diagram ha sulfate and alcohol . There are two numerical variable . Suppose you want to show the relationship between these variable . You can use a scatter plot to help you visualize that there are plot scattered around and the correlation among them might not be that high because the data is scattered . However , you might find some relatively positive relationship between the two variable scatter plot matrix help you look at the relationship between multiple different feature in Pandi , you can easily create scatter plot matrix based on the column . You want to look at this example , ha three column and it will give the pair wise scatter plot for any two column with a scatter plot . You might want to identify special region that a particular subset of data could fit into . In the example , is there a relationship between alcohol sulfate and quality ? You could plot those value against good and poor quality wine . Like the example , plotting give you an idea of how useful particular variable can be . If you 're using them for a classification problem . That 's it . For part two of this section , we 'll see you again for part three where we 'll review correlation and the takeaway for this section .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'Now', 'we', \"'ll\", 'review', 'how', 'to', 'find', 'correlations', 'in', 'your', 'data', 'set', '.', 'How', 'can', 'you', 'quantify', 'the', 'linear', 'relationship', 'among', 'the', 'variables', 'you', \"'re\", 'seeing', 'in', 'a', 'scanner', 'plot', '?', 'A', 'correlation', 'matrix', 'is', 'a', 'good', 'tool', '.', 'In', 'this', 'situation', ',', 'it', 'conveys', 'both', 'the', 'strong', 'and', 'weak', 'linear', 'relationships', 'among', 'numerical', 'variables', '.', 'Correlation', 'can', 'go', 'as', 'high', 'as', 'one', 'or', 'as', 'low', 'as', 'minus', 'one', 'when', 'the', 'correlation', 'is', 'one', ',', 'this', 'means', 'those', 'two', 'numerical', 'features', 'are', 'perfectly', 'correlated', 'with', 'each', 'other', '.', 'It', \"'s\", 'like', 'saying', 'Y', 'is', 'proportional', 'to', 'X', 'when', 'the', 'correlation', 'of', 'those', 'two', 'variables', 'is', 'minus', 'one', '.', 'It', \"'s\", 'like', 'saying', 'Y', 'is', 'proportional', 'to', 'minus', 'X', ',', 'any', 'linear', 'relationship', 'in', 'between', 'can', 'be', 'quantified', 'by', 'the', 'correlation', '.', 'So', 'if', 'the', 'correlation', 'is', 'zero', ',', 'this', 'means', 'there', \"'s\", 'no', 'linear', 'relationship', 'but', 'it', 'does', \"n't\", 'mean', 'that', 'there', \"'s\", 'no', 'relationship', '.', 'It', \"'s\", 'just', 'an', 'indication', 'that', 'there', \"'s\", 'no', 'linear', 'relationship', 'between', 'those', 'two', 'variables', '.', 'However', ',', 'looking', 'at', 'a', 'number', 'is', \"n't\", 'always', 'straightforward', '.', 'Often', 'it', \"'s\", 'easier', 'to', 'view', 'the', 'numbers', 'when', 'they', \"'re\", 'represented', 'by', 'colors', '.', 'Now', 'we', \"'ll\", 'look', 'at', 'the', 'heat', 'map', 'the', 'highest', 'number', 'one', 'in', 'dark', 'green', 'and', 'minus', 'one', 'is', 'in', 'dark', 'brown', '.', 'The', 'color', 'gives', 'you', 'both', 'the', 'positive', 'and', 'negative', 'directions', '.', 'And', 'it', 'also', 'shows', 'how', 'strong', 'the', 'correlations', 'are', '.', 'We', 'can', 'use', 'the', 'Seaborne', 'heat', 'map', 'function', 'to', 'show', 'the', 'correlation', 'matrix', '.', 'Looking', 'at', 'the', 'chart', ',', 'there', \"'s\", 'some', 'correlation', 'between', 'citric', 'acid', 'and', 'fixed', 'acidity', 'that', 'would', 'be', 'expected', 'in', 'wine', 'because', 'citric', 'acid', 'contributes', 'to', 'the', 'acidity', 'of', 'the', 'wine', '.', 'However', ',', 'there', 'is', \"n't\", 'much', 'correlation', 'between', 'fixed', 'acidity', 'and', 'P', 'H', 'P', 'H', 'is', 'a', 'measurement', 'of', 'the', 'strength', 'of', 'those', 'acids', 'present', '.', 'But', 'fixed', 'acidity', 'is', 'a', 'measure', 'of', 'the', 'quantity', 'in', 'this', 'particular', 'data', 'set', '.', 'There', 'does', \"n't\", 'appear', 'to', 'be', 'a', 'correlation', 'here', '.', 'Some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'include', 'these', 'points', '.', 'The', 'first', 'step', 'is', 'to', 'get', 'your', 'data', 'into', 'a', 'format', 'that', 'can', 'be', 'used', 'easily', '.', 'Pandi', 'is', 'a', 'popular', 'Python', 'library', 'for', 'working', 'with', 'data', 'descriptive', 'statistics', 'will', 'help', 'you', 'gain', 'insights', 'into', 'the', 'data', '.', 'You', 'can', 'use', 'visualizations', 'to', 'examine', 'the', 'data', 'set', 'in', 'more', 'detail', '.', 'That', \"'s\", 'it', 'for', 'this', 'section', '.', 'We', \"'ll\", 'see', 'you', 'again', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . Now we 'll review how to find correlation in your data set . How can you quantify the linear relationship among the variable you 're seeing in a scanner plot ? A correlation matrix is a good tool . In this situation , it conveys both the strong and weak linear relationship among numerical variable . Correlation can go a high a one or a low a minus one when the correlation is one , this mean those two numerical feature are perfectly correlated with each other . It 's like saying Y is proportional to X when the correlation of those two variable is minus one . It 's like saying Y is proportional to minus X , any linear relationship in between can be quantified by the correlation . So if the correlation is zero , this mean there 's no linear relationship but it doe n't mean that there 's no relationship . It 's just an indication that there 's no linear relationship between those two variable . However , looking at a number is n't always straightforward . Often it 's easier to view the number when they 're represented by color . Now we 'll look at the heat map the highest number one in dark green and minus one is in dark brown . The color give you both the positive and negative direction . And it also show how strong the correlation are . We can use the Seaborne heat map function to show the correlation matrix . Looking at the chart , there 's some correlation between citric acid and fixed acidity that would be expected in wine because citric acid contributes to the acidity of the wine . However , there is n't much correlation between fixed acidity and P H P H is a measurement of the strength of those acid present . But fixed acidity is a measure of the quantity in this particular data set . There doe n't appear to be a correlation here . Some key takeaway from this section of the module include these point . The first step is to get your data into a format that can be used easily . Pandi is a popular Python library for working with data descriptive statistic will help you gain insight into the data . You can use visualization to examine the data set in more detail . That 's it for this section . We 'll see you again in the next video .\n",
      "['Hi', 'and', 'welcome', 'to', 'section', 'four', 'in', 'this', 'section', '.', 'We', \"'re\", 'going', 'to', 'look', 'at', 'feature', 'engineering', '.', 'Feature', 'engineering', 'is', 'one', 'of', 'the', 'most', 'impactful', 'things', 'you', 'can', 'do', 'to', 'improve', 'your', 'machine', 'learning', 'model', '.', 'We', \"'ll\", 'now', 'look', 'at', 'what', 'it', 'is', '.', 'There', 'are', 'two', 'things', 'that', 'can', 'help', 'make', 'your', 'models', 'more', 'successful', '.', 'The', 'first', 'is', 'feature', 'selection', 'and', 'the', 'second', 'is', 'feature', 'extraction', 'or', 'the', 'process', 'of', 'creating', 'features', 'in', 'feature', 'selection', '.', 'You', 'select', 'the', 'most', 'relevant', 'features', 'and', 'discard', 'the', 'rest', '.', 'You', 'can', 'apply', 'feature', 'selection', 'to', 'prevent', 'redundancy', 'or', 'irrelevance', 'in', 'the', 'existing', 'features', '.', 'You', 'can', 'also', 'use', 'it', 'to', 'limit', 'the', 'number', 'of', 'features', 'to', 'help', 'prevent', 'overfitting', '.', 'Feature', 'extraction', 'builds', 'valuable', 'information', 'from', 'raw', 'data', 'by', 'reformatting', 'combining', 'and', 'transforming', 'primary', 'features', 'into', 'new', 'ones', '.', 'This', 'process', 'continues', 'until', 'it', 'yields', 'a', 'new', 'data', 'set', 'that', 'can', 'be', 'consumed', 'by', 'the', 'model', 'to', 'achieve', 'your', 'goals', '.', 'As', 'the', 'diagram', 'shows', 'feature', 'extraction', 'covers', 'a', 'range', 'of', 'activities', 'from', 'dealing', 'with', 'missing', 'data', 'to', 'converting', 'text', 'data', 'into', 'numerical', 'data', '.', 'Although', 'the', 'list', 'is', \"n't\", 'exhaustive', ',', 'it', 'should', 'give', 'you', 'some', 'idea', 'of', 'the', 'data', 'handling', 'that', \"'s\", 'needed', 'to', 'get', 'data', 'into', 'a', 'useful', 'state', '.', 'Many', 'of', 'the', 'tasks', 'are', 'no', 'different', 'than', 'any', 'other', 'job', 'working', 'with', 'data', '.', 'You', \"'ll\", 'want', 'to', 'make', 'sure', 'data', 'is', 'in', 'the', 'correct', 'format', 'that', 'it', \"'s\", 'consistently', 'represented', 'correctly', 'spelled', 'among', 'other', 'tasks', '.', 'For', 'example', ',', 'you', 'might', 'combine', 'data', 'or', 'extract', 'data', 'into', 'multiple', 'columns', 'or', 'you', 'could', 'also', 'remove', 'columns', 'altogether', '.', 'Specific', 'to', 'machine', 'learning', '.', 'You', \"'ll\", 'need', 'to', 'convert', 'text', 'columns', 'to', 'numerical', 'values', '.', 'You', \"'ll\", 'also', 'need', 'to', 'decide', 'how', 'to', 'handle', 'outliers', 'and', 'potentially', 'res', 'scale', 'your', 'data', '.', 'Next', ',', 'we', \"'ll\", 'look', 'at', 'some', 'of', 'the', 'more', 'common', 'tasks', 'in', 'this', 'section', '.', 'Most', 'machine', 'learning', 'algorithms', 'work', 'best', 'with', 'numerical', 'data', '.', 'You', \"'ll\", 'need', 'to', 'make', 'sure', 'that', 'all', 'columns', 'in', 'your', 'data', 'set', 'contain', 'numeric', 'data', 'by', 'converting', 'or', 'encoding', 'it', '.', 'You', 'might', 'need', 'to', 'make', 'several', 'passes', 'through', 'the', 'data', 'sheet', 'before', 'you', 'can', 'encode', 'it', '.', 'For', 'example', ',', 'you', 'might', 'have', 'variability', 'in', 'the', 'text', 'values', 'such', 'as', 'rows', 'that', 'contain', 'both', 'medium', 'and', 'M', 'E', 'D', 'as', 'values', '.', 'If', 'the', 'categorical', 'data', 'has', 'order', 'to', 'it', ',', 'you', \"'ll\", 'want', 'to', 'encode', 'the', 'text', 'into', 'numerical', 'values', 'that', 'capture', 'this', 'ordinal', 'relationship', '.', 'Say', 'you', 'have', 'data', 'showing', 'maintenance', 'costs', ',', 'you', 'might', 'encode', 'low', 'to', 'one', ',', 'medium', 'to', 'two', ',', 'high', 'to', 'three', 'and', 'very', 'high', 'to', 'four', '.', 'After', 'you', \"'ve\", 'made', 'sure', 'your', 'categorical', 'data', 'is', 'all', 'uniform', ',', 'you', 'can', 'use', 'tools', 'like', 'sky', 'kit', 'learn', 'and', 'pandas', 'to', 'encode', 'your', 'data', '.', 'If', 'the', 'categorical', 'data', 'does', \"n't\", 'have', 'any', 'order', 'to', 'it', ',', 'then', 'you', \"'ll\", 'need', 'to', 'break', 'the', 'data', 'into', 'multiple', 'columns', 'this', 'will', 'help', 'make', 'sure', 'you', 'do', \"n't\", 'introduce', 'an', 'ordinal', 'relationship', 'to', 'the', 'data', 'that', 'is', \"n't\", 'there', '.', 'For', 'example', ',', 'suppose', 'you', 'assigned', 'a', 'value', 'of', 'one', 'to', 'the', 'first', 'color', 'such', 'as', 'red', 'and', 'you', 'then', 'assign', 'two', 'to', 'the', 'next', 'value', '.', 'Say', 'blue', ',', 'the', 'model', 'could', 'interpret', 'blue', 'as', 'being', 'more', 'important', 'than', 'red', 'because', 'blue', 'has', 'a', 'higher', 'numeric', 'value', 'encoding', 'non', 'ordinal', 'data', 'into', 'multiple', 'columns', 'or', 'features', 'is', 'a', 'better', 'way', '.', 'Think', 'of', 'the', 'new', 'features', 'like', 'a', 'check', 'box', '.', 'Consider', 'the', 'example', ',', 'there', 'are', 'three', 'features', 'that', 'were', 'generated', '.', 'The', 'value', 'one', 'indicates', 'that', 'the', 'instance', 'has', 'that', 'feature', 'like', 'its', 'color', '.', 'That', \"'s\", 'it', 'for', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome to section four in this section . We 're going to look at feature engineering . Feature engineering is one of the most impactful thing you can do to improve your machine learning model . We 'll now look at what it is . There are two thing that can help make your model more successful . The first is feature selection and the second is feature extraction or the process of creating feature in feature selection . You select the most relevant feature and discard the rest . You can apply feature selection to prevent redundancy or irrelevance in the existing feature . You can also use it to limit the number of feature to help prevent overfitting . Feature extraction build valuable information from raw data by reformatting combining and transforming primary feature into new one . This process continues until it yield a new data set that can be consumed by the model to achieve your goal . As the diagram show feature extraction cover a range of activity from dealing with missing data to converting text data into numerical data . Although the list is n't exhaustive , it should give you some idea of the data handling that 's needed to get data into a useful state . Many of the task are no different than any other job working with data . You 'll want to make sure data is in the correct format that it 's consistently represented correctly spelled among other task . For example , you might combine data or extract data into multiple column or you could also remove column altogether . Specific to machine learning . You 'll need to convert text column to numerical value . You 'll also need to decide how to handle outlier and potentially re scale your data . Next , we 'll look at some of the more common task in this section . Most machine learning algorithm work best with numerical data . You 'll need to make sure that all column in your data set contain numeric data by converting or encoding it . You might need to make several pass through the data sheet before you can encode it . For example , you might have variability in the text value such a row that contain both medium and M E D a value . If the categorical data ha order to it , you 'll want to encode the text into numerical value that capture this ordinal relationship . Say you have data showing maintenance cost , you might encode low to one , medium to two , high to three and very high to four . After you 've made sure your categorical data is all uniform , you can use tool like sky kit learn and panda to encode your data . If the categorical data doe n't have any order to it , then you 'll need to break the data into multiple column this will help make sure you do n't introduce an ordinal relationship to the data that is n't there . For example , suppose you assigned a value of one to the first color such a red and you then assign two to the next value . Say blue , the model could interpret blue a being more important than red because blue ha a higher numeric value encoding non ordinal data into multiple column or feature is a better way . Think of the new feature like a check box . Consider the example , there are three feature that were generated . The value one indicates that the instance ha that feature like it color . That 's it for this section , we 'll see you again in the next video .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'feature', 'engineering', 'by', 'reviewing', 'how', 'to', 'clean', 'your', 'data', 'set', '.', 'In', 'addition', 'to', 'converting', 'string', 'data', 'to', 'numerical', 'data', ',', 'you', \"'ll\", 'need', 'to', 'clean', 'your', 'data', 'set', 'for', 'several', 'other', 'potential', 'problem', 'areas', 'before', 'encoding', 'the', 'string', 'data', ',', 'make', 'sure', 'the', 'strings', 'are', 'all', 'consistent', '.', 'You', \"'ll\", 'also', 'need', 'to', 'make', 'sure', 'variables', 'use', 'a', 'consistent', 'scale', '.', 'For', 'example', ',', 'if', 'one', 'variable', 'describes', 'the', 'number', 'of', 'doors', 'in', 'a', 'car', ',', 'the', 'scale', 'will', 'probably', 'be', 'between', 'two', 'and', 'eight', '.', 'But', 'if', 'another', 'variable', 'describes', 'the', 'number', 'of', 'cars', 'of', 'a', 'particular', 'type', 'sold', 'in', 'the', 'state', 'of', 'California', ',', 'the', 'scale', 'will', 'probably', 'be', 'in', 'the', 'thousands', '.', 'Some', 'data', 'items', 'might', 'also', 'capture', 'more', 'than', 'one', 'variable', 'in', 'a', 'single', 'value', '.', 'For', 'instance', ',', 'suppose', 'the', 'data', 'set', 'includes', 'variables', 'that', 'combine', 'safety', 'and', 'maintenance', 'into', 'a', 'single', 'variable', 'such', 'as', 'safe', ',', 'high', 'maintenance', ',', 'you', \"'ll\", 'need', 'to', 'train', 'your', 'machine', 'learning', 'system', 'for', 'both', 'variables', 'and', 'also', 'split', 'that', 'single', 'variable', 'into', 'two', 'separate', 'variables', '.', 'You', 'might', 'also', 'encounter', 'data', 'sets', 'that', 'are', 'missing', 'data', 'for', 'some', 'variables', 'and', 'some', 'data', 'sets', 'will', 'include', 'outliers', '.', 'We', \"'ll\", 'cover', 'techniques', 'for', 'dealing', 'with', 'these', 'situations', '.', 'In', 'this', 'section', '.', 'You', 'might', 'find', 'that', 'data', 'is', 'missing', '.', 'For', 'example', ',', 'some', 'columns', 'in', 'your', 'data', 'set', 'could', 'be', 'missing', 'data', 'because', 'of', 'a', 'data', 'collection', 'error', 'or', 'maybe', 'data', 'was', \"n't\", 'collected', 'on', 'a', 'particular', 'feature', 'until', 'the', 'data', 'collection', 'process', 'was', 'underway', '.', 'Missing', 'data', 'can', 'make', 'it', 'difficult', 'to', 'accurately', 'interpret', 'the', 'relationship', 'between', 'the', 'related', 'feature', 'and', 'the', 'target', 'variable', '.', 'So', 'regardless', 'of', 'how', 'the', 'data', 'ended', 'up', 'being', 'missed', ',', 'it', \"'s\", 'important', 'for', 'you', 'to', 'deal', 'with', 'this', 'issue', '.', 'Unfortunately', ',', 'most', 'machine', 'learning', 'algorithms', 'ca', \"n't\", 'handle', 'missing', 'values', 'automatically', '.', 'You', \"'ll\", 'need', 'to', 'use', 'human', 'intelligence', 'to', 'update', 'missing', 'values', 'with', 'data', 'that', \"'s\", 'meaningful', 'and', 'relevant', 'to', 'the', 'problem', '.', 'Most', 'Python', 'libraries', 'for', 'data', 'manipulation', 'include', 'functions', 'for', 'finding', 'missing', 'data', '.', 'So', 'how', 'do', 'you', 'decide', 'if', 'you', 'should', 'drop', 'or', 'impute', 'missing', 'values', '?', 'This', 'question', 'is', 'answered', 'in', 'part', 'by', 'better', 'understanding', 'how', 'those', 'values', 'came', 'to', 'be', 'missing', 'in', 'the', 'first', 'place', 'and', 'how', 'much', 'data', 'the', 'missing', 'values', 'represent', 'within', 'your', 'larger', 'data', 'set', '.', 'For', 'instance', ',', 'say', 'the', 'missing', 'values', 'are', 'randomly', 'spread', 'throughout', 'your', 'data', 'set', 'and', 'do', \"n't\", 'represent', 'a', 'larger', 'portion', 'of', 'its', 'respective', 'row', 'or', 'column', '.', 'In', 'this', 'case', ',', 'imputation', 'is', 'most', 'likely', 'the', 'better', 'option', '.', 'In', 'contrast', ',', 'say', 'that', 'you', 'have', 'a', 'column', 'or', 'row', 'that', 'has', 'a', 'large', 'percentage', 'of', 'missing', 'values', '.', 'In', 'this', 'case', ',', 'dropping', 'the', 'entire', 'row', 'or', 'column', 'would', 'be', 'preferred', 'over', 'imputation', '.', 'If', 'you', 'decide', 'to', 'drop', 'rows', 'with', 'missing', 'data', ',', 'you', 'can', 'use', 'built', 'in', 'functions', 'to', 'do', 'this', 'for', 'example', ',', 'pandas', 'drop', 'N', 'A', 'function', 'can', 'drop', 'all', 'rows', 'with', 'missing', 'data', 'or', 'you', 'can', 'drop', 'specific', 'data', 'values', 'by', 'using', 'a', 'subset', 'as', 'an', 'alternative', 'to', 'dropping', 'missing', 'values', '.', 'You', 'can', 'impute', 'values', 'for', 'those', 'missing', 'values', '.', 'There', 'are', 'different', 'ways', 'to', 'impute', 'a', 'missing', 'value', 'for', 'categorical', 'values', '.', 'The', 'missing', 'value', 'is', 'usually', 'replaced', 'with', 'the', 'mean', ',', 'the', 'median', 'or', 'the', 'most', 'frequent', 'values', 'for', 'numerical', 'or', 'continuous', 'variables', '.', 'The', 'missing', 'value', 'is', 'usually', 'replaced', 'with', 'the', 'mean', 'or', 'the', 'median', '.', 'You', 'can', 'impute', 'a', 'single', 'row', 'of', 'missing', 'data', 'which', 'is', 'known', 'as', 'una', 'variate', '.', 'You', 'can', 'also', 'do', 'this', 'for', 'multiple', 'rows', 'which', 'is', 'known', 'as', 'Multivariate', '.', 'We', \"'ll\", 'now', 'look', 'at', 'a', 'Univ', 'variate', 'example', 'here', ',', 'the', 'ci', 'pit', 'learn', 'computer', 'function', 'is', 'being', 'used', 'to', 'impute', 'some', 'missing', 'values', '.', 'It', \"'s\", 'a', 'fairly', 'small', 'data', 'set', ',', 'but', 'there', 'are', 'two', 'missing', 'values', '.', 'The', 'missing', 'value', 'was', 'imputed', 'by', 'the', 'strategy', 'of', 'the', 'mean', 'to', 'do', 'this', '.', 'You', 'first', 'calculate', 'the', 'mean', 'here', ',', 'it', \"'s\", 'the', 'mean', 'of', 'three', 'and', 'two', ',', 'which', 'is', '2.5', '.', 'Then', 'you', \"'ll\", 'impute', 'the', 'mean', 'value', 'for', 'the', 'missing', 'value', '.', 'Some', 'data', 'libraries', 'include', 'an', 'impute', 'package', 'that', 'provides', 'more', 'complex', 'ways', 'to', 'impute', 'data', '.', 'Examples', 'include', 'K', 'nearest', 'neighbor', 'soft', 'impute', 'multiple', 'imputation', 'by', 'chain', 'equations', 'and', 'others', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'two', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'three', 'where', 'we', \"'ll\", 'review', 'how', 'to', 'work', 'with', 'outliers', 'in', 'your', 'data', '.']\n",
      "Hi , welcome back . We 'll continue exploring feature engineering by reviewing how to clean your data set . In addition to converting string data to numerical data , you 'll need to clean your data set for several other potential problem area before encoding the string data , make sure the string are all consistent . You 'll also need to make sure variable use a consistent scale . For example , if one variable describes the number of door in a car , the scale will probably be between two and eight . But if another variable describes the number of car of a particular type sold in the state of California , the scale will probably be in the thousand . Some data item might also capture more than one variable in a single value . For instance , suppose the data set includes variable that combine safety and maintenance into a single variable such a safe , high maintenance , you 'll need to train your machine learning system for both variable and also split that single variable into two separate variable . You might also encounter data set that are missing data for some variable and some data set will include outlier . We 'll cover technique for dealing with these situation . In this section . You might find that data is missing . For example , some column in your data set could be missing data because of a data collection error or maybe data wa n't collected on a particular feature until the data collection process wa underway . Missing data can make it difficult to accurately interpret the relationship between the related feature and the target variable . So regardless of how the data ended up being missed , it 's important for you to deal with this issue . Unfortunately , most machine learning algorithm ca n't handle missing value automatically . You 'll need to use human intelligence to update missing value with data that 's meaningful and relevant to the problem . Most Python library for data manipulation include function for finding missing data . So how do you decide if you should drop or impute missing value ? This question is answered in part by better understanding how those value came to be missing in the first place and how much data the missing value represent within your larger data set . For instance , say the missing value are randomly spread throughout your data set and do n't represent a larger portion of it respective row or column . In this case , imputation is most likely the better option . In contrast , say that you have a column or row that ha a large percentage of missing value . In this case , dropping the entire row or column would be preferred over imputation . If you decide to drop row with missing data , you can use built in function to do this for example , panda drop N A function can drop all row with missing data or you can drop specific data value by using a subset a an alternative to dropping missing value . You can impute value for those missing value . There are different way to impute a missing value for categorical value . The missing value is usually replaced with the mean , the median or the most frequent value for numerical or continuous variable . The missing value is usually replaced with the mean or the median . You can impute a single row of missing data which is known a una variate . You can also do this for multiple row which is known a Multivariate . We 'll now look at a Univ variate example here , the ci pit learn computer function is being used to impute some missing value . It 's a fairly small data set , but there are two missing value . The missing value wa imputed by the strategy of the mean to do this . You first calculate the mean here , it 's the mean of three and two , which is 2.5 . Then you 'll impute the mean value for the missing value . Some data library include an impute package that provides more complex way to impute data . Examples include K nearest neighbor soft impute multiple imputation by chain equation and others . That 's it . For part two of this section , we 'll see you again for part three where we 'll review how to work with outlier in your data .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'feature', 'engineering', 'by', 'describing', 'how', 'to', 'work', 'with', 'outliers', '.', 'You', 'might', 'also', 'need', 'to', 'clean', 'your', 'data', 'based', 'on', 'any', 'outliers', 'that', 'exist', '.', 'Outliers', 'are', 'points', 'in', 'your', 'data', 'set', 'that', 'lie', 'at', 'an', 'abnormal', 'distance', 'from', 'other', 'values', '.', 'They', \"'re\", 'not', 'always', 'something', 'you', 'want', 'to', 'clean', 'up', 'because', 'they', 'can', 'add', 'richness', 'to', 'your', 'data', 'set', '.', 'But', 'they', 'can', 'also', 'make', 'it', 'harder', 'to', 'make', 'accurate', 'predictions', 'because', 'they', 'skew', 'values', 'away', 'from', 'the', 'other', 'more', 'normal', 'values', 'related', 'to', 'that', 'feature', '.', 'An', 'outlier', 'might', 'also', 'indicate', 'that', 'the', 'data', 'point', 'actually', 'belongs', 'to', 'another', 'column', '.', 'You', 'can', 'think', 'of', 'outliers', 'as', 'falling', 'into', 'two', 'broad', 'categories', '.', 'The', 'first', 'is', 'a', 'single', 'variation', 'for', 'just', 'a', 'single', 'variable', 'or', 'a', 'uni', 'variate', 'outlier', '.', 'The', 'second', 'is', 'a', 'variation', 'of', 'two', 'or', 'more', 'variables', 'or', 'a', 'Multivariate', 'outlier', '.', 'One', 'of', 'the', 'more', 'common', 'ways', 'to', 'find', 'unvaried', 'outliers', 'is', 'with', 'a', 'box', 'plot', '.', 'A', 'box', 'plot', 'shows', 'how', 'far', 'a', 'data', 'point', 'is', 'to', 'the', 'mean', 'for', 'that', 'variable', '.', 'The', 'box', 'in', 'the', 'plot', 'shows', 'the', 'data', 'values', 'within', 'two', 'core', 'tiles', 'of', 'the', 'mean', 'values', 'outside', 'that', 'range', 'are', 'represented', 'by', 'the', 'lines', 'extending', 'from', 'the', 'box', 'which', 'are', 'sometimes', 'called', 'whiskers', '.', 'A', 'scatter', 'plot', 'can', 'be', 'an', 'effective', 'way', 'to', 'see', 'Multivariate', 'outliers', '.', 'For', 'example', ',', 'this', 'diagram', 'shows', 'the', 'amount', 'of', 'sulfates', 'and', 'alcohol', 'in', 'a', 'collection', 'of', 'wines', 'with', 'the', 'scatter', 'plot', '.', 'You', 'can', 'quickly', 'visualize', 'whether', 'there', 'are', 'Multivariate', 'outliers', 'for', 'the', 'two', 'variables', '.', 'The', 'origin', 'of', 'your', 'outlier', 'will', 'most', 'likely', 'inform', 'how', 'you', 'deal', 'with', 'it', 'during', 'this', 'pre', 'processing', 'phase', 'of', 'the', 'pipeline', 'or', 'possibly', 'later', '.', 'During', 'feature', 'engineering', ',', 'there', 'are', 'several', 'different', 'approaches', 'to', 'dealing', 'with', 'outliers', '.', 'You', 'could', 'delete', 'the', 'outlier', 'if', 'your', 'outlier', 'is', 'based', 'on', 'an', 'artificial', 'error', '.', 'This', 'means', 'the', 'outlier', 'is', \"n't\", 'natural', '.', 'It', 'was', 'introduced', 'because', 'of', 'some', 'failure', ',', 'like', 'incorrectly', 'entered', 'data', '.', 'You', 'could', 'also', 'transform', 'the', 'outlier', 'by', 'taking', 'the', 'natural', 'log', 'of', 'a', 'value', '.', 'This', 'in', 'turn', 'reduces', 'the', 'variation', 'caused', 'by', 'the', 'extreme', 'outlier', 'value', 'which', 'would', 'then', 'reduce', 'the', 'outlier', 'influence', 'on', 'the', 'overall', 'data', 'set', '.', 'Finally', ',', 'you', 'could', 'use', 'the', 'mean', 'of', 'the', 'feature', 'and', 'impute', 'that', 'value', 'to', 'replace', 'the', 'outlier', 'value', '.', 'Again', ',', 'this', 'would', 'be', 'a', 'good', 'approach', '.', 'If', 'the', 'outlier', 'was', 'caused', 'by', 'artificial', 'error', ',', 'this', 'is', \"n't\", 'an', 'exhaustive', 'list', ',', 'but', 'it', 'describes', 'the', 'most', 'common', 'options', '.', 'After', 'you', \"'ve\", 'extracted', 'features', ',', 'you', \"'ll\", 'need', 'to', 'select', 'the', 'most', 'appropriate', 'features', 'for', 'training', 'your', 'model', '.', 'There', 'are', 'three', 'main', 'feature', 'selection', 'methods', '.', 'Filter', 'methods', 'use', 'statistical', 'methods', 'to', 'measure', 'the', 'relevance', 'of', 'features', 'by', 'their', 'correlation', 'with', 'the', 'target', 'variable', 'wrapper', 'methods', 'measure', 'how', 'useful', '.', 'A', 'subset', 'of', 'a', 'feature', 'is', 'they', 'do', 'this', 'by', 'training', 'a', 'model', 'on', 'the', 'feature', 'and', 'then', 'measuring', 'how', 'successful', 'the', 'model', 'is', '.', 'Filters', 'are', 'faster', 'and', 'cheaper', 'than', 'wrapper', 'methods', 'because', 'they', 'do', \"n't\", 'involve', 'training', 'the', 'models', 'repeatedly', '.', 'Wrappers', 'typically', 'find', 'the', 'best', 'subset', 'of', 'features', '.', 'But', 'there', \"'s\", 'a', 'risk', 'of', 'overfitting', 'compared', 'to', 'using', 'subsets', 'of', 'features', 'from', 'filter', 'methods', '.', 'Embedded', 'methods', 'are', 'algorithm', 'specific', 'and', 'they', 'might', 'use', 'a', 'combination', 'of', 'both', 'filters', 'and', 'wrappers', '.', 'Filter', 'methods', 'use', 'a', 'proxy', 'measure', 'instead', 'of', 'the', 'actual', 'model', \"'s\", 'performance', ',', 'they', \"'re\", 'fast', 'to', 'compute', 'but', 'they', 'can', 'still', 'capture', 'how', 'useful', 'the', 'feature', 'set', 'is', '.', 'Here', 'are', 'some', 'common', 'measures', '.', 'The', 'first', 'is', 'Pearson', \"'s\", 'correlation', 'coefficient', 'which', 'measures', 'the', 'statistical', 'relationship', 'or', 'association', 'between', 'two', 'continuous', 'variables', '.', 'The', 'second', 'is', 'linear', 'discriminant', 'analysis', 'or', 'LDA', '.', 'This', 'is', 'used', 'to', 'find', 'a', 'linear', 'combination', 'of', 'features', 'that', 'separates', 'two', 'or', 'more', 'classes', '.', 'The', 'third', 'is', 'analysis', 'of', 'variance', 'or', 'a', 'Nova', '.', 'This', 'is', 'used', 'to', 'analyze', 'the', 'differences', 'among', 'group', 'means', 'in', 'a', 'sample', '.', 'And', 'finally', ',', 'chi', 'square', 'is', 'a', 'single', 'number', 'that', 'tells', 'you', 'how', 'much', 'difference', 'exists', 'between', 'your', 'observed', 'counts', 'and', 'the', 'counts', 'you', \"'d\", 'expect', 'if', 'there', 'were', 'absolutely', 'no', 'relationships', 'in', 'the', 'population', 'filters', 'are', 'usually', 'less', 'computationally', 'intensive', 'than', 'wrappers', '.', 'But', 'they', 'produce', 'a', 'feature', 'set', 'that', 'is', \"n't\", 'tuned', 'to', 'a', 'specific', 'type', 'of', 'predictive', 'model', '.', 'This', 'lack', 'of', 'tuning', 'means', 'a', 'feature', 'set', 'from', 'a', 'filter', 'is', 'more', 'general', 'than', 'one', 'from', 'a', 'wrapper', ',', 'the', 'filter', 'also', 'usually', 'has', 'a', 'lower', 'prediction', 'performance', 'than', 'a', 'wrapper', '.', 'However', ',', 'the', 'filter', \"'s\", 'feature', 'set', 'does', \"n't\", 'contain', 'the', 'assumptions', 'of', 'a', 'prediction', 'model', '.', 'So', 'it', \"'s\", 'more', 'useful', 'for', 'exposing', 'relationships', 'between', 'features', '.', 'Many', 'filters', 'provide', 'feature', 'ranking', 'instead', 'of', 'an', 'explicit', 'best', 'feature', 'subset', '.', 'And', 'the', 'cut', 'off', 'point', 'in', 'the', 'ranking', 'is', 'chosen', 'through', 'cross', 'validation', 'filters', 'have', 'also', 'been', 'used', 'as', 'a', 'preprocessing', 'step', 'for', 'wrappers', 'which', 'enables', 'a', 'wrapper', 'to', 'be', 'used', 'on', 'larger', 'problems', '.', 'Wrapper', 'methods', 'use', 'a', 'predictive', 'model', 'to', 'score', 'feature', 'subsets', '.', 'Each', 'new', 'subset', 'is', 'used', 'to', 'train', 'a', 'model', 'which', 'is', 'then', 'tested', 'on', 'a', 'holdout', 'set', '.', 'The', 'score', 'for', 'that', 'subset', 'is', 'calculated', 'by', 'counting', 'the', 'number', 'of', 'mistakes', 'made', 'on', 'that', 'holdout', 'set', 'or', 'the', 'error', 'rate', 'of', 'the', 'model', '.', 'Because', 'rappers', 'train', 'a', 'new', 'model', 'for', 'each', 'subset', 'they', \"'re\", 'computationally', 'intensive', '.', 'However', ',', 'they', 'usually', 'provide', 'the', 'best', 'performing', 'feature', 'set', 'for', 'that', 'particular', 'type', 'of', 'model', 'or', 'problem', 'forward', 'selection', 'starts', 'with', 'no', 'features', 'and', 'adds', 'them', 'until', 'the', 'best', 'model', 'is', 'found', 'backward', 'selection', 'starts', 'with', 'all', 'features', ',', 'drops', 'them', 'one', 'at', 'a', 'time', 'and', 'then', 'selects', 'the', 'best', 'model', '.', 'Embedded', 'methods', 'combine', 'the', 'qualities', 'of', 'filter', 'and', 'wrapper', 'methods', '.', 'They', \"'re\", 'implemented', 'by', 'algorithms', 'that', 'have', 'their', 'own', 'built', 'in', 'feature', 'selection', 'methods', '.', 'Some', 'of', 'the', 'most', 'popular', 'examples', 'of', 'these', 'methods', 'are', 'lasso', 'and', 'ridge', 'regression', '.', 'They', 'have', 'built', 'in', 'penalization', 'functions', 'to', 'reduce', 'overfitting', '.', 'Here', 'are', 'some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', '.', 'First', 'feature', 'engineering', 'involves', 'selecting', 'the', 'best', 'features', 'for', 'machine', 'learning', 'preprocessing', ',', 'gives', 'you', 'better', 'data', 'to', 'work', 'with', 'and', 'better', 'data', 'typically', 'provides', 'better', 'results', '.', 'Two', 'categories', 'for', 'preprocessing', 'are', 'converting', 'data', 'to', 'numerical', 'values', 'and', 'cleaning', 'up', 'dirty', 'data', 'by', 'removing', 'missing', 'data', 'and', 'cleaning', 'outliers', '.', 'Finally', ',', 'how', 'you', 'handle', 'dirty', 'data', 'impacts', 'your', 'model', '?', 'That', \"'s\", 'it', '.', 'For', 'section', 'four', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . We 'll continue exploring feature engineering by describing how to work with outlier . You might also need to clean your data based on any outlier that exist . Outliers are point in your data set that lie at an abnormal distance from other value . They 're not always something you want to clean up because they can add richness to your data set . But they can also make it harder to make accurate prediction because they skew value away from the other more normal value related to that feature . An outlier might also indicate that the data point actually belongs to another column . You can think of outlier a falling into two broad category . The first is a single variation for just a single variable or a uni variate outlier . The second is a variation of two or more variable or a Multivariate outlier . One of the more common way to find unvaried outlier is with a box plot . A box plot show how far a data point is to the mean for that variable . The box in the plot show the data value within two core tile of the mean value outside that range are represented by the line extending from the box which are sometimes called whisker . A scatter plot can be an effective way to see Multivariate outlier . For example , this diagram show the amount of sulfate and alcohol in a collection of wine with the scatter plot . You can quickly visualize whether there are Multivariate outlier for the two variable . The origin of your outlier will most likely inform how you deal with it during this pre processing phase of the pipeline or possibly later . During feature engineering , there are several different approach to dealing with outlier . You could delete the outlier if your outlier is based on an artificial error . This mean the outlier is n't natural . It wa introduced because of some failure , like incorrectly entered data . You could also transform the outlier by taking the natural log of a value . This in turn reduces the variation caused by the extreme outlier value which would then reduce the outlier influence on the overall data set . Finally , you could use the mean of the feature and impute that value to replace the outlier value . Again , this would be a good approach . If the outlier wa caused by artificial error , this is n't an exhaustive list , but it describes the most common option . After you 've extracted feature , you 'll need to select the most appropriate feature for training your model . There are three main feature selection method . Filter method use statistical method to measure the relevance of feature by their correlation with the target variable wrapper method measure how useful . A subset of a feature is they do this by training a model on the feature and then measuring how successful the model is . Filters are faster and cheaper than wrapper method because they do n't involve training the model repeatedly . Wrappers typically find the best subset of feature . But there 's a risk of overfitting compared to using subset of feature from filter method . Embedded method are algorithm specific and they might use a combination of both filter and wrapper . Filter method use a proxy measure instead of the actual model 's performance , they 're fast to compute but they can still capture how useful the feature set is . Here are some common measure . The first is Pearson 's correlation coefficient which measure the statistical relationship or association between two continuous variable . The second is linear discriminant analysis or LDA . This is used to find a linear combination of feature that separate two or more class . The third is analysis of variance or a Nova . This is used to analyze the difference among group mean in a sample . And finally , chi square is a single number that tell you how much difference exists between your observed count and the count you 'd expect if there were absolutely no relationship in the population filter are usually le computationally intensive than wrapper . But they produce a feature set that is n't tuned to a specific type of predictive model . This lack of tuning mean a feature set from a filter is more general than one from a wrapper , the filter also usually ha a lower prediction performance than a wrapper . However , the filter 's feature set doe n't contain the assumption of a prediction model . So it 's more useful for exposing relationship between feature . Many filter provide feature ranking instead of an explicit best feature subset . And the cut off point in the ranking is chosen through cross validation filter have also been used a a preprocessing step for wrapper which enables a wrapper to be used on larger problem . Wrapper method use a predictive model to score feature subset . Each new subset is used to train a model which is then tested on a holdout set . The score for that subset is calculated by counting the number of mistake made on that holdout set or the error rate of the model . Because rapper train a new model for each subset they 're computationally intensive . However , they usually provide the best performing feature set for that particular type of model or problem forward selection start with no feature and add them until the best model is found backward selection start with all feature , drop them one at a time and then selects the best model . Embedded method combine the quality of filter and wrapper method . They 're implemented by algorithm that have their own built in feature selection method . Some of the most popular example of these method are lasso and ridge regression . They have built in penalization function to reduce overfitting . Here are some key takeaway from this section of the module . First feature engineering involves selecting the best feature for machine learning preprocessing , give you better data to work with and better data typically provides better result . Two category for preprocessing are converting data to numerical value and cleaning up dirty data by removing missing data and cleaning outlier . Finally , how you handle dirty data impact your model ? That 's it . For section four , we 'll see you in the next video .\n",
      "['Hi', ',', 'welcome', 'back', 'to', 'module', 'three', '.', 'This', 'is', 'section', 'five', 'on', 'training', 'in', 'this', 'section', '.', 'We', \"'re\", 'going', 'to', 'look', 'at', 'how', 'to', 'select', 'a', 'model', 'and', 'train', 'it', 'with', 'the', 'data', 'we', 'have', 'pre', 'processed', 'at', 'this', 'point', '.', 'You', \"'ve\", 'done', 'a', 'lot', 'to', 'clean', 'and', 'prepare', 'your', 'data', ',', 'but', 'that', 'does', \"n't\", 'mean', 'your', 'data', 'is', 'completely', 'ready', 'to', 'train', 'the', 'algorithm', '.', 'Some', 'algorithms', 'may', 'not', 'be', 'able', 'to', 'work', 'with', 'training', 'data', 'in', 'a', 'data', 'frame', 'format', '.', 'Some', 'file', 'formats', 'like', 'CS', 'V', 'are', 'commonly', 'used', 'by', 'various', 'algorithms', 'but', 'they', 'do', 'not', 'make', 'use', 'of', 'that', 'optimization', 'that', 'some', 'of', 'the', 'file', 'formats', 'like', 'record', 'IO', 'proto', 'buff', 'can', 'use', 'many', 'Amazon', 'Sagemaker', 'algorithms', 'support', 'training', 'with', 'data', 'in', 'a', 'CS', 'V', 'format', '.', 'Amazon', \"'s\", 'sagemaker', 'requires', 'that', 'a', 'CS', 'V', 'file', 'does', \"n't\", 'have', 'a', 'header', 'record', 'and', 'that', 'the', 'target', 'variable', 'is', 'in', 'the', 'first', 'column', '.', 'Most', 'Amazon', 'sagemaker', 'algorithms', 'work', 'best', 'when', 'you', 'use', 'the', 'optimized', 'proto', 'buff', 'record', 'I', 'O', 'format', 'for', 'the', 'training', 'data', 'using', 'this', 'format', 'allows', 'you', 'to', 'take', 'advantage', 'of', 'pipe', 'mode', 'when', 'training', 'the', 'algorithms', 'that', 'support', 'it', 'in', 'pipe', 'mode', '.', 'Your', 'training', 'job', 'streams', 'data', 'directly', 'from', 'Amazon', 'S3', '.', 'When', 'using', 'the', 'CS', 'V', 'format', ',', 'the', 'target', 'variable', 'in', 'your', 'training', 'data', 'set', 'should', 'be', 'the', 'first', 'column', 'on', 'the', 'left', 'and', 'your', 'features', 'should', 'be', 'to', 'the', 'right', 'of', 'the', 'target', 'variable', 'column', '.', 'Evaluating', 'a', 'model', 'with', 'the', 'same', 'data', 'that', 'it', 'trained', 'on', 'will', 'lead', 'to', 'overfitting', '.', 'Recall', '.', 'Overfitting', 'is', 'where', 'your', 'model', 'learns', 'the', 'particulars', 'of', 'a', 'data', 'set', 'too', '.', 'Well', ',', 'it', \"'s\", 'essentially', 'memorizing', 'the', 'training', 'data', 'rather', 'than', 'learning', 'the', 'relationships', 'between', 'features', 'and', 'labels', '.', 'This', 'means', 'the', 'model', 'is', \"n't\", 'learning', 'from', 'those', 'relationships', 'and', 'patterns', 'to', 'apply', 'them', 'to', 'new', 'data', 'in', 'the', 'future', '.', 'Hold', 'out', 'is', 'when', 'you', 'split', 'your', 'data', 'into', 'multiple', 'sets', ',', 'commonly', 'sets', 'for', 'training', 'data', 'validation', 'data', 'and', 'testing', 'data', ',', 'training', 'data', ',', 'which', 'includes', 'both', 'features', 'and', 'labels', 'feeds', 'into', 'the', 'algorithm', 'you', \"'ve\", 'selected', 'to', 'produce', 'your', 'model', '.', 'You', 'then', 'use', 'the', 'model', 'to', 'make', 'predictions', 'over', 'the', 'validation', 'data', 'set', ',', 'which', 'is', 'where', 'you', \"'ll\", 'likely', 'notice', 'things', 'you', \"'ll\", 'want', 'to', 'tweak', 'and', 'tune', 'and', 'change', '.', 'Then', 'when', 'you', \"'re\", 'ready', ',', 'you', 'run', 'the', 'test', 'data', 'set', ',', 'which', 'only', 'includes', 'features', '.', 'Since', 'you', 'want', 'the', 'labels', 'to', 'actually', 'be', 'predicted', 'the', 'performance', 'you', 'get', 'here', 'with', 'the', 'test', 'data', 'set', 'is', 'what', 'you', 'can', 'reasonably', 'expect', 'to', 'see', 'in', 'production', '.', 'A', 'common', 'split', 'when', 'using', 'the', 'holdout', 'method', 'is', 'using', '80', '%', 'of', 'the', 'data', 'for', 'a', 'training', 'set', ',', '10', '%', 'for', 'validation', 'and', '10', '%', 'for', 'test', '.', 'Or', 'if', 'you', 'have', 'a', 'lot', 'of', 'data', ',', 'you', 'can', 'split', 'it', 'into', '70', '%', 'training', '15', '%', 'validation', 'and', '15', '%', 'test', '.', 'So', 'for', 'a', 'small', 'data', 'set', ',', 'we', 'can', 'use', 'K', 'fold', 'cross', 'validation', 'to', 'utilize', 'as', 'much', 'of', 'the', 'data', 'as', 'possible', 'while', 'still', 'having', 'relatively', 'good', 'metrics', '.', 'In', 'order', 'to', 'choose', 'which', 'model', 'is', 'better', 'K', 'fold', 'cross', 'validation', 'randomly', 'partitions', 'the', 'data', 'into', 'k', 'different', 'segments', '.', 'For', 'each', 'segment', ',', 'we', \"'ll\", 'use', 'the', 'rest', 'of', 'the', 'data', 'outside', 'of', 'it', 'for', 'training', 'in', 'order', 'to', 'do', 'a', 'validation', 'on', 'that', 'particular', 'segment', '.', 'Let', \"'s\", 'look', 'at', 'an', 'example', 'here', 'we', 'have', 'a', 'five', 'fold', 'cross', 'validation', '.', 'The', 'available', 'training', 'data', 'is', 'separated', 'into', 'five', 'different', 'chunks', '.', 'For', 'the', 'training', 'of', 'the', 'first', 'model', '.', 'We', \"'re\", 'using', 'all', 'those', 'chunks', 'as', 'the', 'training', 'data', '.', 'And', 'then', 'we', \"'re\", 'going', 'to', 'calculate', 'the', 'metrics', 'on', 'this', 'test', 'piece', 'for', 'the', 'second', 'bottle', '.', 'We', \"'re\", 'going', 'to', 'use', 'these', 'pieces', 'as', 'training', '.', 'After', 'the', 'model', 'is', 'trained', ',', 'you', 'apply', 'it', 'to', 'this', 'test', 'piece', '.', 'We', 'do', 'the', 'same', 'thing', 'five', 'times', '.', 'We', 'use', 'all', 'the', 'training', 'data', 'and', 'we', 'test', 'it', 'on', 'five', 'different', 'models', 'on', 'different', 'chunks', 'of', 'the', 'test', 'data', ',', 'eventually', 'testing', 'it', 'on', 'all', 'data', 'points', '.', 'One', 'other', 'thing', 'to', 'note', 'about', 'splitting', 'your', 'data', 'data', 'in', 'a', 'specific', 'order', 'can', 'lead', 'to', 'biases', 'on', 'your', 'model', '.', 'This', 'is', 'especially', 'true', 'if', 'you', \"'re\", 'working', 'with', 'structured', 'data', '.', 'For', 'example', ',', 'the', 'wine', 'data', 'is', 'ordered', 'by', 'the', 'quality', 'column', '.', 'When', 'you', 'run', 'your', 'model', 'against', 'your', 'test', 'data', ',', 'this', 'ordered', 'pattern', 'will', 'be', 'applied', ',', 'biasing', 'the', 'model', '.', 'It', 'might', 'also', 'mean', 'that', 'some', 'targets', 'are', 'missing', 'from', 'the', 'training', 'data', ',', 'typically', 'randomizing', 'your', 'dataset', 'prior', 'to', 'splitting', 'is', 'sufficient', '.', 'And', 'many', 'libraries', 'will', 'provide', 'functions', 'for', 'this', 'with', 'smaller', 'sets', '.', 'It', 'is', 'sometimes', 'useful', 'to', 'use', 'stratified', 'sampling', ',', 'stratified', 'sampling', 'ensures', 'that', 'the', 'training', 'and', 'test', 'sets', 'have', 'approximately', 'the', 'same', 'percentage', 'of', 'samples', 'of', 'each', 'target', 'class', 'as', 'the', 'complete', 'set', '.', 'An', 'internet', 'search', 'will', 'give', 'you', 'many', 'ways', 'to', 'shuffle', 'and', 'split', 'the', 'data', '.', 'One', 'of', 'the', 'easiest', 'is', 'to', 'use', 'the', 'train', 'test', 'split', 'function', 'from', 'S', 'K', 'Learn', '.', 'Amazon', 'Sagemaker', 'provides', 'four', 'different', 'ways', '.', 'You', 'can', 'train', 'models', '.', 'The', 'built', 'in', 'algorithms', 'available', 'can', 'be', 'easily', 'deployed', 'from', 'the', 'AWS', 'console', 'CL', 'or', 'a', 'Jupiter', 'notebook', 'containers', 'are', 'used', 'behind', 'the', 'scenes', 'when', 'you', 'use', 'one', 'of', 'the', 'Amazon', 'Sage', 'maker', 'built', 'in', 'algorithms', ',', 'but', 'you', 'do', 'not', 'have', 'to', 'deal', 'with', 'them', 'directly', '.', 'Amazon', 'Sagemaker', 'supported', 'frameworks', 'provide', 'prebuilt', 'containers', 'to', 'support', 'deep', 'learning', 'frameworks', 'such', 'as', 'a', 'patchy', 'M', 'X', 'net', 'tensor', 'flow', 'pie', 'Toch', 'and', 'Chainer', '.', 'It', 'also', 'supports', 'machine', 'learning', 'libraries', 'such', 'as', 'Sky', 'Kit', 'Learn', 'and', 'Spark', 'M', 'L', 'by', 'providing', 'prebuilt', 'Docker', 'images', '.', 'If', 'you', 'use', 'the', 'Amazon', 'Sagemaker', ',', 'Python', 'S', 'D', 'K', ',', 'they', \"'re\", 'deployed', 'using', 'their', 'respective', 'Amazon', 'Sagemaker', 'S', 'D', 'K', 'estimator', 'class', '.', 'If', 'there', 'is', 'no', 'prebuilt', 'Amazon', 'Sagemaker', 'container', 'image', 'that', 'you', 'can', 'use', 'or', 'modify', 'for', 'an', 'advanced', 'scenario', ',', 'you', 'can', 'package', 'your', 'own', 'script', 'or', 'algorithm', 'to', 'use', 'with', 'Amazon', 'Sagemaker', ',', 'you', 'can', 'use', 'any', 'programming', 'language', 'or', 'framework', 'to', 'develop', 'your', 'container', '.', 'For', 'an', 'example', '.', 'If', 'your', 'team', 'works', 'and', 'builds', 'M', 'L', 'models', 'in', 'R', ',', 'you', 'can', 'build', 'your', 'own', 'containers', 'to', 'train', 'and', 'host', 'an', 'algorithm', 'in', 'R', 'as', 'well', '.', 'Someone', 'else', 'may', 'have', 'already', 'developed', 'and', 'tuned', 'a', 'model', '.', 'It', 'is', 'worth', 'looking', 'in', 'the', 'AWS', 'marketplace', 'to', 'find', 'available', 'models', '.', 'Amazon', \"'s\", 'Sagemaker', 'provides', 'high', 'performance', 'scalable', 'machine', 'learning', 'algorithms', 'optimized', 'for', 'speed', 'scale', 'and', 'accuracy', 'for', 'supervised', 'learning', '.', 'Amazon', \"'s\", 'sagemaker', 'includes', 'X', 'G', 'boost', 'and', 'linear', 'learner', 'algorithms', 'for', 'classification', 'and', 'quantitative', 'or', 'regression', 'problems', '.', 'There', 'is', 'also', 'a', 'factorization', 'machine', 'to', 'address', 'recommendation', 'and', 'time', 'series', 'prediction', 'problems', '.', 'Amazon', \"'s\", 'sagemaker', 'includes', 'support', 'for', 'unsupervised', 'learning', 'such', 'as', 'with', 'K', 'means', 'clustering', 'and', 'principal', 'component', 'analysis', 'PC', 'A', 'to', 'solve', 'problems', 'like', 'identifying', 'customer', 'groupings', 'based', 'on', 'purchasing', 'behavior', '.', 'Finally', ',', 'there', 'are', 'a', 'selection', 'of', 'specialized', 'algorithms', 'for', 'processing', 'images', 'and', 'other', 'deep', 'learning', 'tasks', '.', 'Let', \"'s\", 'look', 'a', 'little', 'closer', 'at', 'three', 'of', 'the', 'most', 'commonly', 'used', 'built', 'in', 'algorithms', 'and', 'their', 'use', 'cases', 'X', 'G', 'boost', 'or', 'extreme', 'gradient', 'boosting', 'is', 'a', 'popular', 'and', 'efficient', 'open', 'source', 'implementation', 'of', 'the', 'gradient', 'boosted', 'trees', 'algorithm', '.', 'Gradient', 'boosting', 'is', 'a', 'supervised', 'learning', 'algorithm', 'that', 'attempts', 'to', 'accurately', 'predict', 'a', 'target', 'variable', 'by', 'combining', 'an', 'ensemble', 'of', 'estimates', 'from', 'a', 'set', 'of', 'simpler', 'weaker', 'models', 'X', 'G', 'boost', 'has', 'done', 'remarkably', 'well', 'in', 'machine', 'learning', 'competitions', 'because', 'it', 'robustly', 'handles', 'a', 'variety', 'of', 'data', 'types', 'relationships', 'and', 'distributions', '.', 'The', 'large', 'number', 'of', 'hyper', 'parameters', 'can', 'be', 'tweaked', 'and', 'tuned', 'for', 'improved', 'fit', '.', 'This', 'flexibility', 'makes', 'X', 'G', 'boost', 'a', 'solid', 'choice', 'for', 'problems', 'in', 'regression', 'classification', ',', 'binary', 'and', 'multi', 'class', 'and', 'ranking', '.', 'The', 'Amazon', 'Sagemaker', 'linear', 'learner', 'algorithm', 'provides', 'a', 'solution', 'for', 'both', 'classification', 'and', 'regression', 'problems', '.', 'With', 'the', 'Amazon', 'Sagemaker', 'algorithm', '.', 'You', 'can', 'simultaneously', 'explore', 'different', 'training', 'objectives', 'and', 'choose', 'the', 'best', 'solution', 'from', 'your', 'validation', 'set', '.', 'You', 'can', 'also', 'explore', 'a', 'large', 'number', 'of', 'models', 'and', 'choose', 'the', 'best', 'one', 'for', 'your', 'needs', 'compared', 'with', 'methods', 'that', 'provide', 'a', 'solution', 'for', 'only', 'continuous', 'objectives', '.', 'The', 'Amazon', 'Sagemaker', 'linear', 'learner', 'algorithm', 'provides', 'a', 'significant', 'increase', 'in', 'speed', 'over', 'naive', 'hyper', 'parameter', 'optimization', 'techniques', '.', 'K', 'means', 'is', 'an', 'unsupervised', 'learning', 'algorithm', '.', 'It', 'attempts', 'to', 'find', 'discrete', 'groupings', 'within', 'data', 'where', 'members', 'of', 'a', 'group', 'are', 'as', 'similar', 'as', 'possible', 'to', 'one', 'another', 'and', 'as', 'different', 'as', 'possible', 'from', 'members', 'of', 'other', 'groups', '.', 'You', 'define', 'the', 'attributes', 'that', 'you', 'want', 'the', 'algorithm', 'to', 'use', 'to', 'determine', 'similarity', 'to', 'train', 'a', 'model', '.', 'An', 'Amazon', 'Sagemaker', ',', 'you', 'create', 'a', 'training', 'job', '.', 'The', 'training', 'job', 'includes', 'the', 'URL', 'of', 'the', 'Amazon', 'S3', 'bucket', 'where', 'you', \"'ve\", 'stored', 'the', 'training', 'data', ',', 'the', 'URL', 'of', 'the', 'S3', 'bucket', 'where', 'you', 'want', 'to', 'store', 'the', 'output', 'of', 'the', 'job', '.', 'The', 'Amazon', 'elastic', 'container', 'registry', 'path', 'where', 'the', 'training', 'code', 'is', 'stored', '.', 'The', 'compute', 'resources', 'that', 'you', 'want', '.', 'Amazon', 'Sagemaker', 'to', 'use', 'for', 'model', 'training', 'compute', 'resources', 'are', 'M', 'L', 'compute', 'instances', 'managed', 'by', 'Amazon', 'Sagemaker', ',', 'Amazon', '.', 'Sagemaker', 'provides', 'a', 'selection', 'of', 'instance', 'types', 'optimized', 'to', 'fit', 'different', 'machine', 'learning', '.', 'Use', 'cases', 'instance', 'types', 'comprise', 'varying', 'combinations', 'of', 'C', 'P', 'U', 'GP', 'U', 'memory', 'and', 'networking', 'capacity', 'and', 'give', 'you', 'the', 'flexibility', 'to', 'choose', 'the', 'appropriate', 'mix', 'of', 'resources', 'for', 'building', 'training', 'and', 'deploying', 'your', 'M', 'L', 'models', '.', 'Each', 'instance', 'type', 'includes', 'one', 'or', 'more', 'instance', 'sizes', 'allowing', 'you', 'to', 'scale', 'your', 'resources', 'to', 'the', 'requirements', 'of', 'your', 'target', 'workload', '.', 'Some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'include', 'split', 'data', 'into', 'training', 'and', 'testing', 'sets', 'helps', 'you', 'validate', 'the', 'model', \"'s\", 'accuracy', '.', 'K', 'fold', 'cross', 'validation', 'can', 'help', 'with', 'smaller', 'data', 'sets', '.', 'Two', 'key', 'algorithms', 'for', 'supervised', 'learning', 'are', 'X', 'G', 'boost', 'and', 'linear', 'learner', '.', 'Use', 'K', 'means', 'for', 'unsupervised', 'learning', 'and', 'use', 'Amazon', 'Sagemaker', 'to', 'train', 'models', '.', 'That', \"'s\", 'it', 'for', 'section', 'five', '.', 'I', 'hope', 'to', 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back to module three . This is section five on training in this section . We 're going to look at how to select a model and train it with the data we have pre processed at this point . You 've done a lot to clean and prepare your data , but that doe n't mean your data is completely ready to train the algorithm . Some algorithm may not be able to work with training data in a data frame format . Some file format like CS V are commonly used by various algorithm but they do not make use of that optimization that some of the file format like record IO proto buff can use many Amazon Sagemaker algorithm support training with data in a CS V format . Amazon 's sagemaker requires that a CS V file doe n't have a header record and that the target variable is in the first column . Most Amazon sagemaker algorithm work best when you use the optimized proto buff record I O format for the training data using this format allows you to take advantage of pipe mode when training the algorithm that support it in pipe mode . Your training job stream data directly from Amazon S3 . When using the CS V format , the target variable in your training data set should be the first column on the left and your feature should be to the right of the target variable column . Evaluating a model with the same data that it trained on will lead to overfitting . Recall . Overfitting is where your model learns the particular of a data set too . Well , it 's essentially memorizing the training data rather than learning the relationship between feature and label . This mean the model is n't learning from those relationship and pattern to apply them to new data in the future . Hold out is when you split your data into multiple set , commonly set for training data validation data and testing data , training data , which includes both feature and label feed into the algorithm you 've selected to produce your model . You then use the model to make prediction over the validation data set , which is where you 'll likely notice thing you 'll want to tweak and tune and change . Then when you 're ready , you run the test data set , which only includes feature . Since you want the label to actually be predicted the performance you get here with the test data set is what you can reasonably expect to see in production . A common split when using the holdout method is using 80 % of the data for a training set , 10 % for validation and 10 % for test . Or if you have a lot of data , you can split it into 70 % training 15 % validation and 15 % test . So for a small data set , we can use K fold cross validation to utilize a much of the data a possible while still having relatively good metric . In order to choose which model is better K fold cross validation randomly partition the data into k different segment . For each segment , we 'll use the rest of the data outside of it for training in order to do a validation on that particular segment . Let 's look at an example here we have a five fold cross validation . The available training data is separated into five different chunk . For the training of the first model . We 're using all those chunk a the training data . And then we 're going to calculate the metric on this test piece for the second bottle . We 're going to use these piece a training . After the model is trained , you apply it to this test piece . We do the same thing five time . We use all the training data and we test it on five different model on different chunk of the test data , eventually testing it on all data point . One other thing to note about splitting your data data in a specific order can lead to bias on your model . This is especially true if you 're working with structured data . For example , the wine data is ordered by the quality column . When you run your model against your test data , this ordered pattern will be applied , biasing the model . It might also mean that some target are missing from the training data , typically randomizing your dataset prior to splitting is sufficient . And many library will provide function for this with smaller set . It is sometimes useful to use stratified sampling , stratified sampling ensures that the training and test set have approximately the same percentage of sample of each target class a the complete set . An internet search will give you many way to shuffle and split the data . One of the easiest is to use the train test split function from S K Learn . Amazon Sagemaker provides four different way . You can train model . The built in algorithm available can be easily deployed from the AWS console CL or a Jupiter notebook container are used behind the scene when you use one of the Amazon Sage maker built in algorithm , but you do not have to deal with them directly . Amazon Sagemaker supported framework provide prebuilt container to support deep learning framework such a a patchy M X net tensor flow pie Toch and Chainer . It also support machine learning library such a Sky Kit Learn and Spark M L by providing prebuilt Docker image . If you use the Amazon Sagemaker , Python S D K , they 're deployed using their respective Amazon Sagemaker S D K estimator class . If there is no prebuilt Amazon Sagemaker container image that you can use or modify for an advanced scenario , you can package your own script or algorithm to use with Amazon Sagemaker , you can use any programming language or framework to develop your container . For an example . If your team work and build M L model in R , you can build your own container to train and host an algorithm in R a well . Someone else may have already developed and tuned a model . It is worth looking in the AWS marketplace to find available model . Amazon 's Sagemaker provides high performance scalable machine learning algorithm optimized for speed scale and accuracy for supervised learning . Amazon 's sagemaker includes X G boost and linear learner algorithm for classification and quantitative or regression problem . There is also a factorization machine to address recommendation and time series prediction problem . Amazon 's sagemaker includes support for unsupervised learning such a with K mean clustering and principal component analysis PC A to solve problem like identifying customer grouping based on purchasing behavior . Finally , there are a selection of specialized algorithm for processing image and other deep learning task . Let 's look a little closer at three of the most commonly used built in algorithm and their use case X G boost or extreme gradient boosting is a popular and efficient open source implementation of the gradient boosted tree algorithm . Gradient boosting is a supervised learning algorithm that attempt to accurately predict a target variable by combining an ensemble of estimate from a set of simpler weaker model X G boost ha done remarkably well in machine learning competition because it robustly handle a variety of data type relationship and distribution . The large number of hyper parameter can be tweaked and tuned for improved fit . This flexibility make X G boost a solid choice for problem in regression classification , binary and multi class and ranking . The Amazon Sagemaker linear learner algorithm provides a solution for both classification and regression problem . With the Amazon Sagemaker algorithm . You can simultaneously explore different training objective and choose the best solution from your validation set . You can also explore a large number of model and choose the best one for your need compared with method that provide a solution for only continuous objective . The Amazon Sagemaker linear learner algorithm provides a significant increase in speed over naive hyper parameter optimization technique . K mean is an unsupervised learning algorithm . It attempt to find discrete grouping within data where member of a group are a similar a possible to one another and a different a possible from member of other group . You define the attribute that you want the algorithm to use to determine similarity to train a model . An Amazon Sagemaker , you create a training job . The training job includes the URL of the Amazon S3 bucket where you 've stored the training data , the URL of the S3 bucket where you want to store the output of the job . The Amazon elastic container registry path where the training code is stored . The compute resource that you want . Amazon Sagemaker to use for model training compute resource are M L compute instance managed by Amazon Sagemaker , Amazon . Sagemaker provides a selection of instance type optimized to fit different machine learning . Use case instance type comprise varying combination of C P U GP U memory and networking capacity and give you the flexibility to choose the appropriate mix of resource for building training and deploying your M L model . Each instance type includes one or more instance size allowing you to scale your resource to the requirement of your target workload . Some key takeaway from this section of the module include split data into training and testing set help you validate the model 's accuracy . K fold cross validation can help with smaller data set . Two key algorithm for supervised learning are X G boost and linear learner . Use K mean for unsupervised learning and use Amazon Sagemaker to train model . That 's it for section five . I hope to see you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', '.', 'This', 'is', 'section', 'six', 'and', 'we', \"'re\", 'going', 'to', 'look', 'at', 'hosting', 'and', 'using', 'the', 'model', 'in', 'this', 'section', '.', 'We', \"'ll\", 'look', 'at', 'how', 'you', 'can', 'deploy', 'your', 'trained', 'model', 'so', 'it', 'can', 'be', 'consumed', 'by', 'applications', '.', 'After', 'you', \"'ve\", 'trained', ',', 'tuned', 'and', 'tested', 'your', 'model', ',', 'you', \"'ll\", 'learn', 'more', 'about', 'testing', 'in', 'the', 'next', 'section', '.', 'You', \"'re\", 'now', 'ready', 'to', 'deploy', 'your', 'model', '.', 'If', 'you', \"'re\", 'thinking', 'that', 'we', \"'re\", 'looking', 'at', 'the', 'phases', 'out', 'of', 'order', ',', 'here', \"'s\", 'why', 'we', \"'re\", 'discussing', 'deployment', '.', 'Now', ',', 'if', 'you', 'want', 'to', 'test', 'your', 'model', 'and', 'get', 'performance', 'metrics', 'from', 'it', ',', 'you', 'first', 'need', 'to', 'make', 'an', 'inference', 'or', 'prediction', 'from', 'the', 'model', 'and', 'this', 'typically', 'requires', 'deployment', '.', 'Deployment', 'for', 'testing', 'is', 'different', 'from', 'production', '.', 'Although', 'the', 'mechanics', 'are', 'the', 'same', '.', 'Amazon', \"'s\", 'Sagemaker', 'provides', 'everything', 'you', 'need', 'to', 'host', 'your', 'model', 'for', 'simple', 'testing', 'and', 'evaluation', 'from', 'a', 'few', 'requests', 'to', 'deployments', ',', 'handling', 'tens', 'of', 'thousands', 'of', 'requests', '.', 'There', 'are', 'two', 'ways', 'you', 'can', 'deploy', 'your', 'model', 'for', 'single', 'predictions', '.', 'You', 'can', 'deploy', 'your', 'model', 'with', 'Amazon', 'Sagemaker', 'hosting', 'services', '.', 'Sagemaker', 'will', 'deploy', 'multiple', 'compute', 'instances', 'which', 'run', 'your', 'model', 'behind', 'a', 'load', 'balanced', 'endpoint', 'applications', 'can', 'call', 'the', 'API', 'at', 'the', 'end', 'point', 'to', 'make', 'predictions', 'with', 'this', 'model', ',', 'you', 'can', 'scale', 'the', 'number', 'of', 'instances', 'up', 'or', 'down', 'based', 'on', 'demand', 'to', 'get', 'predictions', 'for', 'an', 'entire', 'data', 'set', '.', 'Use', 'Amazon', \"'s\", 'Sagemaker', 'batch', 'transform', 'instead', 'of', 'deploying', 'and', 'maintaining', 'a', 'permanent', 'endpoint', 'sagemaker', 'will', 'spin', 'up', 'your', 'model', 'and', 'perform', 'the', 'predictions', 'for', 'the', 'entire', 'data', 'set', '.', 'You', 'provide', ',', 'it', 'will', 'then', 'store', 'the', 'results', 'in', 'Amazon', 'S3', 'before', 'it', 'shuts', 'down', 'and', 'terminates', 'the', 'compute', 'instances', '.', 'It', \"'s\", 'useful', 'for', 'performing', 'batch', 'predictions', '.', 'When', 'you', 'test', 'the', 'model', ',', 'you', 'can', 'quickly', 'run', 'your', 'entire', 'validation', 'set', 'against', 'the', 'model', 'without', 'writing', 'any', 'code', 'to', 'process', 'and', 'collate', 'the', 'individual', 'results', '.', 'The', 'goal', 'of', 'the', 'deployment', 'phase', 'is', 'to', 'provide', 'a', 'managed', 'environment', 'to', 'host', 'models', 'for', 'providing', 'inference', 'securely', 'and', 'with', 'low', 'latency', '.', 'After', 'your', 'model', 'is', 'deployed', 'into', 'production', ',', 'you', 'should', 'monitor', 'your', 'production', 'data', 'and', 'retrain', 'your', 'model', '.', 'If', 'necessary', 'newly', 'deployed', 'models', 'need', 'to', 'reflect', 'the', 'current', 'production', 'data', ',', 'new', 'data', 'has', 'accumulated', 'over', 'time', 'and', 'it', 'could', 'potentially', 'identify', 'alternative', 'or', 'new', 'outcomes', '.', 'And', 'so', 'deploying', 'a', 'model', 'is', 'not', 'a', 'one', 'time', 'exercise', '.', 'Instead', 'it', \"'s\", 'a', 'continuous', 'process', '.', 'With', 'one', 'click', ',', 'you', 'can', 'deploy', 'your', 'model', 'on', 'Amazon', 'ML', 'instances', 'that', 'can', 'automatically', 'scale', 'across', 'multiple', 'availability', 'zones', '.', 'For', 'higher', 'redundancy', ',', 'just', 'specify', 'the', 'type', 'of', 'instance', 'and', 'the', 'maximum', 'and', 'minimum', 'number', 'of', 'instances', 'desired', 'sagemaker', 'will', 'take', 'care', 'of', 'the', 'rest', '.', 'It', 'will', 'launch', 'the', 'instances', ',', 'deploy', 'your', 'model', 'and', 'set', 'up', 'the', 'secure', 'http', 'S', 'endpoint', 'for', 'your', 'application', '.', 'Your', 'application', 'only', 'needs', 'to', 'include', 'an', 'API', 'call', 'to', 'this', 'end', 'point', 'to', 'achieve', 'inference', 'with', 'low', 'latency', 'and', 'high', 'throughput', '.', 'With', 'this', 'architecture', ',', 'you', 'can', 'integrate', 'your', 'new', 'models', 'into', 'your', 'application', 'in', 'minutes', 'because', 'changes', 'to', 'the', 'model', 'no', 'longer', 'need', 'changes', 'to', 'the', 'application', 'code', '.', 'Sagemaker', 'manages', 'your', 'production', 'compute', 'infrastructure', 'on', 'your', 'behalf', '.', 'It', 'can', 'perform', 'health', 'checks', ',', 'apply', 'security', 'patches', 'and', 'conduct', 'other', 'routine', 'maintenance', 'all', 'with', 'built', 'in', 'Amazon', 'cloudwatch', 'monitoring', 'and', 'logging', '.', 'After', 'you', \"'ve\", 'trained', 'the', 'model', ',', 'you', 'can', 'create', 'the', 'endpoint', 'either', 'in', 'code', 'or', 'by', 'using', 'the', 'Sagemaker', 'console', '.', 'If', 'you', \"'re\", 'planning', 'to', 'host', 'only', 'a', 'single', 'model', ',', 'you', 'can', 'create', 'an', 'endpoint', 'for', 'that', 'model', '.', 'But', 'if', 'you', \"'re\", 'planning', 'to', 'host', 'multiple', 'models', ',', 'you', 'need', 'to', 'create', 'a', 'multi', 'model', 'endpoint', ',', 'multi', 'model', '.', 'End', 'points', 'provide', 'a', 'scalable', 'and', 'cost', 'effective', 'solution', 'for', 'deploying', 'large', 'numbers', 'of', 'models', '.', 'They', 'use', 'a', 'shared', 'serving', 'container', 'that', \"'s\", 'enabled', 'to', 'host', 'multiple', 'models', '.', 'This', 'reduces', 'hosting', 'costs', 'by', 'improving', 'endpoint', 'utilization', 'compared', 'to', 'using', 'single', 'model', 'endpoints', '.', 'It', 'also', 'reduces', 'deployment', 'overhead', 'because', 'Sagemaker', 'manages', 'loading', 'models', 'in', 'memory', 'and', 'scaling', 'the', 'models', 'based', 'on', 'the', 'traffic', 'patterns', 'to', 'them', '.', 'When', 'you', 'deploy', 'machine', 'learning', 'models', 'into', 'production', 'to', 'make', 'predictions', 'on', 'new', 'data', ',', 'you', 'need', 'to', 'make', 'sure', 'you', 'apply', 'the', 'same', 'data', 'processing', 'steps', 'that', 'were', 'used', 'in', 'training', 'to', 'each', 'inference', 'request', '.', 'Otherwise', 'you', 'can', 'get', 'incorrect', 'prediction', 'results', '.', 'By', 'using', 'inference', 'pipelines', ',', 'you', 'can', 'reuse', 'the', 'data', 'processing', 'steps', 'from', 'model', 'training', 'during', 'inference', 'without', 'maintaining', 'two', 'separate', 'copies', 'of', 'the', 'same', 'code', '.', 'This', 'helps', 'ensure', 'the', 'accuracy', 'of', 'your', 'predictions', 'and', 'reduces', 'development', 'overhead', '.', 'Because', 'Sagemaker', 'is', 'a', 'managed', 'service', 'inference', 'pipelines', 'are', 'completely', 'managed', '.', 'When', 'you', 'deploy', 'the', 'pipeline', 'model', ',', 'the', 'service', 'installs', 'and', 'runs', 'the', 'sequence', 'of', 'containers', 'on', 'each', 'EC2', 'instance', 'in', 'the', 'end', 'point', 'or', 'each', 'batch', 'transform', 'job', '.', 'Additionally', ',', 'the', 'sequence', 'of', 'feature', 'processing', 'and', 'inference', 'runs', 'with', 'low', 'latency', 'because', 'the', 'containers', 'are', 'collated', 'on', 'the', 'same', 'E', 'two', 'instances', '.', 'Some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'include', 'these', 'points', '.', 'You', 'can', 'deploy', 'your', 'train', 'model', 'by', 'using', 'sagemaker', 'to', 'handle', 'API', 'calls', 'from', 'applications', 'or', 'to', 'perform', 'predictions', 'using', 'a', 'batch', 'transformation', '.', 'The', 'goal', 'of', 'your', 'model', 'is', 'to', 'generate', 'predictions', 'to', 'answer', 'the', 'business', 'problem', '.', 'Be', 'sure', 'that', 'your', 'model', 'can', 'generate', 'good', 'results', 'before', 'you', 'deploy', 'to', 'production', '.', 'Finally', 'use', 'multi', 'model', 'endpoint', 'support', 'to', 'save', 'resources', 'when', 'you', 'have', 'multiple', 'models', 'to', 'deploy', '.', 'That', \"'s\", 'it', '.', 'For', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome back . This is section six and we 're going to look at hosting and using the model in this section . We 'll look at how you can deploy your trained model so it can be consumed by application . After you 've trained , tuned and tested your model , you 'll learn more about testing in the next section . You 're now ready to deploy your model . If you 're thinking that we 're looking at the phase out of order , here 's why we 're discussing deployment . Now , if you want to test your model and get performance metric from it , you first need to make an inference or prediction from the model and this typically requires deployment . Deployment for testing is different from production . Although the mechanic are the same . Amazon 's Sagemaker provides everything you need to host your model for simple testing and evaluation from a few request to deployment , handling ten of thousand of request . There are two way you can deploy your model for single prediction . You can deploy your model with Amazon Sagemaker hosting service . Sagemaker will deploy multiple compute instance which run your model behind a load balanced endpoint application can call the API at the end point to make prediction with this model , you can scale the number of instance up or down based on demand to get prediction for an entire data set . Use Amazon 's Sagemaker batch transform instead of deploying and maintaining a permanent endpoint sagemaker will spin up your model and perform the prediction for the entire data set . You provide , it will then store the result in Amazon S3 before it shuts down and terminates the compute instance . It 's useful for performing batch prediction . When you test the model , you can quickly run your entire validation set against the model without writing any code to process and collate the individual result . The goal of the deployment phase is to provide a managed environment to host model for providing inference securely and with low latency . After your model is deployed into production , you should monitor your production data and retrain your model . If necessary newly deployed model need to reflect the current production data , new data ha accumulated over time and it could potentially identify alternative or new outcome . And so deploying a model is not a one time exercise . Instead it 's a continuous process . With one click , you can deploy your model on Amazon ML instance that can automatically scale across multiple availability zone . For higher redundancy , just specify the type of instance and the maximum and minimum number of instance desired sagemaker will take care of the rest . It will launch the instance , deploy your model and set up the secure http S endpoint for your application . Your application only need to include an API call to this end point to achieve inference with low latency and high throughput . With this architecture , you can integrate your new model into your application in minute because change to the model no longer need change to the application code . Sagemaker manages your production compute infrastructure on your behalf . It can perform health check , apply security patch and conduct other routine maintenance all with built in Amazon cloudwatch monitoring and logging . After you 've trained the model , you can create the endpoint either in code or by using the Sagemaker console . If you 're planning to host only a single model , you can create an endpoint for that model . But if you 're planning to host multiple model , you need to create a multi model endpoint , multi model . End point provide a scalable and cost effective solution for deploying large number of model . They use a shared serving container that 's enabled to host multiple model . This reduces hosting cost by improving endpoint utilization compared to using single model endpoint . It also reduces deployment overhead because Sagemaker manages loading model in memory and scaling the model based on the traffic pattern to them . When you deploy machine learning model into production to make prediction on new data , you need to make sure you apply the same data processing step that were used in training to each inference request . Otherwise you can get incorrect prediction result . By using inference pipeline , you can reuse the data processing step from model training during inference without maintaining two separate copy of the same code . This help ensure the accuracy of your prediction and reduces development overhead . Because Sagemaker is a managed service inference pipeline are completely managed . When you deploy the pipeline model , the service installs and run the sequence of container on each EC2 instance in the end point or each batch transform job . Additionally , the sequence of feature processing and inference run with low latency because the container are collated on the same E two instance . Some key takeaway from this section of the module include these point . You can deploy your train model by using sagemaker to handle API call from application or to perform prediction using a batch transformation . The goal of your model is to generate prediction to answer the business problem . Be sure that your model can generate good result before you deploy to production . Finally use multi model endpoint support to save resource when you have multiple model to deploy . That 's it . For this section , we 'll see you in the next video .\n",
      "['Hi', ',', 'welcome', 'back', 'to', 'module', 'three', 'in', 'this', 'section', '.', 'We', \"'ll\", 'look', 'at', 'how', 'you', 'can', 'evaluate', 'your', 'model', 'success', 'in', 'predicting', 'results', '.', 'At', 'this', 'point', '.', 'You', \"'ve\", 'trained', 'your', 'models', '.', 'It', \"'s\", 'now', 'time', 'to', 'evaluate', 'that', 'model', 'to', 'determine', 'if', 'it', 'will', 'do', 'a', 'good', 'job', 'predicting', 'the', 'target', 'on', 'new', 'and', 'future', 'data', '.', 'Because', 'future', 'instances', 'have', 'unknown', 'target', 'values', '.', 'You', 'need', 'to', 'assess', 'how', 'the', 'model', 'will', 'perform', 'on', 'data', 'where', 'you', 'already', 'know', 'the', 'target', 'answer', '.', 'You', \"'ll\", 'then', 'use', 'this', 'assessment', 'as', 'a', 'proxy', 'for', 'performance', 'on', 'future', 'data', '.', 'This', 'is', 'the', 'reason', 'why', 'you', 'hold', 'out', 'a', 'sample', 'of', 'your', 'data', 'for', 'evaluating', 'or', 'testing', '.', 'An', 'important', 'part', 'of', 'this', 'phase', 'involves', 'choosing', 'the', 'most', 'appropriate', 'metric', 'for', 'your', 'business', 'situation', '.', 'Think', 'back', 'to', 'the', 'earlier', 'section', 'on', 'problem', 'formulation', '.', 'During', 'that', 'phase', ',', 'you', 'define', 'your', 'business', 'problem', 'and', 'outcome', 'and', 'then', 'you', 'craft', 'a', 'business', 'metric', 'to', 'evaluate', 'success', '.', 'The', 'model', 'metric', 'you', 'choose', 'at', 'this', 'phase', 'should', 'be', 'linked', 'to', 'that', 'business', 'metric', 'as', 'much', 'as', 'possible', '.', 'There', \"'s\", 'often', 'a', 'high', 'correlation', 'between', 'the', 'two', 'metrics', '.', 'In', 'addition', 'to', 'considering', 'your', 'business', 'problem', 'and', 'success', 'metric', ',', 'the', 'type', 'of', 'M', 'L', 'problem', 'you', \"'re\", 'working', 'with', 'will', 'influence', 'the', 'model', 'metric', '.', 'You', 'choose', 'throughout', 'the', 'rest', 'of', 'this', 'module', '.', 'We', \"'ll\", 'look', 'at', 'examples', 'of', 'common', 'metrics', 'used', 'in', 'classification', 'problems', '.', 'We', \"'ll\", 'also', 'look', 'at', 'common', 'metrics', 'used', 'in', 'regression', 'problems', '.', 'We', \"'re\", 'going', 'to', 'start', 'by', 'considering', 'a', 'simple', 'binary', 'classification', 'problem', '.', 'Here', \"'s\", 'a', 'specific', 'example', ',', 'imagine', 'that', 'you', 'have', 'a', 'simple', 'image', 'recognition', 'model', 'that', \"'s\", 'labeling', 'data', 'as', 'either', 'cat', 'or', 'not', 'cat', '.', 'After', 'the', 'model', 'has', 'been', 'trained', ',', 'you', 'can', 'use', 'the', 'test', 'data', 'set', ',', 'you', 'held', 'back', 'to', 'perform', 'predictions', 'to', 'help', 'examine', 'the', 'performance', 'of', 'the', 'model', '.', 'You', 'can', 'compare', 'the', 'predicted', 'values', 'with', 'the', 'actual', 'values', '.', 'If', 'you', 'plot', 'the', 'values', 'into', 'a', 'table', ',', 'like', 'the', 'example', ',', 'you', 'can', 'start', 'getting', 'some', 'insights', 'into', 'how', 'well', 'the', 'model', 'performed', 'in', 'a', 'confusion', 'matrix', '.', 'You', 'can', 'get', 'a', 'high', 'level', 'comparison', 'of', 'how', 'the', 'predicted', 'class', 'is', 'matched', 'up', 'against', 'the', 'actual', 'classes', '.', 'If', 'the', 'actual', 'label', 'or', 'class', 'is', 'cat', ',', 'which', 'is', 'identified', 'as', 'P', 'for', 'positive', 'and', 'the', 'predicted', 'label', 'or', 'class', 'is', 'also', 'cat', ',', 'then', 'you', 'have', 'a', 'true', 'positive', '.', 'This', 'is', 'a', 'good', 'outcome', 'for', 'your', 'model', '.', 'Similarly', ',', 'if', 'you', 'have', 'an', 'actual', 'label', 'of', 'not', 'cat', ',', 'which', 'is', 'identified', 'as', 'N', 'for', 'negative', 'and', 'the', 'predicted', 'label', 'or', 'class', 'is', 'also', 'not', 'cat', ',', 'then', 'you', 'have', 'a', 'true', 'negative', '.', 'This', 'is', 'also', 'a', 'good', 'outcome', 'for', 'your', 'model', '.', 'In', 'both', 'these', 'cases', ',', 'your', 'model', 'predicted', 'the', 'correct', 'outcome', '.', 'When', 'it', 'used', 'the', 'testing', 'data', '.', 'There', 'are', 'two', 'other', 'possible', 'outcomes', 'and', 'both', 'are', \"n't\", 'considered', 'good', 'outcomes', '.', 'The', 'first', 'one', 'is', 'when', 'the', 'actual', 'class', 'is', 'negative', '.', 'So', 'you', 'got', 'not', 'cat', 'but', 'the', 'predicted', 'class', 'is', 'positive', 'or', 'cat', '.', 'This', 'is', 'called', 'a', 'false', 'positive', 'because', 'the', 'prediction', 'is', 'positive', 'but', 'incorrect', '.', 'Finally', ',', 'there', 'are', 'false', 'negatives', '.', 'These', 'happen', 'when', 'the', 'actual', 'class', 'is', 'positive', '.', 'So', 'you', 'got', 'cat', 'but', 'the', 'predicted', 'class', 'is', 'negative', 'or', 'not', 'cat', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'one', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'two', 'where', 'we', \"'ll\", 'review', 'calculating', 'classification', 'metrics', '.']\n",
      "Hi , welcome back to module three in this section . We 'll look at how you can evaluate your model success in predicting result . At this point . You 've trained your model . It 's now time to evaluate that model to determine if it will do a good job predicting the target on new and future data . Because future instance have unknown target value . You need to ass how the model will perform on data where you already know the target answer . You 'll then use this assessment a a proxy for performance on future data . This is the reason why you hold out a sample of your data for evaluating or testing . An important part of this phase involves choosing the most appropriate metric for your business situation . Think back to the earlier section on problem formulation . During that phase , you define your business problem and outcome and then you craft a business metric to evaluate success . The model metric you choose at this phase should be linked to that business metric a much a possible . There 's often a high correlation between the two metric . In addition to considering your business problem and success metric , the type of M L problem you 're working with will influence the model metric . You choose throughout the rest of this module . We 'll look at example of common metric used in classification problem . We 'll also look at common metric used in regression problem . We 're going to start by considering a simple binary classification problem . Here 's a specific example , imagine that you have a simple image recognition model that 's labeling data a either cat or not cat . After the model ha been trained , you can use the test data set , you held back to perform prediction to help examine the performance of the model . You can compare the predicted value with the actual value . If you plot the value into a table , like the example , you can start getting some insight into how well the model performed in a confusion matrix . You can get a high level comparison of how the predicted class is matched up against the actual class . If the actual label or class is cat , which is identified a P for positive and the predicted label or class is also cat , then you have a true positive . This is a good outcome for your model . Similarly , if you have an actual label of not cat , which is identified a N for negative and the predicted label or class is also not cat , then you have a true negative . This is also a good outcome for your model . In both these case , your model predicted the correct outcome . When it used the testing data . There are two other possible outcome and both are n't considered good outcome . The first one is when the actual class is negative . So you got not cat but the predicted class is positive or cat . This is called a false positive because the prediction is positive but incorrect . Finally , there are false negative . These happen when the actual class is positive . So you got cat but the predicted class is negative or not cat . That 's it . For part one of this section , we 'll see you again for part two where we 'll review calculating classification metric .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'how', 'to', 'evaluate', 'your', 'model', '.', 'The', 'diagram', 'shows', 'the', 'confusion', 'matrix', 'of', 'how', 'two', 'different', 'models', 'performed', 'on', 'the', 'same', 'data', '.', 'Can', 'you', 'tell', 'which', 'one', \"'s\", 'better', '?', 'Which', 'is', 'better', 'is', \"n't\", 'a', 'good', 'question', 'to', 'ask', '?', 'What', 'do', 'you', 'mean', 'by', 'better', '?', 'Does', 'better', 'mean', 'making', 'sure', 'you', 'find', 'all', 'the', 'cats', 'even', 'if', 'it', 'means', 'you', \"'ll\", 'get', 'many', 'false', 'positives', 'or', 'does', 'better', 'mean', 'making', 'sure', 'the', 'model', 'is', 'the', 'most', 'accurate', '.', 'It', \"'s\", 'difficult', 'to', 'see', 'just', 'by', 'looking', 'at', 'the', 'two', 'charts', '.', 'What', 'if', 'you', \"'re\", 'trying', 'several', 'models', 'using', 'multiple', 'folds', 'and', 'have', 'hundreds', 'of', 'data', 'points', 'to', 'compare', 'to', 'do', 'that', '?', 'You', \"'ll\", 'need', 'to', 'calculate', 'more', 'metrics', '.', 'The', 'first', 'metric', 'is', 'sensitivity', '.', 'This', 'is', 'sometimes', 'referred', 'to', 'as', 'recall', 'hit', 'rate', 'or', 'true', 'positive', 'rate', '.', 'Sensitivity', 'is', 'the', 'percentage', 'of', 'positive', 'identifications', '.', 'In', 'the', 'cat', 'example', ',', 'it', 'represents', 'what', 'percentage', 'of', 'cats', 'were', 'correctly', 'identified', 'to', 'calculate', 'sensitivity', ',', 'take', 'the', 'number', 'of', 'true', 'positives', 'or', 'the', 'number', 'of', 'positive', 'identifications', 'of', 'cats', 'and', 'divide', 'that', 'by', 'the', 'total', 'number', 'of', 'actual', 'cats', '.', 'In', 'this', 'example', ',', '60', '%', 'of', 'cats', 'that', 'were', 'cats', 'were', 'correctly', 'identified', 'as', 'cats', 'specificity', 'is', 'sometimes', 'referred', 'to', 'as', 'selectivity', 'or', 'true', 'negative', 'rate', 'specificity', 'is', 'the', 'percentage', 'of', 'negatives', 'correctly', 'identified', '.', 'In', 'the', 'cat', 'example', ',', 'this', 'is', 'the', 'number', 'of', 'images', 'that', 'were', 'not', 'cats', 'that', 'were', 'correctly', 'identified', 'as', 'not', 'cats', '.', 'To', 'calculate', 'specificity', ',', 'take', 'the', 'number', 'of', 'true', 'negatives', 'and', 'divide', 'that', 'by', 'the', 'total', 'number', 'of', 'actual', 'negatives', '.', 'So', 'for', 'the', 'example', ',', 'that', \"'s\", 'the', 'number', 'of', 'knot', 'cats', 'that', 'were', 'correctly', 'identified', 'divided', 'by', 'the', 'total', 'number', 'of', 'actual', 'knot', 'cats', '.', 'This', 'means', 'that', 'in', 'the', 'example', ',', '64', '%', 'of', 'not', 'cats', 'were', 'identified', 'as', 'not', 'cats', '.', 'Now', 'that', 'you', 'have', 'these', 'metrics', 'for', 'each', 'model', ',', 'knowing', 'what', 'your', 'business', 'goal', 'is', ',', 'makes', 'it', 'easier', 'to', 'decide', 'which', 'model', 'to', 'use', '.', 'Which', 'model', 'would', 'you', 'choose', 'if', 'you', 'wanted', 'to', 'make', 'sure', 'you', \"'ll\", 'identify', 'as', 'many', 'cats', 'as', 'possible', '?', 'Model', 'B', 'would', 'be', 'a', 'good', 'answer', 'if', 'you', \"'re\", 'not', 'concerned', 'about', 'having', 'many', 'false', 'positives', ',', 'that', 'is', 'if', 'you', \"'re\", 'not', 'concerned', 'about', 'having', 'incorrectly', 'identified', 'not', 'cats', ',', 'which', 'model', 'would', 'you', 'choose', 'if', 'you', 'wanted', 'to', 'make', 'sure', 'you', 'identified', 'animals', 'that', 'were', 'not', 'cats', 'model', 'A', 'might', 'work', 'for', 'this', 'scenario', '.', 'Again', ',', 'it', 'would', 'depend', 'on', 'how', 'many', 'false', 'negatives', 'you', 'can', 'tolerate', '.', 'If', 'this', 'was', 'a', 'classification', 'of', 'patients', 'who', 'had', 'heart', 'disease', 'or', 'not', ',', 'which', 'model', 'would', 'be', 'best', '.', 'This', 'is', 'where', 'it', 'gets', 'interesting', '.', 'A', 'fun', 'website', 'might', 'get', 'a', 'bad', 'reputation', 'if', 'it', 'ca', \"n't\", 'identify', 'cats', 'correctly', '.', 'But', 'if', 'you', \"'re\", 'trying', 'to', 'diagnose', 'patients', ',', 'your', 'focus', 'will', 'probably', 'be', 'very', 'different', '.', 'It', \"'s\", 'important', 'to', 'understand', 'the', 'tradeoffs', 'you', \"'re\", 'making', '.', 'When', 'you', 'decide', 'which', 'model', 'to', 'use', ',', 'there', 'are', 'also', 'other', 'metrics', 'that', 'can', 'help', 'you', 'make', 'your', 'decisions', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'two', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'three', 'where', 'we', \"'ll\", 'start', 'looking', 'at', 'thresholds', '.']\n",
      "Hi , welcome back . We 'll continue exploring how to evaluate your model . The diagram show the confusion matrix of how two different model performed on the same data . Can you tell which one 's better ? Which is better is n't a good question to ask ? What do you mean by better ? Does better mean making sure you find all the cat even if it mean you 'll get many false positive or doe better mean making sure the model is the most accurate . It 's difficult to see just by looking at the two chart . What if you 're trying several model using multiple fold and have hundred of data point to compare to do that ? You 'll need to calculate more metric . The first metric is sensitivity . This is sometimes referred to a recall hit rate or true positive rate . Sensitivity is the percentage of positive identification . In the cat example , it represents what percentage of cat were correctly identified to calculate sensitivity , take the number of true positive or the number of positive identification of cat and divide that by the total number of actual cat . In this example , 60 % of cat that were cat were correctly identified a cat specificity is sometimes referred to a selectivity or true negative rate specificity is the percentage of negative correctly identified . In the cat example , this is the number of image that were not cat that were correctly identified a not cat . To calculate specificity , take the number of true negative and divide that by the total number of actual negative . So for the example , that 's the number of knot cat that were correctly identified divided by the total number of actual knot cat . This mean that in the example , 64 % of not cat were identified a not cat . Now that you have these metric for each model , knowing what your business goal is , make it easier to decide which model to use . Which model would you choose if you wanted to make sure you 'll identify a many cat a possible ? Model B would be a good answer if you 're not concerned about having many false positive , that is if you 're not concerned about having incorrectly identified not cat , which model would you choose if you wanted to make sure you identified animal that were not cat model A might work for this scenario . Again , it would depend on how many false negative you can tolerate . If this wa a classification of patient who had heart disease or not , which model would be best . This is where it get interesting . A fun website might get a bad reputation if it ca n't identify cat correctly . But if you 're trying to diagnose patient , your focus will probably be very different . It 's important to understand the tradeoff you 're making . When you decide which model to use , there are also other metric that can help you make your decision . That 's it . For part two of this section , we 'll see you again for part three where we 'll start looking at threshold .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'how', 'to', 'evaluate', 'your', 'model', '.', 'Classification', 'models', 'are', 'going', 'to', 'return', 'a', 'probability', 'for', 'the', 'target', '.', 'This', 'is', 'a', 'value', 'of', 'the', 'input', 'belonging', 'to', 'the', 'target', 'class', 'and', 'it', 'will', 'be', 'between', 'zero', 'and', 'one', '.', 'To', 'convert', 'the', 'value', 'to', 'a', 'class', '.', 'You', 'need', 'to', 'determine', 'the', 'threshold', 'to', 'use', '.', 'You', 'might', 'think', 'it', \"'s\", '50', '%', 'but', 'you', 'could', 'change', 'it', 'to', 'be', 'lower', 'or', 'higher', 'to', 'improve', 'your', 'results', '.', 'As', 'you', \"'ve\", 'seen', 'with', 'sensitivity', 'and', 'specificity', ',', 'there', \"'s\", 'a', 'trade', 'off', 'between', 'correctly', 'and', 'incorrectly', 'identifying', 'classes', ',', 'changing', 'the', 'threshold', 'can', 'impact', 'that', 'outcome', '.', 'We', \"'re\", 'going', 'to', 'take', 'a', 'look', 'at', 'how', 'you', 'can', 'visualize', 'this', 'a', 'receiver', 'operating', 'characteristic', 'graph', 'is', 'also', 'known', 'as', 'an', 'R', 'C', 'graph', '.', 'It', 'summarizes', 'all', 'the', 'confusion', 'matrices', 'that', 'each', 'threshold', 'produced', 'to', 'build', 'one', 'you', 'calculate', 'and', 'plot', 'the', 'sensitivity', 'or', 'true', 'positive', 'rate', 'against', 'the', 'false', 'positive', 'rate', '.', 'On', 'a', 'graph', '.', 'For', 'each', 'threshold', 'value', ',', 'you', 'can', 'calculate', 'the', 'false', 'positive', 'rate', 'by', 'subtracting', 'the', 'specificity', 'from', 'one', '.', 'After', 'you', 'plot', 'those', 'points', ',', 'you', 'can', 'draw', 'a', 'line', 'between', 'them', '.', 'The', 'dotted', 'black', 'line', 'from', '00', 'to', '11', 'means', 'that', 'the', 'sensitivity', 'or', 'true', 'positive', 'rate', 'is', 'equal', 'to', 'the', 'false', 'positive', 'rate', '.', 'The', 'point', 'at', '11', 'means', 'that', 'you', \"'ve\", 'correctly', 'identified', 'all', 'the', 'cats', ',', 'but', 'you', \"'ve\", 'also', 'incorrectly', 'identified', 'all', 'the', 'not', 'cats', '.', 'This', 'is', 'bad', '.', 'Any', 'point', 'on', 'this', 'line', 'means', 'that', 'the', 'proportion', 'of', 'correctly', 'classified', 'samples', 'is', 'the', 'same', 'as', 'the', 'proportion', 'of', 'incorrectly', 'classified', 'samples', '.', 'The', 'point', 'at', '00', 'represents', 'that', 'there', 'are', 'zero', 'true', 'positives', 'and', 'zero', 'false', 'positives', '.', 'A', 'model', 'that', 'has', 'high', 'sensitivity', 'and', 'low', 'false', 'positive', 'rate', 'is', 'usually', 'the', 'goal', '.', 'So', 'it', \"'s\", 'considered', 'to', 'be', 'better', 'when', 'the', 'line', 'between', 'the', 'threshold', 'recordings', 'is', 'closer', 'towards', 'the', 'top', 'left', 'corner', '.', 'If', 'you', 'had', 'the', 'data', 'from', 'two', 'models', ',', 'you', 'could', 'plot', 'out', 'the', 'R', 'O', 'C', 'curve', 'for', 'each', 'model', 'and', 'compare', 'them', '.', 'However', ',', 'that', 'can', 'be', 'tedious', '.', 'There', \"'s\", 'another', 'graph', 'you', 'can', 'use', 'for', 'this', 'which', 'we', \"'ll\", 'look', 'at', 'next', '.', 'Another', 'evaluation', 'metric', 'you', 'can', 'use', 'is', 'the', 'area', 'under', 'the', 'curve', 'receiver', 'operator', 'curve', 'which', 'is', 'also', 'known', 'as', 'an', 'A', 'R', 'C.', 'The', 'A', 'U', 'C', 'part', 'is', 'the', 'area', 'under', 'the', 'plotted', 'line', '.', 'When', 'the', 'A', 'U', 'C', 'is', 'higher', ',', 'it', 'means', 'the', 'model', 'will', 'be', 'better', 'at', 'predicting', 'cats', 'as', 'cats', 'and', 'not', 'cats', 'as', 'not', 'cats', '.', 'You', 'can', 'use', 'the', 'A', 'U', 'C', 'to', 'quickly', 'compare', 'models', 'with', 'each', 'other', 'with', 'the', 'four', 'numbers', 'from', 'our', 'confusion', 'matrix', '.', 'You', 'can', 'calculate', 'the', 'model', \"'s\", 'accuracy', '.', 'This', 'is', 'also', 'known', 'as', 'its', 'score', '.', 'You', 'can', 'do', 'this', 'by', 'adding', 'up', 'the', 'correct', 'predictions', 'and', 'then', 'dividing', 'that', 'number', 'by', 'the', 'total', 'number', 'of', 'predictions', '.', 'Though', 'accuracy', 'is', 'widely', 'used', 'metric', 'for', 'classification', 'problems', ',', 'it', 'has', 'limitations', '.', 'This', 'metric', 'is', \"n't\", 'as', 'effective', 'when', 'there', 'are', 'a', 'lot', 'of', 'true', 'negative', 'cases', 'in', 'your', 'data', 'set', '.', 'Think', 'about', 'the', 'cat', ',', 'not', 'cat', 'example', ',', 'if', 'most', 'of', 'your', 'accuracy', 'is', 'based', 'on', 'true', 'negatives', ',', 'it', 'says', 'that', 'your', 'model', 'is', 'good', 'at', 'predicting', 'what', 'is', \"n't\", 'a', 'cat', '.', 'In', 'this', 'case', ',', 'you', 'might', 'not', 'feel', 'confident', 'in', 'your', 'models', 'ability', 'to', 'predict', 'cats', 'after', 'you', 'roll', 'it', 'out', 'into', 'production', '.', 'This', 'leads', 'to', 'an', 'example', 'of', 'why', 'it', \"'s\", 'important', 'to', 'make', 'sure', 'that', 'the', 'metric', 'you', 'choose', 'for', 'model', 'evaluation', 'aligns', 'to', 'your', 'business', 'goal', '.', 'Think', 'about', 'the', 'credit', 'card', 'fraud', 'example', ',', 'in', 'this', 'case', ',', 'using', 'accuracy', 'as', 'your', 'main', 'metric', 'probably', 'is', \"n't\", 'a', 'good', 'idea', 'because', 'you', 'have', 'a', 'lot', 'of', 'true', 'negatives', '.', 'Your', 'high', 'true', 'negative', 'number', 'might', 'hide', 'the', 'fact', 'that', 'your', 'model', \"'s\", 'ability', 'to', 'identify', 'cases', 'of', 'fraud', 'that', 'is', 'to', 'identify', 'true', 'positives', 'is', \"n't\", 'ideal', 'as', 'a', 'credit', 'card', 'company', '.', 'It', \"'s\", 'probably', 'unacceptable', 'to', 'have', 'less', 'than', 'almost', 'perfect', 'performance', ',', 'identifying', 'fraud', 'cases', 'that', 'would', 'drive', 'customers', 'away', ',', 'which', 'would', 'be', 'the', 'opposite', 'of', 'what', 'you', \"'d\", 'want', 'to', 'achieve', 'from', 'a', 'business', 'standpoint', '.', 'This', 'is', 'why', 'two', 'other', 'metrics', 'are', 'often', 'used', 'in', 'these', 'situations', '.', 'The', 'first', 'one', 'is', 'precision', ',', 'which', 'essentially', 'removes', 'the', 'negative', 'predictions', '.', 'Precision', 'is', 'the', 'proportion', 'of', 'positive', 'predictions', 'that', 'are', 'actually', 'correct', '.', 'You', 'can', 'calculate', 'it', 'by', 'taking', 'the', 'true', 'positive', 'and', 'dividing', 'it', 'by', 'true', 'positive', 'plus', 'false', 'positive', '.', 'When', 'the', 'cost', 'of', 'false', 'positives', 'is', 'high', 'in', 'your', 'particular', 'business', 'situation', ',', 'precision', 'might', 'be', 'a', 'good', 'metric', '.', 'Think', 'about', 'a', 'classification', 'model', 'that', 'identifies', 'email', 'messages', 'as', 'spam', 'or', 'not', '.', 'In', 'this', 'case', ',', 'you', 'do', \"n't\", 'want', 'your', 'model', 'to', 'label', 'an', 'email', 'message', 'as', 'spam', 'and', 'thus', 'prevent', 'your', 'users', 'from', 'seeing', 'that', 'message', 'when', 'it', \"'s\", 'actually', 'legitimate', 'or', 'consider', 'an', 'example', 'of', 'a', 'model', 'that', 'needs', 'to', 'predict', 'whether', 'a', 'patient', 'has', 'a', 'terminal', 'illness', '.', 'In', 'this', 'case', ',', 'using', 'precision', 'as', 'your', 'evaluation', 'metric', 'does', \"n't\", 'account', 'for', 'false', 'negatives', 'in', 'your', 'model', 'here', '.', 'For', 'the', 'model', 'to', 'be', 'successful', '.', 'It', \"'s\", 'crucial', 'that', 'it', 'does', \"n't\", 'falsely', 'predict', 'the', 'absence', 'of', 'illness', 'in', 'a', 'patient', 'who', 'actually', 'has', 'that', 'illness', 'sensitivity', 'would', 'be', 'a', 'better', 'metric', 'to', 'use', 'for', 'this', 'situation', '.', 'But', 'it', 'does', \"n't\", 'always', 'need', 'to', 'be', 'one', 'or', 'the', 'other', '.', 'The', 'F', 'one', 'score', 'combines', 'precision', 'and', 'sensitivity', 'together', '.', 'It', 'gives', 'you', 'one', 'number', 'that', 'quantifies', 'the', 'overall', 'performance', 'of', 'a', 'particular', 'M', 'L', 'algorithm', '.', 'You', 'should', 'consider', 'using', 'an', 'F', 'one', 'score', 'when', 'you', 'have', 'a', 'class', 'imbalance', 'but', 'want', 'to', 'preserve', 'the', 'equality', 'between', 'precision', 'and', 'sensitivity', '.', 'But', 'what', 'do', 'you', 'do', 'if', 'you', \"'re\", 'dealing', 'with', 'a', 'regression', 'problem', '?', 'In', 'that', 'case', ',', 'there', 'are', 'other', 'common', 'metrics', 'you', 'can', 'use', 'to', 'evaluate', 'your', 'model', 'including', 'the', 'main', 'squared', 'error', '.', 'The', 'mean', 'squared', 'error', 'is', 'frequently', 'used', '.', 'Its', 'general', 'purpose', 'is', 'the', 'same', 'as', 'what', 'you', 'saw', 'with', 'classification', 'metrics', ',', 'you', 'determine', 'the', 'prediction', 'from', 'the', 'model', 'and', 'you', 'compare', 'the', 'difference', 'between', 'the', 'prediction', 'and', 'the', 'actual', 'outcome', '.', 'More', 'specifically', ',', 'you', 'take', 'the', 'difference', 'between', 'the', 'prediction', 'and', 'actual', 'value', 'square', 'that', 'difference', 'and', 'then', 'sum', 'up', 'all', 'the', 'squared', 'differences', 'for', 'all', 'the', 'observations', 'in', 'sky', 'kit', 'learn', '.', 'You', 'can', 'use', 'the', 'mean', 'squared', 'error', 'function', 'directly', 'from', 'the', 'metrics', 'library', '.', 'There', 'are', 'other', 'metrics', 'you', 'can', 'use', 'for', 'linear', 'models', 'such', 'as', 'R', 'squared', '.', 'So', 'you', \"'ve\", 'trained', 'your', 'model', 'performed', 'a', 'batch', 'transformation', 'on', 'your', 'test', 'data', 'and', 'calculated', 'your', 'metrics', '.', 'Now', ',', 'what', 'will', 'you', 'do', '?', 'You', \"'ll\", 'use', 'these', 'metrics', 'to', 'help', 'you', 'tune', 'the', 'model', '.', 'You', 'could', 'select', 'a', 'different', 'set', 'of', 'features', 'and', 'train', 'the', 'model', 'again', '.', 'After', 'you', 'retrain', 'the', 'model', 'ask', 'yourself', 'which', 'was', 'the', 'better', 'model', '?', 'The', 'metrics', 'will', 'help', 'inform', 'you', 'you', 'could', 'also', 'use', 'different', 'data', 'and', 'retrain', 'the', 'model', 'with', 'the', 'same', 'features', '.', 'Remember', 'K', 'fold', 'cross', 'validation', 'from', 'earlier', 'in', 'this', 'module', '.', 'Finally', ',', 'you', 'could', 'tune', 'the', 'parameters', 'of', 'the', 'model', 'itself', ',', 'which', 'is', 'the', 'subject', 'of', 'the', 'next', 'section', '.', 'Here', 'are', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'to', 'evaluate', 'the', 'model', '.', 'You', 'need', 'to', 'have', 'data', 'that', 'the', 'model', 'has', \"n't\", 'seen', '.', 'This', 'could', 'be', 'either', 'a', 'holdout', 'set', 'or', 'you', 'could', 'use', 'K', 'fold', 'cross', 'validation', '.', 'Different', 'machine', 'learning', 'models', 'use', 'different', 'metrics', 'classification', 'can', 'use', 'the', 'confusion', 'matrix', 'and', 'the', 'A', 'U', 'C', 'R', 'O', 'C', 'that', 'you', 'can', 'generate', 'from', 'it', 'regression', 'can', 'use', 'means', 'squared', '.', 'That', \"'s\", 'it', 'for', 'section', 'seven', '.', 'See', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . We 'll continue exploring how to evaluate your model . Classification model are going to return a probability for the target . This is a value of the input belonging to the target class and it will be between zero and one . To convert the value to a class . You need to determine the threshold to use . You might think it 's 50 % but you could change it to be lower or higher to improve your result . As you 've seen with sensitivity and specificity , there 's a trade off between correctly and incorrectly identifying class , changing the threshold can impact that outcome . We 're going to take a look at how you can visualize this a receiver operating characteristic graph is also known a an R C graph . It summarizes all the confusion matrix that each threshold produced to build one you calculate and plot the sensitivity or true positive rate against the false positive rate . On a graph . For each threshold value , you can calculate the false positive rate by subtracting the specificity from one . After you plot those point , you can draw a line between them . The dotted black line from 00 to 11 mean that the sensitivity or true positive rate is equal to the false positive rate . The point at 11 mean that you 've correctly identified all the cat , but you 've also incorrectly identified all the not cat . This is bad . Any point on this line mean that the proportion of correctly classified sample is the same a the proportion of incorrectly classified sample . The point at 00 represents that there are zero true positive and zero false positive . A model that ha high sensitivity and low false positive rate is usually the goal . So it 's considered to be better when the line between the threshold recording is closer towards the top left corner . If you had the data from two model , you could plot out the R O C curve for each model and compare them . However , that can be tedious . There 's another graph you can use for this which we 'll look at next . Another evaluation metric you can use is the area under the curve receiver operator curve which is also known a an A R C. The A U C part is the area under the plotted line . When the A U C is higher , it mean the model will be better at predicting cat a cat and not cat a not cat . You can use the A U C to quickly compare model with each other with the four number from our confusion matrix . You can calculate the model 's accuracy . This is also known a it score . You can do this by adding up the correct prediction and then dividing that number by the total number of prediction . Though accuracy is widely used metric for classification problem , it ha limitation . This metric is n't a effective when there are a lot of true negative case in your data set . Think about the cat , not cat example , if most of your accuracy is based on true negative , it say that your model is good at predicting what is n't a cat . In this case , you might not feel confident in your model ability to predict cat after you roll it out into production . This lead to an example of why it 's important to make sure that the metric you choose for model evaluation aligns to your business goal . Think about the credit card fraud example , in this case , using accuracy a your main metric probably is n't a good idea because you have a lot of true negative . Your high true negative number might hide the fact that your model 's ability to identify case of fraud that is to identify true positive is n't ideal a a credit card company . It 's probably unacceptable to have le than almost perfect performance , identifying fraud case that would drive customer away , which would be the opposite of what you 'd want to achieve from a business standpoint . This is why two other metric are often used in these situation . The first one is precision , which essentially remove the negative prediction . Precision is the proportion of positive prediction that are actually correct . You can calculate it by taking the true positive and dividing it by true positive plus false positive . When the cost of false positive is high in your particular business situation , precision might be a good metric . Think about a classification model that identifies email message a spam or not . In this case , you do n't want your model to label an email message a spam and thus prevent your user from seeing that message when it 's actually legitimate or consider an example of a model that need to predict whether a patient ha a terminal illness . In this case , using precision a your evaluation metric doe n't account for false negative in your model here . For the model to be successful . It 's crucial that it doe n't falsely predict the absence of illness in a patient who actually ha that illness sensitivity would be a better metric to use for this situation . But it doe n't always need to be one or the other . The F one score combine precision and sensitivity together . It give you one number that quantifies the overall performance of a particular M L algorithm . You should consider using an F one score when you have a class imbalance but want to preserve the equality between precision and sensitivity . But what do you do if you 're dealing with a regression problem ? In that case , there are other common metric you can use to evaluate your model including the main squared error . The mean squared error is frequently used . Its general purpose is the same a what you saw with classification metric , you determine the prediction from the model and you compare the difference between the prediction and the actual outcome . More specifically , you take the difference between the prediction and actual value square that difference and then sum up all the squared difference for all the observation in sky kit learn . You can use the mean squared error function directly from the metric library . There are other metric you can use for linear model such a R squared . So you 've trained your model performed a batch transformation on your test data and calculated your metric . Now , what will you do ? You 'll use these metric to help you tune the model . You could select a different set of feature and train the model again . After you retrain the model ask yourself which wa the better model ? The metric will help inform you you could also use different data and retrain the model with the same feature . Remember K fold cross validation from earlier in this module . Finally , you could tune the parameter of the model itself , which is the subject of the next section . Here are key takeaway from this section of the module to evaluate the model . You need to have data that the model ha n't seen . This could be either a holdout set or you could use K fold cross validation . Different machine learning model use different metric classification can use the confusion matrix and the A U C R O C that you can generate from it regression can use mean squared . That 's it for section seven . See you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', 'to', 'module', 'three', '.', 'This', 'is', 'section', 'eight', 'in', 'this', 'section', ',', 'we', \"'re\", 'going', 'to', 'take', 'a', 'look', 'at', 'how', 'you', 'can', 'tune', 'the', 'model', \"'s\", 'hyper', 'parameters', 'to', 'improve', 'model', 'performance', '.', 'Recall', 'from', 'an', 'earlier', 'module', 'that', 'hyper', 'parameters', 'can', 'be', 'thought', 'of', 'as', 'the', 'knobs', 'that', 'tune', 'the', 'machine', 'learning', 'algorithm', 'to', 'improve', 'its', 'performance', '.', 'Now', 'that', 'we', \"'re\", 'looking', 'more', 'explicitly', 'at', 'tuning', 'models', '.', 'It', \"'s\", 'time', 'to', 'look', 'more', 'specifically', 'at', 'the', 'different', 'types', 'of', 'hyper', 'parameters', 'and', 'how', 'to', 'perform', 'hyper', 'parameter', 'optimization', '.', 'There', 'are', 'a', 'couple', 'of', 'different', 'categories', 'of', 'hyper', 'parameters', '.', 'The', 'first', 'kind', 'are', 'model', 'hyper', 'parameters', '.', 'They', 'help', 'define', 'the', 'model', 'itself', 'as', 'an', 'example', ',', 'consider', 'a', 'neural', 'network', 'for', 'a', 'computer', 'vision', 'problem', 'for', 'this', 'case', '.', 'Additional', 'attributes', 'of', 'the', 'architecture', 'need', 'to', 'be', 'defined', 'like', 'filter', 'size', 'pooling', 'and', 'the', 'stride', 'or', 'padding', '.', 'The', 'second', 'kind', 'are', 'optimizer', 'hyper', 'parameters', '.', 'They', 'relate', 'to', 'how', 'the', 'model', 'learns', 'patterns', 'based', 'on', 'data', 'and', 'they', \"'re\", 'used', 'for', 'a', 'neural', 'network', 'model', '.', 'These', 'types', 'of', 'hyper', 'parameters', 'include', 'optimizes', 'like', 'gradient', 'descent', 'and', 'stochastic', 'gradient', 'descent', '.', 'They', 'can', 'also', 'include', 'optimizes', 'that', 'use', 'momentum', 'like', 'atom', 'or', 'that', 'initialize', 'the', 'parameter', 'weights', 'with', 'methods', 'like', 'xavier', 'initialization', 'or', 'he', 'initialization', '.', 'The', 'third', 'kind', 'are', 'data', 'hyper', 'parameters', '.', 'They', 'relate', 'to', 'the', 'attributes', 'of', 'the', 'data', 'itself', '.', 'These', 'include', 'attributes', 'that', 'define', 'different', 'data', 'augmentation', 'techniques', 'like', 'cropping', 'or', 'resizing', 'for', 'image', 'related', 'problems', '.', 'They', \"'re\", 'often', 'used', 'when', 'you', 'do', \"n't\", 'have', 'enough', 'data', 'or', 'enough', 'variation', 'in', 'your', 'data', 'tuning', 'hyper', 'parameters', 'can', 'be', 'very', 'labor', 'intensive', '.', 'Traditionally', ',', 'this', 'was', 'done', 'manually', 'by', 'someone', 'who', 'had', 'domain', 'experience', 'related', 'to', 'the', 'hyper', 'parameter', '.', 'And', 'the', 'use', 'case', ',', 'this', 'person', 'would', 'manually', 'select', 'the', 'hyper', 'parameters', 'based', 'on', 'their', 'intuition', 'and', 'experience', '.', 'Then', 'they', 'would', 'train', 'the', 'model', 'and', 'score', 'it', 'on', 'the', 'validation', 'data', '.', 'This', 'process', 'would', 'be', 'repeated', 'over', 'and', 'over', 'again', 'until', 'they', 'achieved', 'satisfactory', 'results', '.', 'This', 'manual', 'process', 'is', \"n't\", 'always', 'the', 'most', 'thorough', 'and', 'efficient', 'way', 'of', 'tuning', 'hyper', 'parameters', 'with', 'Sagemaker', ',', 'you', 'can', 'perform', 'automated', 'hyper', 'parameter', 'tuning', 'with', 'Amazon', \"'s\", 'Sagemaker', ',', 'automatic', 'model', 'tuning', '.', 'It', 'finds', 'the', 'best', 'version', 'of', 'a', 'model', 'by', 'running', 'multiple', 'training', 'jobs', 'on', 'your', 'data', 'set', '.', 'By', 'using', 'the', 'algorithm', 'and', 'hyper', 'parameter', 'ranges', '.', 'You', 'specify', 'it', 'then', 'chooses', 'the', 'hyper', 'parameter', 'values', 'that', 'results', 'in', 'a', 'model', 'that', 'performs', 'the', 'best', 'as', 'measured', 'by', 'a', 'metric', '.', 'You', 'choose', 'it', 'uses', 'Gaussian', 'process', 'regression', 'to', 'predict', 'which', 'hyper', 'parameter', 'values', 'might', 'be', 'most', 'effective', 'at', 'improving', 'fit', '.', 'It', 'also', 'uses', 'Bayesian', 'optimization', 'to', 'balance', 'exploring', 'the', 'hyper', 'parameter', 'space', 'and', 'exploiting', 'specific', 'hyper', 'parameter', 'values', 'when', 'appropriate', 'and', 'importantly', ',', 'automatic', 'model', 'tuning', 'can', 'be', 'used', 'with', 'built', 'in', 'algorithms', 'from', 'sagemaker', 'pre', 'built', 'deep', 'learning', 'frameworks', 'and', 'bring', 'your', 'own', 'algorithm', 'containers', 'suppose', 'that', 'you', 'want', 'to', 'solve', 'a', 'binary', 'classification', 'problem', 'on', 'a', 'fraud', 'data', 'set', '.', 'Your', 'goal', 'is', 'to', 'maximize', 'the', 'area', 'under', 'the', 'A', 'U', 'C', 'curve', 'metric', 'of', 'the', 'algorithm', '.', 'By', 'training', 'a', 'linear', 'learner', 'algorithm', 'model', '.', 'You', 'do', \"n't\", 'know', 'which', 'values', 'of', 'the', 'learning', 'rate', 'beta', 'one', 'beta', 'two', 'and', 'epoch', 'you', 'should', 'use', 'to', 'train', 'the', 'best', 'model', 'to', 'find', 'the', 'best', 'values', 'for', 'these', 'hyper', 'parameters', '.', 'You', 'can', 'specify', 'ranges', 'of', 'values', 'that', 'sagemaker', 'hyper', 'parameter', 'tuning', 'will', 'then', 'search', 'it', 'will', 'find', 'the', 'combination', 'of', 'values', 'that', 'results', 'in', 'the', 'training', 'job', 'that', 'performs', 'the', 'best', 'as', 'measured', 'by', 'the', 'objective', 'metric', 'that', 'you', 'chose', '.', 'In', 'the', 'example', '.', 'Sagemaker', 'hyper', 'parameter', 'tuning', 'launches', 'training', 'jobs', 'that', 'use', 'hyper', 'parameter', 'values', 'in', 'the', 'ranges', 'you', 'specified', 'and', 'then', 'returns', 'the', 'training', 'job', 'with', 'the', 'highest', 'A', 'U', 'C', 'hyper', 'parameter', 'tuning', 'might', 'not', 'necessarily', 'improve', 'your', 'model', '.', 'It', \"'s\", 'an', 'advanced', 'tool', 'for', 'building', 'machine', 'solutions', 'as', 'such', '.', 'It', 'should', 'be', 'considered', 'part', 'of', 'the', 'scientific', 'method', 'process', '.', 'When', 'you', 'build', 'complex', 'machine', 'learning', 'systems', 'like', 'deep', 'learning', 'neural', 'networks', 'exploring', 'all', 'possible', 'combinations', 'is', 'impractical', 'to', 'improve', 'optimization', '.', 'Use', 'the', 'following', 'guidelines', 'when', 'you', 'create', 'hyper', 'parameters', 'first', ',', 'instead', 'of', 'using', 'all', 'hyper', 'parameters', ',', 'limit', 'the', 'number', 'of', 'hyper', 'parameters', 'to', 'the', 'ones', 'you', 'think', 'would', 'give', 'you', 'good', 'results', '.', 'The', 'range', 'of', 'values', 'for', 'the', 'hyper', 'parameters', '.', 'You', 'choose', 'to', 'search', 'can', 'significantly', 'affect', 'the', 'success', 'of', 'hyper', 'parameter', 'optimization', '.', 'Although', 'you', 'might', 'want', 'to', 'specify', 'a', 'large', 'range', 'that', 'covers', 'every', 'possible', 'value', 'for', 'a', 'hyper', 'parameter', ',', 'you', \"'ll\", 'get', 'better', 'results', 'by', 'limiting', 'your', 'search', 'to', 'a', 'small', 'range', 'of', 'values', '.', 'If', 'you', 'get', 'the', 'best', 'metric', 'values', 'within', 'a', 'part', 'of', 'a', 'range', ',', 'consider', 'limiting', 'the', 'range', 'to', 'only', 'that', 'part', 'during', 'hyper', 'parameter', 'tuning', '.', 'Sagemaker', 'attempts', 'to', 'figure', 'out', 'if', 'your', 'hyper', 'parameters', 'are', 'log', 'scaled', 'or', 'linear', 'scaled', '.', 'Initially', ',', 'it', 'assumes', 'the', 'hyper', 'parameters', 'are', 'linear', 'scaled', '.', 'If', 'they', 'should', 'be', 'log', 'scaled', ',', 'it', 'might', 'take', 'time', 'for', 'Sagemaker', 'to', 'discover', 'that', 'on', 'its', 'own', '.', 'If', 'you', 'know', 'that', 'a', 'hyper', 'parameter', 'should', 'be', 'long', 'scale', 'and', 'you', 'can', 'convert', 'it', 'yourself', 'doing', 'so', 'can', 'improve', 'hyper', 'parameter', 'optimization', '.', 'Running', 'more', 'hyper', 'parameter', 'tuning', 'jobs', 'concurrently', 'gets', 'more', 'work', 'done', 'quickly', '.', 'But', 'a', 'tuning', 'job', 'improves', 'only', 'through', 'successive', 'rounds', 'of', 'experiments', '.', 'Typically', 'running', 'one', 'training', 'job', 'at', 'a', 'time', ',', 'achieves', 'the', 'best', 'results', 'with', 'the', 'least', 'amount', 'of', 'compute', 'time', '.', 'See', 'that', 'you', 'have', 'a', 'distributed', 'training', 'job', 'that', 'runs', 'on', 'multiple', 'instances', '.', 'In', 'this', 'case', ',', 'hyper', 'parameter', 'tuning', 'uses', 'the', 'last', 'reported', 'objective', 'metric', 'from', 'all', 'instances', 'of', 'that', 'training', 'job', 'as', 'the', 'value', 'of', 'the', 'objective', 'metric', 'for', 'that', 'training', 'job', 'design', ',', 'distributed', 'training', 'jobs', 'so', 'that', 'they', 'report', 'the', 'objective', 'metric', 'you', 'want', 'now', 'that', 'you', \"'ve\", 'gone', 'through', 'the', 'end', 'to', 'end', 'process', 'of', 'training', 'and', 'tuning', 'a', 'machine', 'learning', 'model', '.', 'It', \"'s\", 'worth', 'talking', 'about', 'Amazon', \"'s\", 'sagemaker', 'autopilot', '.', 'This', 'service', 'can', 'help', 'you', 'find', 'a', 'good', 'model', 'with', 'little', 'effort', 'or', 'input', 'on', 'your', 'part', 'with', 'autopilot', '.', 'You', 'create', 'a', 'job', 'that', 'supplies', 'the', 'test', 'training', 'and', 'target', 'autopilot', 'will', 'analyze', 'the', 'data', ',', 'select', 'appropriate', 'features', 'and', 'then', 'train', 'and', 'tune', 'the', 'models', '.', 'It', 'will', 'document', 'the', 'metrics', 'and', 'find', 'the', 'best', 'model', 'based', 'on', 'the', 'provided', 'data', '.', 'The', 'results', 'include', 'the', 'winning', 'model', 'and', 'metrics', 'and', 'a', 'Jupiter', 'notebook', '.', 'You', 'can', 'use', 'to', 'investigate', 'the', 'results', '.', 'Although', 'using', 'autopilot', 'does', \"n't\", 'remove', 'your', 'need', 'to', 'prep', 'process', 'the', 'data', '.', 'It', 'can', 'save', 'you', 'time', 'during', 'feature', 'selection', 'and', 'model', 'tuning', '.', 'Some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'include', 'these', 'points', '.', 'First', 'model', 'tuning', 'is', 'important', 'for', 'finding', 'the', 'best', 'solution', 'to', 'your', 'business', 'problem', '.', 'Hyper', 'parameters', 'can', 'be', 'tuned', 'for', 'the', 'model', 'optimizer', 'and', 'data', 'sagemaker', 'can', 'perform', 'automatic', 'hyper', 'parameter', 'tuning', '.', 'And', 'finally', ',', 'overall', 'model', 'development', 'can', 'be', 'accelerated', 'by', 'using', 'autopilot', '.', 'That', \"'s\", 'it', 'for', 'this', 'video', '.', 'See', 'you', 'in', 'the', 'next', 'one', '.']\n",
      "Hi and welcome back to module three . This is section eight in this section , we 're going to take a look at how you can tune the model 's hyper parameter to improve model performance . Recall from an earlier module that hyper parameter can be thought of a the knob that tune the machine learning algorithm to improve it performance . Now that we 're looking more explicitly at tuning model . It 's time to look more specifically at the different type of hyper parameter and how to perform hyper parameter optimization . There are a couple of different category of hyper parameter . The first kind are model hyper parameter . They help define the model itself a an example , consider a neural network for a computer vision problem for this case . Additional attribute of the architecture need to be defined like filter size pooling and the stride or padding . The second kind are optimizer hyper parameter . They relate to how the model learns pattern based on data and they 're used for a neural network model . These type of hyper parameter include optimizes like gradient descent and stochastic gradient descent . They can also include optimizes that use momentum like atom or that initialize the parameter weight with method like xavier initialization or he initialization . The third kind are data hyper parameter . They relate to the attribute of the data itself . These include attribute that define different data augmentation technique like cropping or resizing for image related problem . They 're often used when you do n't have enough data or enough variation in your data tuning hyper parameter can be very labor intensive . Traditionally , this wa done manually by someone who had domain experience related to the hyper parameter . And the use case , this person would manually select the hyper parameter based on their intuition and experience . Then they would train the model and score it on the validation data . This process would be repeated over and over again until they achieved satisfactory result . This manual process is n't always the most thorough and efficient way of tuning hyper parameter with Sagemaker , you can perform automated hyper parameter tuning with Amazon 's Sagemaker , automatic model tuning . It find the best version of a model by running multiple training job on your data set . By using the algorithm and hyper parameter range . You specify it then chooses the hyper parameter value that result in a model that performs the best a measured by a metric . You choose it us Gaussian process regression to predict which hyper parameter value might be most effective at improving fit . It also us Bayesian optimization to balance exploring the hyper parameter space and exploiting specific hyper parameter value when appropriate and importantly , automatic model tuning can be used with built in algorithm from sagemaker pre built deep learning framework and bring your own algorithm container suppose that you want to solve a binary classification problem on a fraud data set . Your goal is to maximize the area under the A U C curve metric of the algorithm . By training a linear learner algorithm model . You do n't know which value of the learning rate beta one beta two and epoch you should use to train the best model to find the best value for these hyper parameter . You can specify range of value that sagemaker hyper parameter tuning will then search it will find the combination of value that result in the training job that performs the best a measured by the objective metric that you chose . In the example . Sagemaker hyper parameter tuning launch training job that use hyper parameter value in the range you specified and then return the training job with the highest A U C hyper parameter tuning might not necessarily improve your model . It 's an advanced tool for building machine solution a such . It should be considered part of the scientific method process . When you build complex machine learning system like deep learning neural network exploring all possible combination is impractical to improve optimization . Use the following guideline when you create hyper parameter first , instead of using all hyper parameter , limit the number of hyper parameter to the one you think would give you good result . The range of value for the hyper parameter . You choose to search can significantly affect the success of hyper parameter optimization . Although you might want to specify a large range that cover every possible value for a hyper parameter , you 'll get better result by limiting your search to a small range of value . If you get the best metric value within a part of a range , consider limiting the range to only that part during hyper parameter tuning . Sagemaker attempt to figure out if your hyper parameter are log scaled or linear scaled . Initially , it assumes the hyper parameter are linear scaled . If they should be log scaled , it might take time for Sagemaker to discover that on it own . If you know that a hyper parameter should be long scale and you can convert it yourself doing so can improve hyper parameter optimization . Running more hyper parameter tuning job concurrently get more work done quickly . But a tuning job improves only through successive round of experiment . Typically running one training job at a time , achieves the best result with the least amount of compute time . See that you have a distributed training job that run on multiple instance . In this case , hyper parameter tuning us the last reported objective metric from all instance of that training job a the value of the objective metric for that training job design , distributed training job so that they report the objective metric you want now that you 've gone through the end to end process of training and tuning a machine learning model . It 's worth talking about Amazon 's sagemaker autopilot . This service can help you find a good model with little effort or input on your part with autopilot . You create a job that supply the test training and target autopilot will analyze the data , select appropriate feature and then train and tune the model . It will document the metric and find the best model based on the provided data . The result include the winning model and metric and a Jupiter notebook . You can use to investigate the result . Although using autopilot doe n't remove your need to prep process the data . It can save you time during feature selection and model tuning . Some key takeaway from this section of the module include these point . First model tuning is important for finding the best solution to your business problem . Hyper parameter can be tuned for the model optimizer and data sagemaker can perform automatic hyper parameter tuning . And finally , overall model development can be accelerated by using autopilot . That 's it for this video . See you in the next one .\n",
      "['It', \"'s\", 'now', 'time', 'to', 'review', 'the', 'module', 'and', 'wrap', 'up', 'with', 'a', 'knowledge', 'check', 'in', 'this', 'module', '.', 'You', 'learned', 'how', 'to', 'formulate', 'a', 'problem', 'from', 'a', 'business', 'request', ',', 'obtain', 'and', 'secure', 'data', 'for', 'machine', 'learning', 'build', 'a', 'Jupiter', 'notebook', 'by', 'using', 'Amazon', 'Sagemaker', 'outline', 'the', 'process', 'for', 'evaluating', 'data', '.', 'Explain', 'why', 'data', 'needs', 'to', 'be', 'pre', 'processed', '.', 'Use', 'open', 'source', 'tools', 'to', 'examine', 'and', 'prep', 'process', 'data', '.', 'Use', 'Amazon', 'Sagemaker', 'to', 'train', 'and', 'host', 'a', 'machine', 'learning', 'model', '.', 'Use', 'cross', 'validation', 'to', 'test', 'the', 'performance', 'of', 'an', 'ML', 'model', '.', 'Use', 'a', 'hosted', 'model', 'for', 'inference', 'and', 'create', 'an', 'Amazon', 'Sagemaker', 'hyper', 'parameter', 'tuning', 'job', 'to', 'optimize', 'a', 'model', \"'s\", 'effectiveness', 'that', 'concludes', 'this', 'module', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'again', 'in', 'the', 'next', 'video', '.']\n",
      "It 's now time to review the module and wrap up with a knowledge check in this module . You learned how to formulate a problem from a business request , obtain and secure data for machine learning build a Jupiter notebook by using Amazon Sagemaker outline the process for evaluating data . Explain why data need to be pre processed . Use open source tool to examine and prep process data . Use Amazon Sagemaker to train and host a machine learning model . Use cross validation to test the performance of an ML model . Use a hosted model for inference and create an Amazon Sagemaker hyper parameter tuning job to optimize a model 's effectiveness that concludes this module . Thanks for watching . We 'll see you again in the next video .\n",
      "['Hi', 'and', 'welcome', 'to', 'module', 'four', 'of', 'Aws', 'Academy', 'Machine', 'learning', 'in', 'this', 'module', ',', 'we', \"'re\", 'going', 'to', 'look', 'at', 'forecasting', '.', 'We', \"'ll\", 'start', 'with', 'an', 'introduction', 'to', 'forecasting', 'and', 'look', 'at', 'how', 'time', 'series', 'data', 'is', 'different', 'from', 'other', 'kinds', 'of', 'data', '.', 'Then', 'we', \"'re\", 'going', 'to', 'look', 'at', 'Amazon', 'forecast', 'a', 'service', 'that', 'helps', 'you', 'simplify', 'building', 'forecasts', '.', 'At', 'the', 'end', 'of', 'this', 'module', ',', 'you', \"'ll\", 'be', 'able', 'to', 'describe', 'the', 'business', 'problem', 'solved', 'with', 'Amazon', 'forecast', '.', 'Describe', 'the', 'challenges', 'of', 'working', 'with', 'time', 'series', 'data', 'list', 'the', 'steps', 'required', 'to', 'create', 'a', 'forecast', 'by', 'using', 'Amazon', 'forecast', 'and', 'use', 'Amazon', 'forecast', 'to', 'make', 'a', 'prediction', '.', 'See', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome to module four of Aws Academy Machine learning in this module , we 're going to look at forecasting . We 'll start with an introduction to forecasting and look at how time series data is different from other kind of data . Then we 're going to look at Amazon forecast a service that help you simplify building forecast . At the end of this module , you 'll be able to describe the business problem solved with Amazon forecast . Describe the challenge of working with time series data list the step required to create a forecast by using Amazon forecast and use Amazon forecast to make a prediction . See you in the next video .\n",
      "['Hi', 'and', 'welcome', 'to', 'section', 'one', '.', 'We', \"'ll\", 'get', 'started', 'by', 'reviewing', 'what', 'forecasting', 'is', 'and', 'some', 'use', 'cases', 'for', 'it', '.', 'Forecasting', 'is', 'an', 'important', 'area', 'of', 'machine', 'learning', '.', 'It', \"'s\", 'important', 'because', 'there', 'are', 'so', 'many', 'opportunities', 'for', 'predicting', 'future', 'outcomes', 'based', 'on', 'historical', 'data', '.', 'Many', 'of', 'these', 'opportunities', 'involve', 'a', 'time', 'component', '.', 'However', ',', 'while', 'the', 'time', 'component', 'adds', 'additional', 'information', ',', 'it', 'also', 'makes', 'time', 'series', 'problems', 'more', 'difficult', 'to', 'handle', 'compared', 'to', 'other', 'types', 'of', 'predictions', '.', 'You', 'can', 'think', 'of', 'time', 'series', 'data', 'as', 'falling', 'into', 'two', 'broad', 'categories', '.', 'The', 'first', 'type', 'is', 'Univ', 'variate', 'data', 'which', 'means', 'there', \"'s\", 'just', 'one', 'variable', '.', 'The', 'second', 'one', 'is', 'Multivariate', 'data', 'which', 'means', 'there', \"'s\", 'more', 'than', 'one', 'variable', '.', 'There', 'are', 'several', 'common', 'patterns', 'in', 'time', 'series', 'data', '.', 'The', 'first', 'pattern', 'is', 'a', 'trend', 'with', 'a', 'trend', '.', 'You', 'get', 'a', 'pattern', 'with', 'the', 'values', 'increasing', ',', 'decreasing', 'or', 'staying', 'the', 'same', 'over', 'time', '.', 'There', 'are', 'seasonal', 'patterns', ',', 'these', 'reflect', 'times', 'of', 'the', 'year', ',', 'month', ',', 'day', 'or', 'other', 'patterns', '.', 'Cyclical', 'patterns', 'are', 'similar', 'to', 'seasonal', 'patterns', '.', 'These', 'are', 'patterns', 'that', 'repeat', 'like', 'a', 'large', 'retail', 'sale', 'event', 'that', 'happens', 'the', 'same', 'time', 'each', 'year', '.', 'Finally', ',', 'there', 'are', 'changes', 'in', 'the', 'data', 'over', 'time', 'that', 'appear', 'to', 'be', 'random', 'or', 'that', 'have', 'no', 'discernible', 'pattern', '.', 'There', 'are', 'many', 'uses', 'for', 'forecasting', '.', 'You', 'can', 'use', 'forecasting', 'in', 'marketing', 'applications', 'such', 'as', 'for', 'sales', 'forecasting', 'or', 'demand', 'projections', '.', 'It', 'could', 'also', 'be', 'used', 'in', 'inventory', 'management', 'systems', 'that', 'anticipate', 'required', 'inventory', 'levels', '.', 'Forecasting', '.', 'Energy', 'consumption', 'can', 'help', 'predict', 'when', 'and', 'where', 'energy', 'is', 'needed', '.', 'And', 'weather', 'forecasting', 'systems', 'can', 'be', 'used', 'for', 'governments', 'and', 'commercial', 'applications', 'such', 'as', 'agriculture', '.', 'That', \"'s\", 'it', 'for', 'this', 'section', '.', 'See', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi and welcome to section one . We 'll get started by reviewing what forecasting is and some use case for it . Forecasting is an important area of machine learning . It 's important because there are so many opportunity for predicting future outcome based on historical data . Many of these opportunity involve a time component . However , while the time component add additional information , it also make time series problem more difficult to handle compared to other type of prediction . You can think of time series data a falling into two broad category . The first type is Univ variate data which mean there 's just one variable . The second one is Multivariate data which mean there 's more than one variable . There are several common pattern in time series data . The first pattern is a trend with a trend . You get a pattern with the value increasing , decreasing or staying the same over time . There are seasonal pattern , these reflect time of the year , month , day or other pattern . Cyclical pattern are similar to seasonal pattern . These are pattern that repeat like a large retail sale event that happens the same time each year . Finally , there are change in the data over time that appear to be random or that have no discernible pattern . There are many us for forecasting . You can use forecasting in marketing application such a for sale forecasting or demand projection . It could also be used in inventory management system that anticipate required inventory level . Forecasting . Energy consumption can help predict when and where energy is needed . And weather forecasting system can be used for government and commercial application such a agriculture . That 's it for this section . See you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', '.', 'This', 'is', 'section', 'two', 'and', 'we', \"'re\", 'going', 'to', 'focus', 'on', 'processing', 'time', 'series', 'data', 'because', 'it', 'can', 'be', 'different', 'from', 'other', 'types', 'of', 'data', 'you', \"'ve\", 'been', 'using', 'so', 'far', '.', 'Time', 'series', 'data', 'is', 'data', 'that', 'is', 'captured', 'in', 'chronological', 'sequence', 'over', 'a', 'defined', 'period', 'of', 'time', '.', 'Introducing', 'time', 'into', 'a', 'machine', 'learning', 'model', 'has', 'a', 'positive', 'impact', 'because', 'the', 'model', 'can', 'derive', 'meaning', 'from', 'changes', 'in', 'the', 'data', 'points', 'over', 'time', 'time', 'series', 'data', 'tends', 'to', 'be', 'correlated', '.', 'This', 'means', 'that', 'there', \"'s\", 'a', 'dependency', 'between', 'data', 'points', '.', 'This', 'has', 'mixed', 'results', 'for', 'forecasting', '.', 'This', 'is', 'because', 'you', \"'re\", 'dealing', 'with', 'the', 'regression', 'problem', 'and', 'regression', 'assumes', 'that', 'data', 'points', 'are', 'independent', '.', 'You', 'need', 'to', 'develop', 'a', 'method', 'for', 'dealing', 'with', 'data', 'dependence', '.', 'So', 'you', 'can', 'increase', 'the', 'validity', 'of', 'the', 'predictions', '.', 'In', 'addition', 'to', 'the', 'time', 'series', 'data', ',', 'you', 'can', 'add', 'related', 'data', 'to', 'augment', 'a', 'forecasting', 'model', '.', 'For', 'example', ',', 'suppose', 'you', 'want', 'to', 'make', 'a', 'prediction', 'about', 'retail', 'sales', '.', 'You', 'could', 'include', 'information', 'about', 'the', 'product', 'being', 'sold', 'such', 'as', 'item', 'identification', 'or', 'sales', 'price', 'along', 'with', 'the', 'number', 'of', 'units', 'sold', 'per', 'time', 'period', '.', 'The', 'third', 'type', 'of', 'data', 'is', 'metadata', 'about', 'the', 'data', 'set', 'for', 'instance', ',', 'say', 'that', 'you', 'have', 'a', 'retail', 'data', 'set', ',', 'you', 'might', 'want', 'to', 'include', 'metadata', 'like', 'a', 'brand', 'name', 'or', 'a', 'genre', 'for', 'music', 'or', 'videos', '.', 'So', 'you', 'can', 'group', 'results', '.', 'It', \"'s\", 'better', 'to', 'have', 'more', 'data', '.', 'When', 'you', 'work', 'with', 'multiple', 'data', 'sources', ',', 'you', \"'ll\", 'face', 'the', 'challenge', 'of', 'handling', 'the', 'time', 'stamp', 'of', 'the', 'data', '.', 'You', \"'ll\", 'observe', 'differences', 'in', 'the', 'time', 'stamp', 'format', 'and', 'other', 'challenges', 'such', 'as', 'incomplete', 'data', '.', 'However', ',', 'you', 'might', 'be', 'able', 'to', 'infer', 'missing', 'data', 'in', 'some', 'cases', '.', 'For', 'example', ',', 'say', 'you', 'have', 'some', 'data', 'that', 'contains', 'both', 'the', 'month', 'and', 'the', 'day', ',', 'but', 'no', 'year', 'observe', 'whether', 'the', 'data', 'seems', 'to', 'sequence', 'through', 'the', 'month', 'numbers', 'in', 'the', 'database', 'repeating', 'after', '12', 'if', 'it', 'does', '.', 'So', 'you', 'could', 'add', 'the', 'year', '.', 'If', 'you', 'knew', 'when', 'the', 'data', 'started', ',', 'you', 'could', 'then', 'infer', 'future', 'years', 'based', 'on', 'the', 'order', 'of', 'the', 'data', '.', 'Much', 'time', 'stamp', 'data', 'is', 'stored', 'in', 'U', 'T', 'C', 'format', 'but', 'not', 'all', 'data', 'is', ',', 'you', 'should', 'check', 'if', 'the', 'time', 'stamp', 'is', 'in', 'local', 'or', 'universal', 'time', '.', 'Sometimes', 'the', 'time', 'stamp', 'does', \"n't\", 'represent', 'the', 'time', 'you', 'think', 'it', 'does', '.', 'For', 'example', ',', 'suppose', 'you', 'have', 'a', 'database', 'of', 'cars', 'that', 'were', 'serviced', 'at', 'a', 'garage', '.', 'Does', 'the', 'time', 'stamp', 'indicate', 'the', 'time', 'the', 'car', 'arrived', 'was', 'completed', 'or', 'picked', 'up', '?', 'Or', 'does', 'it', 'indicate', 'when', 'the', 'final', 'entry', 'was', 'entered', 'into', 'the', 'system', '?', 'Say', 'you', \"'re\", 'trying', 'to', 'model', 'the', 'hourly', 'caloric', 'intake', 'of', 'patients', '.', 'However', ',', 'you', 'only', 'have', 'daily', 'data', 'then', 'you', \"'ll\", 'need', 'to', 'adjust', 'your', 'target', 'time', 'scale', '.', 'Also', ',', 'your', 'data', 'might', 'not', 'have', 'any', 'time', 'stamps', '.', 'There', 'could', 'be', 'other', 'ways', 'to', 'extrapolate', 'a', 'time', 'series', 'depending', 'on', 'the', 'data', 'and', 'domain', '.', 'For', 'example', ',', 'you', 'might', 'have', 'wavelength', 'measurements', 'or', 'vectors', 'within', 'an', 'image', '.', 'As', 'a', 'final', 'note', ',', 'remember', 'that', 'daylight', 'savings', 'is', 'different', 'around', 'the', 'world', '.', 'Also', 'because', 'of', 'daylight', 'savings', 'time', 'might', 'even', 'occur', 'twice', 'a', 'year', 'in', 'their', 'time', 'zones', '.', 'A', 'common', 'occurrence', 'in', 'real', 'world', 'forecasting', 'problems', 'is', 'missing', 'values', 'in', 'the', 'raw', 'data', '.', 'Missing', 'values', 'makes', 'it', 'harder', 'for', 'a', 'model', 'to', 'generate', 'a', 'forecast', '.', 'The', 'primary', 'example', 'in', 'retail', 'is', 'an', 'out', 'of', 'stock', 'situation', 'in', 'demand', 'forecasting', '.', 'If', 'an', 'item', 'goes', 'out', 'of', 'stock', ',', 'the', 'sales', 'for', 'the', 'day', 'will', 'zero', '.', 'If', 'the', 'forecast', 'is', 'generated', 'based', 'on', 'those', 'zero', 'sales', 'values', ',', 'the', 'forecast', 'will', 'be', 'incorrect', '.', 'There', 'are', 'many', 'reasons', 'why', 'values', 'can', 'be', 'marked', 'as', 'missing', ',', 'missing', 'values', 'can', 'occur', 'because', 'of', 'no', 'transaction', '.', 'They', 'can', 'also', 'occur', 'because', 'of', 'possible', 'measurement', 'errors', '.', 'For', 'example', ',', 'a', 'service', 'that', 'monitored', 'certain', 'data', 'was', \"n't\", 'working', 'correctly', '.', 'Or', 'as', 'another', 'example', ',', 'the', 'measurement', 'could', \"n't\", 'happen', 'correctly', 'in', 'retail', '.', 'The', 'primary', 'example', 'for', 'an', 'inability', 'to', 'take', 'correct', 'measurements', 'is', 'an', 'out', 'of', 'stock', 'situation', 'in', 'demand', 'forecasting', '.', 'This', 'means', 'that', 'demand', 'does', \"n't\", 'equal', 'sales', 'on', 'that', 'day', '.', 'There', 'are', 'several', 'ways', 'you', 'can', 'calculate', 'the', 'missing', 'data', '.', 'The', 'first', 'method', 'is', 'forward', 'fill', '.', 'This', 'uses', 'the', 'last', 'known', 'value', 'for', 'the', 'missing', 'value', '.', 'Building', 'on', 'that', 'idea', ',', 'moving', 'average', 'uses', 'the', 'average', 'of', 'the', 'last', 'known', 'values', 'to', 'calculate', 'the', 'missing', 'value', 'backward', 'fill', 'uses', 'the', 'next', 'known', 'value', 'after', 'the', 'missing', 'value', '.', 'The', 'danger', 'here', 'is', 'that', 'you', \"'re\", 'using', 'the', 'future', 'to', 'calculate', 'the', 'past', ',', 'which', 'is', 'bad', 'in', 'forecasting', '.', 'This', 'method', 'is', 'also', 'known', 'as', 'look', 'ahead', 'and', 'should', 'be', 'avoided', '.', 'Interpolation', 'uses', 'an', 'equation', 'to', 'calculate', 'the', 'missing', 'value', '.', 'You', 'can', 'also', 'use', 'a', 'zero', 'fill', '.', 'This', 'is', 'often', 'used', 'in', 'retail', 'because', 'missing', 'sales', 'data', 'should', \"n't\", 'be', 'calculated', '.', 'The', 'missing', 'data', 'represents', 'that', 'there', 'were', 'no', 'orders', 'on', 'that', 'day', '.', 'It', 'would', 'be', 'wise', 'to', 'investigate', 'why', 'this', 'happened', '.', 'But', 'in', 'this', 'case', ',', 'you', 'do', \"n't\", 'want', 'to', 'fill', 'in', 'the', 'missing', 'value', '.', 'You', 'might', 'get', 'data', 'at', 'different', 'frequencies', '.', 'For', 'example', ',', 'you', 'might', 'have', 'sales', 'data', 'that', 'includes', 'the', 'exact', 'time', 'stamp', ',', 'the', 'sale', 'was', 'recorded', 'but', 'have', 'inventory', 'data', 'that', 'only', 'contains', 'the', 'year', ',', 'month', 'and', 'day', 'of', 'the', 'inventory', 'level', '.', 'When', 'you', 'have', 'data', 'that', \"'s\", 'at', 'a', 'different', 'frequency', 'than', 'other', 'data', 'sets', 'or', 'data', 'that', \"'s\", 'not', 'compatible', 'with', 'your', 'question', '.', 'You', 'might', 'need', 'to', 'down', 'sample', '.', 'Down', 'sampling', 'is', 'moving', 'from', 'a', 'more', 'finely', 'grained', 'time', 'to', 'a', 'less', 'finely', 'grained', 'time', '.', 'As', 'the', 'example', 'shows', 'this', 'could', 'be', 'converting', 'an', 'hourly', 'data', 'set', 'to', 'a', 'daily', 'data', 'set', '.', 'When', 'down', 'sampling', ',', 'you', 'need', 'to', 'decide', 'how', 'to', 'combine', 'the', 'values', '.', 'In', 'the', 'previous', 'case', 'of', 'sales', 'data', '.', 'Summing', 'the', 'quantity', 'makes', 'the', 'most', 'sense', '.', 'If', 'the', 'data', 'is', 'temperature', '.', 'You', 'might', 'want', 'to', 'find', 'the', 'average', 'understanding', 'your', 'data', 'helps', 'you', 'decide', 'what', \"'s\", 'the', 'best', 'course', 'of', 'action', '.', 'The', 'opposite', 'of', 'down', 'sampling', 'is', 'up', 'sampling', '.', 'When', 'you', 'move', 'from', 'a', 'less', 'finely', 'grained', 'time', 'to', 'a', 'more', 'finely', 'grained', 'time', '.', 'The', 'problem', 'with', 'up', 'sampling', 'is', 'that', 'it', \"'s\", 'extremely', 'difficult', 'to', 'achieve', '.', 'In', 'most', 'cases', ',', 'suppose', 'you', 'want', 'to', 'up', 'sample', 'your', 'sales', 'data', 'from', 'daily', 'sales', 'to', 'hourly', 'sales', '.', 'Unless', 'you', 'have', 'some', 'other', 'data', 'source', 'to', 'reference', ',', 'you', 'would', \"n't\", 'be', 'able', 'to', 'do', 'this', '.', 'There', 'are', 'cases', 'when', 'you', 'need', 'to', 'do', 'something', 'perhaps', 'to', 'match', 'the', 'frequency', 'of', 'another', 'time', 'series', 'or', 'you', 'might', 'have', 'an', 'irregular', 'time', 'series', 'or', 'specific', 'domain', 'knowledge', 'that', 'would', 'help', 'in', 'those', 'cases', ',', 'you', 'need', 'to', 'be', 'careful', 'how', 'you', 'make', 'the', 'conversion', 'for', 'the', 'retail', 'example', '.', 'The', 'best', 'you', 'could', 'do', 'is', 'create', 'a', 'single', 'order', 'for', 'the', 'day', 'at', 'a', 'specified', 'hour', '.', 'For', 'temperature', '.', 'You', 'could', 'copy', 'the', 'daily', 'temperature', 'into', 'each', 'hourly', 'slot', 'or', 'use', 'a', 'formula', 'to', 'calculate', 'a', 'curve', 'in', 'data', 'science', '.', 'Outliers', 'have', 'a', 'mix', 'of', 'positive', 'and', 'negative', 'attributes', '.', 'The', 'same', 'is', 'true', 'of', 'time', 'series', 'data', '.', 'Suppose', 'you', 'were', 'examining', 'sales', 'data', 'and', 'you', 'had', 'an', 'order', 'that', 'has', 'an', 'unusually', 'high', 'number', 'of', 'items', '.', 'You', 'might', 'not', 'want', 'to', 'include', 'that', 'in', 'your', 'forecast', 'calculations', 'because', 'the', 'order', 'size', 'might', 'never', 'be', 'repeated', ',', 'removing', 'these', 'outliers', 'and', 'anomalies', 'is', 'known', 'as', 'smoothing', ',', 'smoothing', 'your', 'data', 'can', 'help', 'you', 'deal', 'with', 'outliers', 'and', 'other', 'anomalies', '.', 'There', 'are', 'a', 'few', 'reasons', 'why', 'you', 'might', 'consider', 'smoothing', 'first', 'during', 'data', 'preparation', ',', 'you', 'remove', 'error', 'values', 'and', 'could', 'also', 'remove', 'outliers', '.', 'You', 'might', 'also', 'want', 'to', 'smooth', 'your', 'data', 'to', 'generate', 'features', '.', 'For', 'visualization', '.', 'You', 'could', 'smooth', 'your', 'data', 'to', 'reduce', 'the', 'noise', 'in', 'a', 'plot', '.', 'It', \"'s\", 'important', 'to', 'understand', 'why', 'you', 'are', 'smoothing', 'the', 'data', 'and', 'the', 'impact', 'that', 'it', 'might', 'have', '.', 'The', 'outcome', 'might', 'be', 'to', 'reduce', 'noise', 'and', 'create', 'a', 'better', 'model', '.', 'But', 'an', 'equally', 'important', 'question', 'is', ',', 'could', 'your', 'smoothing', 'compromise', 'the', 'model', 'is', 'the', 'model', 'expecting', 'noisy', 'data', '?', 'Will', 'you', 'also', 'be', 'able', 'to', 'smooth', 'the', 'data', 'in', 'production', '?', 'That', \"'s\", 'it', '.', 'For', 'part', 'one', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'two', 'where', 'we', \"'ll\", 'review', 'more', 'time', 'series', 'specific', 'challenges', 'and', 'the', 'tools', 'and', 'algorithms', 'that', 'can', 'help', 'us', 'wrangle', 'your', 'data', '.']\n",
      "Hi and welcome back . This is section two and we 're going to focus on processing time series data because it can be different from other type of data you 've been using so far . Time series data is data that is captured in chronological sequence over a defined period of time . Introducing time into a machine learning model ha a positive impact because the model can derive meaning from change in the data point over time time series data tends to be correlated . This mean that there 's a dependency between data point . This ha mixed result for forecasting . This is because you 're dealing with the regression problem and regression assumes that data point are independent . You need to develop a method for dealing with data dependence . So you can increase the validity of the prediction . In addition to the time series data , you can add related data to augment a forecasting model . For example , suppose you want to make a prediction about retail sale . You could include information about the product being sold such a item identification or sale price along with the number of unit sold per time period . The third type of data is metadata about the data set for instance , say that you have a retail data set , you might want to include metadata like a brand name or a genre for music or video . So you can group result . It 's better to have more data . When you work with multiple data source , you 'll face the challenge of handling the time stamp of the data . You 'll observe difference in the time stamp format and other challenge such a incomplete data . However , you might be able to infer missing data in some case . For example , say you have some data that contains both the month and the day , but no year observe whether the data seems to sequence through the month number in the database repeating after 12 if it doe . So you could add the year . If you knew when the data started , you could then infer future year based on the order of the data . Much time stamp data is stored in U T C format but not all data is , you should check if the time stamp is in local or universal time . Sometimes the time stamp doe n't represent the time you think it doe . For example , suppose you have a database of car that were serviced at a garage . Does the time stamp indicate the time the car arrived wa completed or picked up ? Or doe it indicate when the final entry wa entered into the system ? Say you 're trying to model the hourly caloric intake of patient . However , you only have daily data then you 'll need to adjust your target time scale . Also , your data might not have any time stamp . There could be other way to extrapolate a time series depending on the data and domain . For example , you might have wavelength measurement or vector within an image . As a final note , remember that daylight saving is different around the world . Also because of daylight saving time might even occur twice a year in their time zone . A common occurrence in real world forecasting problem is missing value in the raw data . Missing value make it harder for a model to generate a forecast . The primary example in retail is an out of stock situation in demand forecasting . If an item go out of stock , the sale for the day will zero . If the forecast is generated based on those zero sale value , the forecast will be incorrect . There are many reason why value can be marked a missing , missing value can occur because of no transaction . They can also occur because of possible measurement error . For example , a service that monitored certain data wa n't working correctly . Or a another example , the measurement could n't happen correctly in retail . The primary example for an inability to take correct measurement is an out of stock situation in demand forecasting . This mean that demand doe n't equal sale on that day . There are several way you can calculate the missing data . The first method is forward fill . This us the last known value for the missing value . Building on that idea , moving average us the average of the last known value to calculate the missing value backward fill us the next known value after the missing value . The danger here is that you 're using the future to calculate the past , which is bad in forecasting . This method is also known a look ahead and should be avoided . Interpolation us an equation to calculate the missing value . You can also use a zero fill . This is often used in retail because missing sale data should n't be calculated . The missing data represents that there were no order on that day . It would be wise to investigate why this happened . But in this case , you do n't want to fill in the missing value . You might get data at different frequency . For example , you might have sale data that includes the exact time stamp , the sale wa recorded but have inventory data that only contains the year , month and day of the inventory level . When you have data that 's at a different frequency than other data set or data that 's not compatible with your question . You might need to down sample . Down sampling is moving from a more finely grained time to a le finely grained time . As the example show this could be converting an hourly data set to a daily data set . When down sampling , you need to decide how to combine the value . In the previous case of sale data . Summing the quantity make the most sense . If the data is temperature . You might want to find the average understanding your data help you decide what 's the best course of action . The opposite of down sampling is up sampling . When you move from a le finely grained time to a more finely grained time . The problem with up sampling is that it 's extremely difficult to achieve . In most case , suppose you want to up sample your sale data from daily sale to hourly sale . Unless you have some other data source to reference , you would n't be able to do this . There are case when you need to do something perhaps to match the frequency of another time series or you might have an irregular time series or specific domain knowledge that would help in those case , you need to be careful how you make the conversion for the retail example . The best you could do is create a single order for the day at a specified hour . For temperature . You could copy the daily temperature into each hourly slot or use a formula to calculate a curve in data science . Outliers have a mix of positive and negative attribute . The same is true of time series data . Suppose you were examining sale data and you had an order that ha an unusually high number of item . You might not want to include that in your forecast calculation because the order size might never be repeated , removing these outlier and anomaly is known a smoothing , smoothing your data can help you deal with outlier and other anomaly . There are a few reason why you might consider smoothing first during data preparation , you remove error value and could also remove outlier . You might also want to smooth your data to generate feature . For visualization . You could smooth your data to reduce the noise in a plot . It 's important to understand why you are smoothing the data and the impact that it might have . The outcome might be to reduce noise and create a better model . But an equally important question is , could your smoothing compromise the model is the model expecting noisy data ? Will you also be able to smooth the data in production ? That 's it . For part one of this section , we 'll see you again for part two where we 'll review more time series specific challenge and the tool and algorithm that can help u wrangle your data .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'wrangling', 'time', 'series', 'data', 'seasonality', 'in', 'data', 'is', 'any', 'kind', 'of', 'repeating', 'observation', 'where', 'the', 'frequency', 'of', 'the', 'observation', 'is', 'stable', '.', 'For', 'example', ',', 'in', 'sales', ',', 'you', 'typically', 'see', 'higher', 'sales', 'at', 'the', 'end', 'of', 'a', 'quarter', 'and', 'into', 'the', 'fourth', 'quarter', ',', 'consumer', 'retail', 'sees', 'even', 'higher', 'sales', 'in', 'the', 'fourth', 'quarter', '.', 'Be', 'aware', 'that', 'data', 'can', 'have', 'multiple', 'types', 'of', 'seasonality', 'in', 'the', 'same', 'data', 'set', '.', 'There', 'are', 'many', 'times', 'when', 'you', 'should', 'incorporate', 'seasonality', 'information', 'into', 'your', 'forecast', '.', 'For', 'instance', ',', 'localized', 'holidays', 'are', 'a', 'good', 'example', 'for', 'sales', '.', 'The', 'chart', 'shows', 'that', 'the', 'total', 'revenue', 'generated', 'by', 'arcades', 'has', 'a', 'strong', 'correlation', 'with', 'the', 'number', 'of', 'computer', 'science', 'doctorates', 'awarded', 'in', 'the', 'US', '.', 'But', 'correlations', 'do', 'not', 'mean', 'causation', '.', 'If', 'you', 'disagree', ',', 'see', 'the', 'source', 'for', 'the', 'chart', ',', 'there', 'are', 'many', 'other', 'correlations', 'plotted', 'on', 'the', 'site', 'and', 'none', 'of', 'them', 'make', 'any', 'sense', 'with', 'your', 'own', 'data', '.', 'Be', 'careful', 'that', 'you', \"'re\", 'not', 'seeing', 'and', 'acting', 'on', 'correlations', 'that', 'do', \"n't\", 'have', 'meaning', 'in', 'the', 'real', 'world', '.', 'Here', \"'s\", 'an', 'experiment', '.', 'If', 'you', 'generate', 'two', 'random', 'time', 'series', 'data', 'sets', 'of', 'numbers', 'between', 'zero', 'and', 'one', ',', 'you', \"'ll\", 'find', 'that', 'they', 'have', 'a', 'very', 'low', 'correlation', 'But', 'if', 'you', 'introduce', 'the', 'same', 'slope', 'to', 'both', 'datasets', ',', 'you', \"'ll\", 'see', 'a', 'very', 'strong', 'correlation', '.', 'You', 'need', 'to', 'know', 'how', 'stable', 'a', 'system', 'is', '.', 'The', 'level', 'of', 'stability', 'or', 'stationery', 'can', 'inform', 'how', 'much', 'you', 'should', 'expect', 'the', 'system', \"'s\", 'past', 'behavior', '.', 'To', 'inform', 'future', 'behavior', '.', 'A', 'system', 'with', 'low', 'stability', 'wo', \"n't\", 'be', 'successful', 'at', 'predicting', 'the', 'future', '.', 'You', \"'ll\", 'often', 'want', 'to', 'determine', 'the', 'trend', 'for', 'a', 'time', 'series', '.', 'But', 'if', 'you', 'adjust', 'the', 'series', 'for', 'the', 'trend', ',', 'it', 'can', 'be', 'difficult', 'to', 'compare', 'it', 'with', 'another', 'series', 'that', 'was', 'also', 'adjusted', 'for', 'the', 'trend', '.', 'This', 'is', 'because', 'the', 'trends', 'might', 'dominate', 'the', 'values', 'in', 'the', 'series', '.', 'This', 'could', 'then', 'lead', 'to', 'overestimates', 'and', 'correlation', 'between', 'the', 'two', 'series', 'like', 'we', 'discussed', 'previously', 'auto', 'correlation', 'is', 'one', 'of', 'the', 'special', 'problems', 'you', 'face', 'with', 'time', 'series', 'data', 'as', 'you', \"'ve\", 'seen', 'in', 'other', 'machine', 'learning', 'problems', '.', 'The', 'goal', 'of', 'building', 'an', 'ML', 'model', 'is', 'to', 'make', 'sure', 'you', \"'re\", 'separating', 'the', 'signal', 'from', 'the', 'noise', '.', 'Auto', 'correlation', 'is', 'a', 'form', 'of', 'noise', 'because', 'separate', 'observations', 'are', \"n't\", 'independent', 'of', 'each', 'other', '.', 'A', 'time', 'series', 'with', 'auto', 'correlation', 'might', 'overstate', 'the', 'accuracy', 'of', 'the', 'model', 'that', \"'s\", 'produced', 'some', 'of', 'the', 'algorithms', 'you', \"'ll\", 'look', 'at', 'in', 'this', 'module', 'can', 'help', 'correct', 'for', 'auto', 'correlation', '.', 'These', 'factors', 'along', 'with', 'seasonality', 'will', 'influence', 'the', 'model', '.', 'You', \"'ll\", 'select', 'to', 'produce', 'your', 'forecast', '.', 'Some', 'algorithms', 'handle', 'seasonality', 'and', 'auto', 'correlation', 'but', 'others', 'do', 'not', '.', 'Penda', 'was', 'developed', 'with', 'financial', 'data', 'analysis', 'in', 'mind', 'as', 'such', ',', 'it', \"'s\", 'good', 'at', 'handling', 'time', 'series', 'data', '.', 'First', ',', 'you', 'can', 'set', 'the', 'index', 'for', 'your', 'penda', 'data', 'frame', 'to', 'be', 'a', 'date', 'time', '.', 'You', 'can', 'then', 'use', 'date', 'and', 'time', 'to', 'select', 'your', 'data', '.', 'You', 'can', 'use', 'ranges', 'that', 'contain', 'partial', 'dates', '.', 'You', 'can', 'also', 'extract', 'date', 'parts', 'such', 'as', 'year', ',', 'month', ',', 'weekday', 'name', 'and', 'more', 'for', 'grouping', 'and', 're', 'sampling', 'tasks', '.', 'Pentas', 'has', 'built', 'in', 'functions', 'to', 'do', 'both', '.', 'Finally', ',', 'pandas', 'can', 'give', 'you', 'insights', 'into', 'auto', 'correlation', 'for', 'more', 'information', 'about', 'pandas', 'in', 'the', 'time', 'series', '.', 'Refer', 'to', 'the', 'pandas', 'documentation', '.', 'One', 'of', 'the', 'tasks', 'in', 'building', 'a', 'forecasting', 'application', 'is', 'to', 'choose', 'an', 'appropriate', 'algorithm', '.', 'Your', 'choice', 'of', 'algorithm', 'should', 'be', 'determined', 'by', 'the', 'type', 'of', 'data', 'set', 'you', \"'re\", 'using', '.', 'And', 'the', 'features', 'of', 'that', 'data', 'set', '.', 'Amazon', 'forecast', 'supports', 'these', 'five', 'algorithms', 'but', 'there', 'are', 'others', '.', 'Each', 'algorithm', 'can', 'handle', 'data', 'with', 'slightly', 'different', 'characteristics', '.', 'For', 'example', ',', 'take', 'autoregressive', 'integrated', 'moving', 'average', ',', 'which', 'is', 'also', 'known', 'as', 'Arima', '.', 'It', 'removes', 'auto', 'correlations', 'that', 'could', 'influence', 'the', 'pattern', 'of', 'observations', 'or', 'take', 'exponential', 'smoothing', ',', 'which', 'is', 'also', 'known', 'as', 'ETS', '.', 'This', 'algorithm', 'is', 'useful', 'for', 'data', 'sets', 'with', 'seasonality', '.', 'You', 'can', 'find', 'out', 'more', 'about', 'these', 'algorithms', 'in', 'the', 'Amazon', 'forecast', 'documentation', '.', 'Some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'include', 'time', 'series', 'data', 'is', 'sequence', 'data', 'that', 'includes', 'a', 'time', 'element', 'which', 'makes', 'it', 'different', 'from', 'regular', 'data', 'sets', '.', 'Some', 'of', 'the', 'time', 'challenges', 'include', 'dealing', 'with', 'different', 'time', 'formats', ',', 'handling', 'missing', 'data', 'through', 'down', 'sampling', 'up', 'sampling', 'and', 'smoothing', ',', 'dealing', 'with', 'seasonality', 'such', 'as', 'weekdays', 'and', 'yearly', 'cycles', ',', 'avoiding', 'bad', 'correlations', '.', 'PEDIS', 'has', 'excellent', 'time', 'series', 'support', 'with', 'functions', 'for', 'dealing', 'with', 'time', '.', 'There', 'are', 'five', 'algorithms', 'used', 'by', 'Amazon', 'forecast', 'arema', 'deep', 'A', 'R', 'plus', 'ETS', 'N', 'PTS', 'and', 'profit', '.', 'That', \"'s\", 'it', 'for', 'this', 'section', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . We 'll continue exploring wrangling time series data seasonality in data is any kind of repeating observation where the frequency of the observation is stable . For example , in sale , you typically see higher sale at the end of a quarter and into the fourth quarter , consumer retail see even higher sale in the fourth quarter . Be aware that data can have multiple type of seasonality in the same data set . There are many time when you should incorporate seasonality information into your forecast . For instance , localized holiday are a good example for sale . The chart show that the total revenue generated by arcade ha a strong correlation with the number of computer science doctorate awarded in the US . But correlation do not mean causation . If you disagree , see the source for the chart , there are many other correlation plotted on the site and none of them make any sense with your own data . Be careful that you 're not seeing and acting on correlation that do n't have meaning in the real world . Here 's an experiment . If you generate two random time series data set of number between zero and one , you 'll find that they have a very low correlation But if you introduce the same slope to both datasets , you 'll see a very strong correlation . You need to know how stable a system is . The level of stability or stationery can inform how much you should expect the system 's past behavior . To inform future behavior . A system with low stability wo n't be successful at predicting the future . You 'll often want to determine the trend for a time series . But if you adjust the series for the trend , it can be difficult to compare it with another series that wa also adjusted for the trend . This is because the trend might dominate the value in the series . This could then lead to overestimate and correlation between the two series like we discussed previously auto correlation is one of the special problem you face with time series data a you 've seen in other machine learning problem . The goal of building an ML model is to make sure you 're separating the signal from the noise . Auto correlation is a form of noise because separate observation are n't independent of each other . A time series with auto correlation might overstate the accuracy of the model that 's produced some of the algorithm you 'll look at in this module can help correct for auto correlation . These factor along with seasonality will influence the model . You 'll select to produce your forecast . Some algorithm handle seasonality and auto correlation but others do not . Penda wa developed with financial data analysis in mind a such , it 's good at handling time series data . First , you can set the index for your penda data frame to be a date time . You can then use date and time to select your data . You can use range that contain partial date . You can also extract date part such a year , month , weekday name and more for grouping and re sampling task . Pentas ha built in function to do both . Finally , panda can give you insight into auto correlation for more information about panda in the time series . Refer to the panda documentation . One of the task in building a forecasting application is to choose an appropriate algorithm . Your choice of algorithm should be determined by the type of data set you 're using . And the feature of that data set . Amazon forecast support these five algorithm but there are others . Each algorithm can handle data with slightly different characteristic . For example , take autoregressive integrated moving average , which is also known a Arima . It remove auto correlation that could influence the pattern of observation or take exponential smoothing , which is also known a ETS . This algorithm is useful for data set with seasonality . You can find out more about these algorithm in the Amazon forecast documentation . Some key takeaway from this section of the module include time series data is sequence data that includes a time element which make it different from regular data set . Some of the time challenge include dealing with different time format , handling missing data through down sampling up sampling and smoothing , dealing with seasonality such a weekday and yearly cycle , avoiding bad correlation . PEDIS ha excellent time series support with function for dealing with time . There are five algorithm used by Amazon forecast arema deep A R plus ETS N PTS and profit . That 's it for this section . We 'll see you in the next video .\n",
      "['Hi', 'and', 'welcome', 'back', 'in', 'this', 'section', '.', 'We', \"'ll\", 'look', 'at', 'how', 'you', 'can', 'use', 'Amazon', 'forecast', 'to', 'create', 'a', 'predictor', 'and', 'generate', 'forecasts', '.', 'When', 'you', 'generate', 'forecasts', ',', 'you', 'can', 'apply', 'the', 'machine', 'learning', 'development', 'pipeline', 'you', \"'ve\", 'seen', 'throughout', 'this', 'course', ',', 'but', 'you', 'still', 'need', 'data', '.', 'You', 'need', 'to', 'import', 'as', 'much', 'data', 'as', 'you', 'have', 'both', 'historical', 'data', 'and', 'related', 'data', '.', 'You', \"'ll\", 'want', 'to', 'do', 'some', 'basic', 'evaluation', 'and', 'feature', 'engineering', 'before', 'you', 'use', 'the', 'data', 'to', 'train', 'a', 'model', '.', 'So', 'you', 'can', 'meet', 'the', 'requirements', 'of', 'Amazon', 'forecast', 'to', 'train', 'a', 'predictor', '.', 'You', 'need', 'to', 'choose', 'an', 'algorithm', 'if', 'you', \"'re\", 'not', 'sure', 'which', 'algorithm', 'is', 'the', 'best', 'for', 'your', 'data', '.', 'Amazon', 'forecast', 'can', 'choose', 'for', 'you', 'to', 'do', 'this', 'select', 'auto', 'M', 'L', 'as', 'your', 'algorithm', '.', 'You', 'also', 'need', 'to', 'select', 'a', 'domain', 'for', 'your', 'data', '.', 'If', 'you', \"'re\", 'not', 'sure', 'what', 'the', 'best', 'fit', 'is', ',', 'you', 'can', 'also', 'select', 'a', 'custom', 'domain', 'domains', 'have', 'specific', 'types', 'of', 'data', 'they', 'require', '.', 'When', 'you', 'have', 'a', 'trained', 'model', ',', 'you', 'can', 'then', 'use', 'the', 'model', 'to', 'make', 'a', 'forecast', ',', 'using', 'an', 'input', 'data', 'set', 'group', '.', 'After', 'you', \"'ve\", 'generated', 'a', 'forecast', ',', 'you', 'can', 'query', 'the', 'forecast', '.', 'You', 'can', 'also', 'export', 'it', 'to', 'a', 'bucket', 'in', 'Amazon', 'S3', '.', 'And', 'finally', ',', 'you', 'can', 'encrypt', 'the', 'data', 'in', 'the', 'forecast', 'before', 'exporting', 'it', '.', 'The', 'overall', 'process', 'for', 'working', 'with', 'Amazon', 'forecast', 'is', 'to', 'import', 'historical', 'and', 'related', 'data', '.', 'Amazon', 'forecast', 'inspects', 'the', 'data', 'identifies', 'key', 'data', 'and', 'selects', 'an', 'appropriate', 'algorithm', '.', 'It', 'uses', 'the', 'algorithm', 'to', 'train', 'and', 'optimize', 'a', 'custom', 'model', 'and', 'produce', 'a', 'predictor', '.', 'You', 'create', 'forecasts', 'by', 'applying', 'the', 'predictor', 'to', 'your', 'data', 'set', '.', 'You', 'can', 'then', 'retrieve', 'these', 'forecasts', 'in', 'the', 'Aws', 'management', 'console', 'or', 'you', 'can', 'export', 'the', 'forecasts', 'as', 'comma', 'delimited', 'files', '.', 'You', 'can', 'also', 'use', 'an', 'API', 'and', 'AWS', 'C', 'commands', 'to', 'create', 'and', 'retrieve', 'forecasts', '.', 'When', 'you', 'work', 'with', 'Amazon', 'forecast', ',', 'you', 'can', 'select', 'the', 'domain', 'you', \"'re\", 'working', 'in', '.', 'There', 'are', 'domains', 'ranging', 'from', 'retail', 'to', 'web', 'traffic', 'and', 'there', \"'s\", 'also', 'a', 'custom', 'option', 'for', 'everything', 'else', '.', 'By', 'selecting', 'a', 'domain', ',', 'you', 'improve', 'the', 'efficiency', 'of', 'the', 'predictor', '.', 'Each', 'domain', 'has', 'specific', 'types', 'of', 'data', 'that', 'you', \"'ll\", 'supply', '.', 'When', 'you', 'build', 'the', 'predictor', '.', 'For', 'example', ',', 'the', 'retail', 'domain', 'expects', 'data', 'for', 'the', 'item', 'identifiers', '.', 'A', 'time', 'stamp', 'for', 'the', 'observation', ',', 'the', 'number', 'of', 'sales', 'for', 'that', 'item', 'and', 'the', 'specified', 'time', 'stamp', '.', 'Here', \"'s\", 'an', 'example', 'of', 'the', 'data', 'you', \"'d\", 'need', 'to', 'provide', 'for', 'a', 'retail', 'demand', 'forecast', 'for', 'the', 'time', 'series', '.', 'You', 'need', 'the', 'time', 'when', 'the', 'transaction', 'took', 'place', '.', 'Ideally', 'in', 'U', 'T', 'C', 'format', ',', 'the', 'item', 'id', 'of', 'the', 'item', 'and', 'how', 'many', 'items', 'were', 'sold', '.', 'The', 'metadata', 'for', 'the', 'item', 'might', 'include', 'the', 'category', ',', 'the', 'item', 'color', 'and', 'other', 'attributes', '.', 'The', 'link', 'back', 'to', 'the', 'time', 'series', 'data', 'will', 'be', 'only', 'the', 'item', 'id', 'because', 'item', 'metadata', 'typically', 'does', \"n't\", 'change', 'related', 'data', 'for', 'creating', 'a', 'more', 'useful', 'forecast', 'could', 'include', 'the', 'sales', 'price', 'or', 'other', 'promotion', 'data', '.', 'To', 'link', 'this', 'back', 'to', 'the', 'item', '.', 'You', 'must', 'include', 'the', 'time', 'stamp', 'and', 'the', 'item', 'id', '.', 'Here', \"'s\", 'an', 'example', 'of', 'the', 'data', 'you', \"'d\", 'need', 'to', 'provide', 'for', 'a', 'web', 'traffic', 'forecast', 'for', 'the', 'time', 'series', '.', 'You', 'need', 'the', 'web', 'page', 'id', ',', 'the', 'number', 'of', 'page', 'views', 'per', 'month', 'and', 'the', 'time', 'stamp', 'related', 'data', 'for', 'creating', 'a', 'more', 'useful', 'forecast', 'could', 'include', 'the', 'page', 'category', 'such', 'as', 'navigation', 'or', 'content', 'category', '.', 'You', \"'ll\", 'also', 'need', 'the', 'geographic', 'identifier', 'for', 'the', 'web', 'client', 'for', 'metadata', '.', 'You', 'might', 'also', 'need', 'to', 'provide', 'the', 'region', 'and', 'the', 'sales', 'promotion', 'information', '.', 'Amazon', 'forecast', 'predictors', 'use', 'an', 'algorithm', 'to', 'train', 'a', 'model', '.', 'They', 'then', 'use', 'the', 'model', 'to', 'make', 'a', 'forecast', 'using', 'an', 'input', 'data', 'set', 'group', 'to', 'help', 'you', 'get', 'started', '.', 'Amazon', 'forecast', 'provides', 'predefined', 'algorithms', 'arema', 'deep', 'A', 'R', 'plus', 'E', 'T', 'SNP', 'T', 'S', 'and', 'profit', '.', 'You', 'can', 'also', 'use', 'the', 'auto', 'ML', 'feature', '.', 'It', 'will', 'try', 'all', 'the', 'algorithms', 'to', 'see', 'which', 'ones', 'at', 'the', 'best', 'at', 'predicting', 'data', '.', 'When', 'you', 'prepare', 'data', 'for', 'training', 'and', 'machine', 'learning', ',', 'you', 'typically', 'hold', 'back', 'data', 'to', 'use', 'when', 'you', 'validate', 'and', 'score', 'the', 'model', '.', 'The', 'data', 'that', 'you', 'hold', 'back', 'is', 'usually', 'a', 'random', 'sample', 'of', 'your', 'available', 'data', 'with', 'time', 'series', 'data', '.', 'You', 'must', 'process', 'your', 'data', 'differently', 'because', 'of', 'a', 'correlation', 'between', 'time', 'when', 'you', 'import', 'your', 'data', '.', 'Amazon', 'forecast', 'breaks', 'it', 'into', 'training', 'and', 'test', 'data', 'sets', 'which', 'the', 'diagram', 'shows', 'the', 'training', 'data', 'is', 'used', 'to', 'train', 'the', 'model', 'which', 'is', 'then', 'tested', 'against', 'the', 'data', 'that', 'was', 'held', 'back', '.', 'You', 'can', 'specify', 'multiple', 'back', 'test', 'windows', 'which', 'will', 'split', 'the', 'data', 'multiple', 'times', ',', 'train', 'the', 'model', 'and', 'use', 'metrics', 'to', 'determine', 'which', 'model', 'gives', 'the', 'best', 'results', '.', 'The', 'default', 'back', 'test', 'window', 'is', 'one', 'you', 'can', 'change', 'how', 'Amazon', 'forecasts', 'splits', 'the', 'data', 'by', 'setting', 'the', 'back', 'test', 'window', 'offset', 'parameter', '.', 'When', 'you', 'create', 'the', 'predictor', ',', 'if', 'you', 'do', \"n't\", 'set', 'this', 'value', ',', 'the', 'algorithms', 'use', 'default', 'values', 'after', 'you', \"'ve\", 'trained', 'a', 'model', ',', 'you', 'will', 'need', 'to', 'measure', 'its', 'accuracy', 'which', 'you', \"'ll\", 'learn', 'about', 'next', '.', 'The', 'first', 'Amazon', 'forecast', 'evaluation', 'metric', 'is', 'the', 'weighted', 'quantile', 'loss', 'or', 'w', 'quantile', 'loss', '.', 'When', 'Amazon', 'forecast', 'creates', 'a', 'forecast', ',', 'it', 'provides', 'probabilistic', 'predictions', 'at', 'three', 'distinct', 'quantiles', ',', '10', '%', '50', '%', 'and', '90', '%', '.', 'These', 'prediction', 'quantiles', 'show', 'you', 'how', 'much', 'uncertainty', 'is', 'associated', 'with', 'each', 'forecast', '.', 'A', 'P', '10', 'quantile', 'predicts', 'that', '10', '%', 'of', 'the', 'time', 'the', 'true', 'value', 'will', 'be', 'less', 'than', 'the', 'predicted', 'value', '.', 'For', 'example', ',', 'suppose', 'that', 'you', 'are', 'a', 'retailer', ',', 'you', 'want', 'to', 'forecast', 'product', 'demand', 'for', 'winter', 'gloves', 'that', 'sell', 'well', 'only', 'during', 'the', 'fall', 'and', 'winter', 'say', 'that', 'you', 'do', \"n't\", 'have', 'sufficient', 'storage', 'space', 'and', 'the', 'cost', 'of', 'invested', 'capital', 'is', 'high', 'or', 'that', 'the', 'price', 'of', 'being', 'overstocked', 'on', 'winter', 'gloves', 'concerns', '.', 'You', ',', 'then', 'you', 'might', 'use', 'the', 'P', '10', 'quantile', 'to', 'order', 'a', 'relatively', 'low', 'number', 'of', 'winter', 'gloves', '.', 'You', 'know', 'that', 'the', 'P', '10', 'forecast', 'overestimates', 'the', 'demand', 'for', 'your', 'winter', 'gloves', 'only', '10', '%', 'of', 'the', 'time', '.', 'So', 'you', \"'ll\", 'be', 'sold', 'out', 'of', 'your', 'winter', 'gloves', 'for', '90', '%', 'of', 'the', 'time', '.', 'A', 'P', '50', 'quantile', 'predicts', 'that', '50', '%', 'of', 'the', 'time', 'the', 'true', 'value', 'will', 'be', 'less', 'than', 'the', 'predicted', 'value', '.', 'Continuing', 'the', 'winter', 'gloves', 'example', ',', 'say', ',', 'you', 'know', 'that', 'there', 'will', 'be', 'a', 'moderate', 'amount', 'of', 'demand', 'for', 'the', 'gloves', 'and', 'you', 'are', \"n't\", 'concerned', 'about', 'being', 'overstocked', '.', 'Then', 'you', 'might', 'choose', 'to', 'use', 'the', 'P', '50', 'quantile', 'to', 'order', 'gloves', '.', 'A', 'P', '90', 'quantile', 'predicts', 'that', '90', '%', 'of', 'the', 'time', 'the', 'true', 'value', 'will', 'be', 'less', 'than', 'the', 'predicted', 'value', '.', 'Suppose', 'you', 'determine', 'that', 'being', 'under', 'stocked', 'on', 'gloves', 'will', 'result', 'in', 'large', 'amounts', 'of', 'lost', 'revenue', '.', 'For', 'example', ',', 'the', 'cost', 'of', 'not', 'selling', 'gloves', 'is', 'extremely', 'high', 'or', 'the', 'cost', 'of', 'invested', 'capital', 'is', 'low', '.', 'In', 'this', 'case', ',', 'you', 'might', 'choose', 'to', 'use', 'the', 'P', '90', 'quantile', 'to', 'order', 'gloves', '.', 'Amazon', 'forecast', 'also', 'calculates', 'the', 'associated', 'loss', 'or', 'error', 'at', 'each', 'quantile', 'weighted', 'quantile', 'loss', '.', 'Calculates', 'how', 'far', 'off', 'the', 'forecast', '.', 'A', 'certain', 'quantile', 'is', 'from', 'actual', 'demand', 'in', 'either', 'direction', '.', 'Lower', 'W', 'quantile', 'loss', 'metrics', 'mean', 'that', 'the', 'model', \"'s\", 'forecasts', 'are', 'more', 'reliable', '.', 'The', 'root', 'mean', 'square', 'error', 'or', 'R', 'MS', 'E', 'is', 'another', 'method', 'for', 'evaluating', 'the', 'reliability', 'of', 'your', 'forecasts', 'like', 'W', 'quantile', 'loss', 'RMS', '.', 'E', 'calculates', 'how', 'far', 'off', 'the', 'forecasted', 'values', 'were', 'from', 'the', 'actual', 'test', 'data', '.', 'The', 'R', 'MS', 'E', 'finds', 'the', 'difference', 'between', 'the', 'actual', 'target', 'value', 'in', 'the', 'data', 'set', 'and', 'the', 'forecasted', 'value', 'for', 'that', 'time', 'period', '.', 'And', 'it', 'then', 'squares', 'the', 'differences', '.', 'The', 'example', 'shows', 'how', 'to', 'calculate', 'R', 'M', 'E.', 'The', 'R', 'MS', 'E', 'value', 'represents', 'the', 'standard', 'deviation', 'of', 'the', 'prediction', 'errors', '.', 'This', 'test', 'is', 'good', 'for', 'forecast', 'validity', '.', 'When', 'the', 'errors', 'are', 'mostly', 'of', 'the', 'same', 'size', 'that', 'is', 'there', ',', 'there', 'are', \"n't\", 'many', 'outliers', 'lower', 'R', 'MS', 'E', 'metrics', 'indicate', 'that', 'the', 'model', \"'s\", 'forecasts', 'are', 'more', 'reliable', '.', 'Here', \"'s\", 'an', 'example', 'of', 'how', 'a', 'web', 'retailer', 'might', 'use', 'the', 'accuracy', 'metrics', 'to', 'evaluate', 'a', 'forecast', '.', 'The', 'retailer', 'wants', 'to', 'predict', 'the', 'demand', 'for', 'sales', 'of', 'a', 'particular', 'brand', 'of', 'shoes', '.', 'They', 'input', 'the', 'sales', 'records', 'for', 'this', 'brand', 'into', 'Amazon', 'forecast', 'to', 'create', 'a', 'predictor', '.', 'The', 'predictor', 'provides', 'a', 'forecasted', 'demand', 'of', '1000', 'pairs', 'with', 'the', 'P', '10', 'P', '50', 'and', 'P', '90', 'values', 'shown', '.', 'The', 'weighted', 'quantile', 'loss', 'values', 'indicate', 'that', '10', '%', 'of', 'the', 'time', 'there', 'will', 'be', 'fewer', 'than', '880', 'pairs', 'sold', ',', '50', '%', 'of', 'the', 'time', ',', 'fewer', 'than', '1050', 'pairs', 'will', 'be', 'sold', 'and', '90', '%', 'of', 'the', 'time', 'fewer', 'than', '1200', 'pairs', 'will', 'be', 'sold', '.', 'The', 'retailer', 'can', 'then', 'use', 'these', 'values', 'to', 'determine', 'which', 'level', 'of', 'inventory', 'to', 'hold', '.', 'They', 'can', 'base', 'their', 'decision', 'on', 'their', 'assessment', 'of', 'the', 'risk', 'that', 'they', 'wo', \"n't\", 'be', 'able', 'to', 'fulfill', 'orders', 'or', 'that', 'they', \"'ll\", 'have', 'excess', 'inventory', '.', 'Some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'include', ',', 'you', 'can', 'use', 'Amazon', 'forecast', 'to', 'train', 'and', 'use', 'a', 'model', 'for', 'time', 'series', 'data', '.', 'There', 'are', 'specific', 'schemas', 'defined', 'for', 'domains', 'such', 'as', 'retail', 'and', 'EC2', 'capacity', 'planning', 'or', 'you', 'can', 'use', 'a', 'custom', 'schema', '.', 'You', 'need', 'to', 'supply', 'at', 'least', 'the', 'time', 'series', 'data', 'but', 'can', 'also', 'provide', 'metadata', 'and', 'related', 'data', 'to', 'add', 'more', 'information', 'to', 'the', 'model', '.', 'As', 'with', 'most', 'supervised', 'machine', 'learning', 'problems', ',', 'your', 'data', 'is', 'split', 'into', 'training', 'and', 'testing', 'data', 'but', 'takes', 'into', 'account', 'the', 'time', 'element', 'use', 'R', 'MS', 'E', 'and', 'W', 'quantile', 'loss', 'metrics', 'to', 'evaluate', 'the', 'efficiency', 'of', 'the', 'model', '.', 'That', \"'s\", 'it', 'for', 'this', 'video', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'one', '.']\n",
      "Hi and welcome back in this section . We 'll look at how you can use Amazon forecast to create a predictor and generate forecast . When you generate forecast , you can apply the machine learning development pipeline you 've seen throughout this course , but you still need data . You need to import a much data a you have both historical data and related data . You 'll want to do some basic evaluation and feature engineering before you use the data to train a model . So you can meet the requirement of Amazon forecast to train a predictor . You need to choose an algorithm if you 're not sure which algorithm is the best for your data . Amazon forecast can choose for you to do this select auto M L a your algorithm . You also need to select a domain for your data . If you 're not sure what the best fit is , you can also select a custom domain domain have specific type of data they require . When you have a trained model , you can then use the model to make a forecast , using an input data set group . After you 've generated a forecast , you can query the forecast . You can also export it to a bucket in Amazon S3 . And finally , you can encrypt the data in the forecast before exporting it . The overall process for working with Amazon forecast is to import historical and related data . Amazon forecast inspects the data identifies key data and selects an appropriate algorithm . It us the algorithm to train and optimize a custom model and produce a predictor . You create forecast by applying the predictor to your data set . You can then retrieve these forecast in the Aws management console or you can export the forecast a comma delimited file . You can also use an API and AWS C command to create and retrieve forecast . When you work with Amazon forecast , you can select the domain you 're working in . There are domain ranging from retail to web traffic and there 's also a custom option for everything else . By selecting a domain , you improve the efficiency of the predictor . Each domain ha specific type of data that you 'll supply . When you build the predictor . For example , the retail domain expects data for the item identifier . A time stamp for the observation , the number of sale for that item and the specified time stamp . Here 's an example of the data you 'd need to provide for a retail demand forecast for the time series . You need the time when the transaction took place . Ideally in U T C format , the item id of the item and how many item were sold . The metadata for the item might include the category , the item color and other attribute . The link back to the time series data will be only the item id because item metadata typically doe n't change related data for creating a more useful forecast could include the sale price or other promotion data . To link this back to the item . You must include the time stamp and the item id . Here 's an example of the data you 'd need to provide for a web traffic forecast for the time series . You need the web page id , the number of page view per month and the time stamp related data for creating a more useful forecast could include the page category such a navigation or content category . You 'll also need the geographic identifier for the web client for metadata . You might also need to provide the region and the sale promotion information . Amazon forecast predictor use an algorithm to train a model . They then use the model to make a forecast using an input data set group to help you get started . Amazon forecast provides predefined algorithm arema deep A R plus E T SNP T S and profit . You can also use the auto ML feature . It will try all the algorithm to see which one at the best at predicting data . When you prepare data for training and machine learning , you typically hold back data to use when you validate and score the model . The data that you hold back is usually a random sample of your available data with time series data . You must process your data differently because of a correlation between time when you import your data . Amazon forecast break it into training and test data set which the diagram show the training data is used to train the model which is then tested against the data that wa held back . You can specify multiple back test window which will split the data multiple time , train the model and use metric to determine which model give the best result . The default back test window is one you can change how Amazon forecast split the data by setting the back test window offset parameter . When you create the predictor , if you do n't set this value , the algorithm use default value after you 've trained a model , you will need to measure it accuracy which you 'll learn about next . The first Amazon forecast evaluation metric is the weighted quantile loss or w quantile loss . When Amazon forecast creates a forecast , it provides probabilistic prediction at three distinct quantiles , 10 % 50 % and 90 % . These prediction quantiles show you how much uncertainty is associated with each forecast . A P 10 quantile predicts that 10 % of the time the true value will be le than the predicted value . For example , suppose that you are a retailer , you want to forecast product demand for winter glove that sell well only during the fall and winter say that you do n't have sufficient storage space and the cost of invested capital is high or that the price of being overstocked on winter glove concern . You , then you might use the P 10 quantile to order a relatively low number of winter glove . You know that the P 10 forecast overestimate the demand for your winter glove only 10 % of the time . So you 'll be sold out of your winter glove for 90 % of the time . A P 50 quantile predicts that 50 % of the time the true value will be le than the predicted value . Continuing the winter glove example , say , you know that there will be a moderate amount of demand for the glove and you are n't concerned about being overstocked . Then you might choose to use the P 50 quantile to order glove . A P 90 quantile predicts that 90 % of the time the true value will be le than the predicted value . Suppose you determine that being under stocked on glove will result in large amount of lost revenue . For example , the cost of not selling glove is extremely high or the cost of invested capital is low . In this case , you might choose to use the P 90 quantile to order glove . Amazon forecast also calculates the associated loss or error at each quantile weighted quantile loss . Calculates how far off the forecast . A certain quantile is from actual demand in either direction . Lower W quantile loss metric mean that the model 's forecast are more reliable . The root mean square error or R MS E is another method for evaluating the reliability of your forecast like W quantile loss RMS . E calculates how far off the forecasted value were from the actual test data . The R MS E find the difference between the actual target value in the data set and the forecasted value for that time period . And it then square the difference . The example show how to calculate R M E. The R MS E value represents the standard deviation of the prediction error . This test is good for forecast validity . When the error are mostly of the same size that is there , there are n't many outlier lower R MS E metric indicate that the model 's forecast are more reliable . Here 's an example of how a web retailer might use the accuracy metric to evaluate a forecast . The retailer want to predict the demand for sale of a particular brand of shoe . They input the sale record for this brand into Amazon forecast to create a predictor . The predictor provides a forecasted demand of 1000 pair with the P 10 P 50 and P 90 value shown . The weighted quantile loss value indicate that 10 % of the time there will be fewer than 880 pair sold , 50 % of the time , fewer than 1050 pair will be sold and 90 % of the time fewer than 1200 pair will be sold . The retailer can then use these value to determine which level of inventory to hold . They can base their decision on their assessment of the risk that they wo n't be able to fulfill order or that they 'll have excess inventory . Some key takeaway from this section of the module include , you can use Amazon forecast to train and use a model for time series data . There are specific schema defined for domain such a retail and EC2 capacity planning or you can use a custom schema . You need to supply at least the time series data but can also provide metadata and related data to add more information to the model . As with most supervised machine learning problem , your data is split into training and testing data but take into account the time element use R MS E and W quantile loss metric to evaluate the efficiency of the model . That 's it for this video . We 'll see you in the next one .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'It', \"'s\", 'now', 'time', 'to', 'review', 'the', 'module', 'and', 'wrap', 'it', 'up', 'in', 'this', 'module', '.', 'You', 'learned', 'how', 'to', 'describe', 'the', 'business', 'problem', 'solved', 'by', 'Amazon', 'forecast', '.', 'Describe', 'the', 'challenges', 'of', 'working', 'with', 'time', 'series', 'data', 'list', '.', 'The', 'steps', 'required', 'to', 'create', 'forecast', 'by', 'using', 'Amazon', 'forecast', 'and', 'use', 'Amazon', 'forecast', 'to', 'make', 'a', 'prediction', '.', 'Thanks', 'for', 'participating', '.', 'See', 'you', 'in', 'the', 'next', 'module', '.']\n",
      "Hi , welcome back . It 's now time to review the module and wrap it up in this module . You learned how to describe the business problem solved by Amazon forecast . Describe the challenge of working with time series data list . The step required to create forecast by using Amazon forecast and use Amazon forecast to make a prediction . Thanks for participating . See you in the next module .\n",
      "['Welcome', 'back', 'to', 'Aws', 'Academy', 'Machine', 'Learning', '.', 'This', 'is', 'module', 'five', 'and', 'we', 'have', 'a', 'great', 'topic', 'for', 'you', 'today', '.', 'Computer', 'vision', 'in', 'this', 'module', ',', 'we', \"'ll\", 'start', 'with', 'an', 'overview', 'of', 'the', 'computer', 'vision', 'space', 'and', 'you', \"'ll\", 'learn', 'about', 'some', 'of', 'the', 'use', 'cases', 'and', 'terminology', '.', 'Next', ',', 'we', \"'ll\", 'explore', 'details', 'about', 'analyzing', 'image', 'and', 'video', 'with', 'managed', 'services', 'from', 'Amazon', 'web', 'services', 'or', 'Aws', '.', 'Finally', ',', 'we', \"'ll\", 'look', 'at', 'how', 'you', 'can', 'use', 'your', 'own', 'customized', 'data', 'sets', 'for', 'performing', 'object', 'detection', '.', 'At', 'the', 'end', 'of', 'this', 'module', ',', 'you', \"'ll\", 'be', 'able', 'to', 'describe', 'the', 'use', 'cases', 'for', 'computer', 'vision', '.', 'Describe', 'the', 'Amazon', 'managed', 'machine', 'learning', 'services', 'available', 'for', 'image', 'and', 'video', 'analysis', 'list', 'the', 'steps', 'required', 'to', 'prepare', 'a', 'custom', 'data', 'set', 'for', 'object', 'detection', '.', 'Describe', 'how', 'Amazon', 'sagemaker', 'ground', 'truth', 'can', 'be', 'used', 'to', 'prepare', 'a', 'custom', 'data', 'set', 'and', 'finally', 'use', 'Amazon', 'recognition', 'to', 'perform', 'facial', 'detection', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Welcome back to Aws Academy Machine Learning . This is module five and we have a great topic for you today . Computer vision in this module , we 'll start with an overview of the computer vision space and you 'll learn about some of the use case and terminology . Next , we 'll explore detail about analyzing image and video with managed service from Amazon web service or Aws . Finally , we 'll look at how you can use your own customized data set for performing object detection . At the end of this module , you 'll be able to describe the use case for computer vision . Describe the Amazon managed machine learning service available for image and video analysis list the step required to prepare a custom data set for object detection . Describe how Amazon sagemaker ground truth can be used to prepare a custom data set and finally use Amazon recognition to perform facial detection . Thanks for watching . We 'll see you in the next video .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'This', 'is', 'Section', 'one', 'and', 'we', \"'re\", 'going', 'to', 'introduce', 'computer', 'vision', '.', 'Computer', 'vision', 'is', 'an', 'exciting', 'space', 'in', 'machine', 'learning', '.', 'You', 'can', 'think', 'of', 'computer', 'vision', 'as', 'the', 'automated', 'extraction', 'of', 'information', 'from', 'digital', 'images', '.', 'Using', 'computer', 'vision', 'machines', 'can', 'identify', 'people', 'places', 'and', 'things', 'in', 'images', 'with', 'an', 'accuracy', 'that', \"'s\", 'at', 'or', 'above', 'human', 'levels', 'and', 'with', 'greater', 'speed', 'and', 'efficiency', '.', 'Computer', 'vision', 'is', 'often', 'built', 'with', 'deep', 'learning', 'models', '.', 'It', 'automates', 'the', 'extraction', 'analysis', ',', 'classification', 'and', 'understanding', 'of', 'useful', 'information', 'from', 'a', 'single', 'image', 'or', 'a', 'sequence', 'of', 'images', '.', 'The', 'image', 'data', 'can', 'take', 'many', 'forms', 'such', 'as', 'single', 'images', ',', 'video', 'sequences', ',', 'views', 'from', 'multiple', 'cameras', 'or', 'three', 'dimensional', 'data', '.', 'Computing', 'power', 'and', 'algorithms', 'have', 'advanced', 'over', 'the', 'last', '10', 'years', '.', 'This', 'has', 'led', 'to', 'an', 'increase', 'in', 'capabilities', 'and', 'easier', 'access', 'to', 'computer', 'vision', 'technologies', '.', 'So', 'how', 'is', 'computer', 'vision', 'being', 'used', '?', 'Here', 'are', 'some', 'of', 'the', 'primary', 'use', 'cases', 'for', 'computer', 'vision', '.', 'You', 'can', 'use', 'image', 'and', 'facial', 'recognition', 'to', 'improve', 'public', 'safety', 'and', 'home', 'security', 'or', 'as', 'a', 'way', 'to', 'authenticate', 'access', 'to', 'personal', 'devices', ',', 'you', 'can', 'also', 'use', 'it', 'to', 'automatically', 'classify', 'images', 'for', 'content', 'management', 'and', 'analysis', '.', 'Autonomous', 'driving', 'is', 'partly', 'enabled', 'by', 'computer', 'vision', 'technologies', '.', 'And', 'so', 'are', 'safety', 'features', 'of', 'cars', 'such', 'as', 'lane', 'detection', 'or', 'collision', 'avoidance', '.', 'Medical', 'image', 'analysis', 'with', 'computer', 'vision', 'can', 'improve', 'the', 'accuracy', 'and', 'speed', 'of', 'a', 'patient', \"'s\", 'medical', 'diagnosis', '.', 'This', 'can', 'result', 'in', 'better', 'treatment', 'outcomes', 'and', 'life', 'expectancy', 'for', 'the', 'patient', '.', 'And', 'finally', ',', 'in', 'manufacturing', ',', 'well', 'trained', 'computer', 'vision', 'is', 'incorporated', 'into', 'robotics', '.', 'This', 'can', 'improve', 'quality', 'assurance', 'and', 'operational', 'efficiencies', '.', 'These', 'are', 'just', 'a', 'few', 'examples', 'and', 'you', 'can', 'probably', 'think', 'of', 'more', 'computer', 'vision', 'problems', 'can', 'be', 'broken', 'down', 'into', 'a', 'few', 'areas', '.', 'Content', 'recognition', 'is', 'about', 'identifying', 'things', 'in', 'images', '.', 'It', \"'s\", 'a', 'classification', 'problem', 'but', 'it', \"'s\", 'a', 'complex', 'one', 'with', 'several', 'layers', 'in', 'the', 'picture', 'here', '.', 'What', \"'s\", 'represented', 'is', 'it', 'breakfast', ',', 'lunch', 'or', 'dinner', ',', 'would', 'the', 'classification', 'only', 'be', 'food', '?', 'The', 'answer', 'depends', 'on', 'what', 'model', 'you', 'use', 'to', 'perform', '.', 'The', 'classification', 'models', 'must', 'be', 'trained', '.', 'And', 'the', 'training', 'data', 'provides', 'the', 'algorithm', 'with', 'data', 'for', 'it', 'to', 'learn', 'from', '.', 'Say', 'that', 'you', 'have', 'a', 'model', 'that', 'was', 'trained', 'with', 'pictures', 'of', 'different', 'types', 'of', 'food', '.', 'You', 'might', 'expect', 'the', 'image', 'to', 'output', 'categories', 'such', 'as', 'milk', ',', 'peaches', ',', 'mashed', 'potato', ',', 'chicken', ',', 'nuggets', ',', 'and', 'salad', '.', 'If', 'you', 'trained', 'the', 'model', 'with', 'different', 'images', ',', 'it', 'could', 'classify', 'objects', 'as', 'tray', ',', 'cutlery', 'and', 'napkin', '.', 'Instead', ',', 'when', 'you', 'work', 'with', 'images', ',', 'you', 'might', 'want', 'to', 'know', 'what', 'kinds', 'of', 'objects', 'are', 'in', 'the', 'image', 'and', 'the', 'location', 'of', 'those', 'objects', '.', 'Object', 'detection', 'provides', 'the', 'image', 'categories', 'and', 'where', 'the', 'objects', 'are', 'located', 'in', 'the', 'image', ',', 'there', \"'s\", 'a', 'set', 'of', 'coordinates', 'defining', 'the', 'location', 'of', 'a', 'box', 'surrounding', 'the', 'image', ',', 'which', 'is', 'known', 'as', 'the', 'bounding', 'box', '.', 'Bounding', 'boxes', '.', 'For', 'detection', 'are', 'typically', 'top', 'left', 'width', 'and', 'height', 'coordinates', 'surrounding', 'the', 'images', '.', 'You', 'can', 'use', 'these', 'coordinates', 'in', 'your', 'applications', '.', 'When', 'objects', 'are', 'detected', 'in', 'an', 'image', ',', 'there', \"'s\", 'a', 'confidence', 'number', 'usually', 'associated', 'with', 'that', 'object', '.', 'This', 'percentage', 'indicates', 'the', 'probability', 'that', 'the', 'object', 'belongs', 'to', 'a', 'specific', 'class', '.', 'This', 'confidence', 'level', 'is', 'important', 'when', 'you', 'want', 'to', 'determine', 'an', 'action', 'that', \"'s\", 'based', 'on', 'object', 'detection', ',', 'especially', 'in', 'facial', 'detection', 'applications', 'or', 'cases', 'where', 'the', 'action', 'has', 'significance', '.', 'Object', 'segmentation', 'is', 'also', 'known', 'as', 'semantic', 'segmentation', '.', 'It', \"'s\", 'like', 'object', 'detection', ',', 'but', 'you', 'go', 'into', 'more', 'detail', 'to', 'get', 'fine', 'boundaries', 'for', 'each', 'detected', 'object', '.', 'Basically', ',', 'it', \"'s\", 'a', 'fine', 'grained', 'inference', 'for', 'predicting', 'each', 'pixel', 'in', 'the', 'image', '.', 'Some', 'applications', 'that', 'require', 'object', 'segmentation', 'include', 'autonomous', 'vehicles', 'and', 'advanced', 'computer', 'human', 'interactions', '.', 'Though', 'object', 'segmentation', 'is', 'a', 'key', 'problem', 'in', 'the', 'field', 'of', 'computer', 'vision', ',', 'we', 'wo', \"n't\", 'be', 'covering', 'it', 'in', 'this', 'course', '.', 'Video', 'adds', 'another', 'dimension', 'to', 'computer', 'vision', 'with', 'video', 'you', 'get', 'more', 'data', 'to', 'work', 'with', '.', 'So', 'you', 'can', 'capture', 'the', 'movement', 'of', 'people', 'or', 'objects', 'which', 'are', 'referred', 'to', 'as', 'instances', '.', 'For', 'example', ',', 'you', 'can', 'detect', 'people', 'who', 'enter', 'and', 'leave', 'frames', 'and', 'also', 'deal', 'with', 'moving', 'cameras', '.', 'Here', \"'s\", 'a', 'use', 'case', 'for', 'computer', 'vision', 'building', 'on', 'detection', 'and', 'tracking', '.', 'You', 'can', 'analyze', 'shopper', 'behavior', 'in', 'your', 'retail', 'store', 'by', 'studying', 'the', 'path', 'each', 'person', 'follows', '.', 'If', 'you', 'use', 'face', 'analysis', ',', 'you', 'can', 'understand', 'other', 'details', 'about', 'shoppers', 'such', 'as', 'average', 'age', 'ranges', ',', 'gender', 'distribution', 'and', 'expressed', 'emotions', 'without', 'identifying', 'them', '.', 'Here', \"'s\", 'another', 'computer', 'vision', 'use', 'case', ',', 'you', 'can', 'also', 'analyze', 'images', 'to', 'identify', 'actions', 'using', 'the', 'motion', 'in', 'the', 'video', ',', 'for', 'example', ',', 'activities', 'such', 'as', 'delivering', 'a', 'package', 'or', 'dancing', ',', 'looking', 'at', 'this', 'image', 'of', 'a', 'baseball', 'player', '.', 'Some', 'examples', 'from', 'the', 'image', 'could', 'include', 'capturing', 'the', 'batter', \"'s\", 'accuracy', '.', 'The', 'pitcher', \"'s\", 'pitching', 'style', ',', 'the', 'type', 'of', 'pitch', ',', 'slow', 'ball', 'slider', 'and', 'others', ',', 'the', 'inning', 'and', 'the', 'batter', \"'s\", 'performance', 'versus', 'the', 'specific', 'pitcher', 'managers', 'could', 'use', 'all', 'that', 'data', 'to', 'coach', 'players', 'on', 'how', 'to', 'improve', 'their', 'performance', '.', 'And', 'they', 'do', 'coaches', 'can', 'also', 'use', 'the', 'data', 'during', 'the', 'game', 'to', 'make', 'game', 'time', 'decisions', '.', 'Say', 'you', 'want', 'to', 'initiate', 'various', 'actions', 'based', 'on', 'the', 'speed', 'of', 'the', 'baseball', 'leaving', 'the', 'bat', 'and', 'its', 'trajectory', 'a', 'hit', 'that', \"'s\", 'calculated', 'by', 'an', 'M', 'L', 'model', 'could', 'lead', 'to', 'an', 'audio', 'or', 'visual', 'warning', 'about', 'a', 'possible', 'foul', 'ball', 'into', 'the', 'crowd', 'or', 'it', 'could', 'result', 'in', 'a', 'preemptive', 'alarm', 'that', 'a', 'hit', 'has', 'a', 'high', 'probability', 'of', 'being', 'a', 'home', 'run', '.', 'This', 'means', 'that', 'events', 'following', 'a', 'home', 'run', 'could', 'be', 'both', 'well', 'timed', 'and', 'automated', 'such', 'as', 'playing', 'music', 'or', 'setting', 'off', 'fireworks', 'when', 'the', 'home', 'run', 'is', 'hit', 'by', 'the', 'home', 'team', 'to', 'wrap', 'up', '.', 'This', 'section', ',', 'here', 'are', 'some', 'key', 'takeaways', 'for', 'this', 'section', '.', 'First', ',', 'we', 'covered', 'how', 'computer', 'vision', 'is', 'the', 'automated', 'extraction', 'of', 'information', 'from', 'images', '.', 'You', 'can', 'divide', 'computer', 'vision', 'into', 'two', 'distinct', 'areas', '.', 'Image', 'analysis', 'and', 'video', 'analysis', '.', 'Image', 'analysis', 'includes', 'object', 'classification', 'detection', 'and', 'segmentation', '.', 'Video', 'analysis', 'includes', 'instance', ',', 'tracking', 'action', 'recognition', 'and', 'motion', 'estimation', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . This is Section one and we 're going to introduce computer vision . Computer vision is an exciting space in machine learning . You can think of computer vision a the automated extraction of information from digital image . Using computer vision machine can identify people place and thing in image with an accuracy that 's at or above human level and with greater speed and efficiency . Computer vision is often built with deep learning model . It automates the extraction analysis , classification and understanding of useful information from a single image or a sequence of image . The image data can take many form such a single image , video sequence , view from multiple camera or three dimensional data . Computing power and algorithm have advanced over the last 10 year . This ha led to an increase in capability and easier access to computer vision technology . So how is computer vision being used ? Here are some of the primary use case for computer vision . You can use image and facial recognition to improve public safety and home security or a a way to authenticate access to personal device , you can also use it to automatically classify image for content management and analysis . Autonomous driving is partly enabled by computer vision technology . And so are safety feature of car such a lane detection or collision avoidance . Medical image analysis with computer vision can improve the accuracy and speed of a patient 's medical diagnosis . This can result in better treatment outcome and life expectancy for the patient . And finally , in manufacturing , well trained computer vision is incorporated into robotics . This can improve quality assurance and operational efficiency . These are just a few example and you can probably think of more computer vision problem can be broken down into a few area . Content recognition is about identifying thing in image . It 's a classification problem but it 's a complex one with several layer in the picture here . What 's represented is it breakfast , lunch or dinner , would the classification only be food ? The answer depends on what model you use to perform . The classification model must be trained . And the training data provides the algorithm with data for it to learn from . Say that you have a model that wa trained with picture of different type of food . You might expect the image to output category such a milk , peach , mashed potato , chicken , nugget , and salad . If you trained the model with different image , it could classify object a tray , cutlery and napkin . Instead , when you work with image , you might want to know what kind of object are in the image and the location of those object . Object detection provides the image category and where the object are located in the image , there 's a set of coordinate defining the location of a box surrounding the image , which is known a the bounding box . Bounding box . For detection are typically top left width and height coordinate surrounding the image . You can use these coordinate in your application . When object are detected in an image , there 's a confidence number usually associated with that object . This percentage indicates the probability that the object belongs to a specific class . This confidence level is important when you want to determine an action that 's based on object detection , especially in facial detection application or case where the action ha significance . Object segmentation is also known a semantic segmentation . It 's like object detection , but you go into more detail to get fine boundary for each detected object . Basically , it 's a fine grained inference for predicting each pixel in the image . Some application that require object segmentation include autonomous vehicle and advanced computer human interaction . Though object segmentation is a key problem in the field of computer vision , we wo n't be covering it in this course . Video add another dimension to computer vision with video you get more data to work with . So you can capture the movement of people or object which are referred to a instance . For example , you can detect people who enter and leave frame and also deal with moving camera . Here 's a use case for computer vision building on detection and tracking . You can analyze shopper behavior in your retail store by studying the path each person follows . If you use face analysis , you can understand other detail about shopper such a average age range , gender distribution and expressed emotion without identifying them . Here 's another computer vision use case , you can also analyze image to identify action using the motion in the video , for example , activity such a delivering a package or dancing , looking at this image of a baseball player . Some example from the image could include capturing the batter 's accuracy . The pitcher 's pitching style , the type of pitch , slow ball slider and others , the inning and the batter 's performance versus the specific pitcher manager could use all that data to coach player on how to improve their performance . And they do coach can also use the data during the game to make game time decision . Say you want to initiate various action based on the speed of the baseball leaving the bat and it trajectory a hit that 's calculated by an M L model could lead to an audio or visual warning about a possible foul ball into the crowd or it could result in a preemptive alarm that a hit ha a high probability of being a home run . This mean that event following a home run could be both well timed and automated such a playing music or setting off firework when the home run is hit by the home team to wrap up . This section , here are some key takeaway for this section . First , we covered how computer vision is the automated extraction of information from image . You can divide computer vision into two distinct area . Image analysis and video analysis . Image analysis includes object classification detection and segmentation . Video analysis includes instance , tracking action recognition and motion estimation . Thanks for watching . We 'll see you in the next video .\n",
      "['Welcome', 'back', 'in', 'this', 'section', ',', 'we', \"'ll\", 'explore', 'image', 'analysis', 'in', 'more', 'detail', '.', 'And', 'in', 'part', 'two', ',', 'we', \"'ll\", 'take', 'a', 'closer', 'look', 'into', 'video', 'analysis', 'to', 'start', '.', 'We', \"'ll\", 'introduce', 'the', 'main', 'Amazon', 'service', '.', 'We', \"'ll\", 'be', 'using', 'Amazon', 'recognition', '.', 'Amazon', 'recognition', 'is', 'a', 'computer', 'vision', 'service', 'that', \"'s\", 'based', 'on', 'deep', 'learning', '.', 'You', 'can', 'use', 'it', 'to', 'add', 'image', 'and', 'video', 'analysis', 'to', 'your', 'applications', '.', 'There', 'are', 'many', 'uses', 'for', 'Amazon', 'recognition', 'including', 'creating', 'searchable', 'image', 'and', 'video', 'libraries', '.', 'Amazon', 'recognition', 'makes', 'both', 'images', 'and', 'stored', 'videos', 'searchable', 'so', 'you', 'can', 'discover', 'the', 'objects', 'and', 'scenes', 'that', 'appear', 'in', 'them', '.', 'You', 'can', 'use', 'Amazon', 'recognition', 'to', 'build', 'a', 'face', 'based', 'user', 'verification', 'system', '.', 'So', 'your', 'applications', 'can', 'confirm', 'user', 'identities', 'by', 'comparing', 'their', 'live', 'image', 'with', 'a', 'reference', 'image', '.', 'Amazon', 'recognition', 'interprets', 'emotional', 'expressions', 'such', 'as', 'happy', ',', 'sad', 'or', 'surprise', '.', 'It', 'can', 'also', 'interpret', 'demographic', 'information', 'from', 'facial', 'images', 'such', 'as', 'gender', '.', 'Amazon', 'recognition', 'can', 'also', 'detect', 'inappropriate', 'content', 'in', 'both', 'images', 'and', 'stored', 'videos', '.', 'And', 'finally', ',', 'Amazon', 'recognition', 'can', 'recognize', 'and', 'extract', 'text', 'content', 'from', 'images', 'before', 'we', 'go', 'further', '.', 'Here', \"'s\", 'a', 'quick', 'note', 'on', 'security', '.', 'You', 'need', 'to', 'check', 'if', 'the', 'applications', 'you', 'build', 'using', 'Amazon', 'recognition', 'fall', 'under', 'any', 'regulatory', 'restrictions', 'as', 'defined', 'in', 'your', 'field', 'or', 'country', 'security', 'and', 'compliance', '.', 'For', 'Amazon', 'Recognition', 'is', 'a', 'shared', 'responsibility', 'between', 'Aws', 'and', 'the', 'customer', '.', 'For', 'more', 'information', 'about', 'this', 'topic', ',', 'see', 'the', 'Aws', 'compliance', 'page', ',', 'Amazon', 'Recognition', 'is', 'an', 'Aws', 'managed', 'service', 'with', 'a', 'managed', 'service', '.', 'Amazon', 'hosts', 'the', 'machine', 'learning', 'models', ',', 'maintains', 'an', 'API', 'and', 'scales', 'out', 'to', 'meet', 'demand', 'for', 'you', '.', 'You', 'can', 'benefit', 'from', 'a', 'set', 'of', 'models', 'that', 'constantly', 'learn', 'and', 'improve', '.', 'Also', ',', 'you', 'can', 'focus', 'on', 'building', 'applications', 'that', 'use', 'the', 'API', 'and', 'optionally', 'training', 'the', 'service', 'to', 'understand', 'your', 'unique', 'business', 'needs', '.', 'There', 'are', 'various', 'resources', 'you', 'can', 'use', 'to', 'access', 'and', 'interact', 'with', 'Amazon', 'recognition', 'such', 'as', 'API', 'S', 'S', 'D', 'K', 'S', 'and', 'commands', 'for', 'the', 'AWS', 'command', 'line', 'interface', ',', 'which', 'is', 'also', 'known', 'as', 'the', 'AWS', 'cli', '.', 'The', 'languages', 'supported', 'by', 'the', 'S', 'D', 'K', 's', 'include', 'javascript', ',', 'Python', 'P', 'H', 'P', 'dot', 'net', ',', 'Ruby', ',', 'Java', 'go', 'no', 'J', 'S', 'and', 'C++', '.', 'Finally', ',', 'Amazon', 'recognition', 'integrates', 'with', 'other', 'Aws', 'services', '.', 'For', 'example', ',', 'if', 'you', 'need', 'storage', ',', 'you', 'can', 'use', 'Amazon', \"'s\", 'simple', 'storage', 'service', 'or', 'S3', 'for', 'authentication', 'and', 'authorization', '.', 'You', 'can', 'use', 'Aws', 'identity', 'and', 'access', 'management', ',', 'which', 'is', 'also', 'known', 'as', 'I', 'AM', '.', 'This', 'diagram', 'illustrates', 'an', 'image', 'search', 'feature', 'where', 'users', 'can', 'take', 'pictures', 'and', 'get', 'information', 'about', 'the', 'real', 'estate', 'properties', 'they', \"'re\", 'viewing', 'first', '.', 'The', 'user', 'takes', 'a', 'picture', 'with', 'their', 'mobile', 'device', '.', 'The', 'user', 'then', 'initiates', 'a', 'search', 'which', 'causes', 'the', 'application', 'to', 'upload', 'to', 'Amazon', 'S3', 'S3', 'is', 'configured', 'to', 'call', 'other', 'services', '.', 'When', 'a', 'write', 'event', 'occurs', '.', 'In', 'this', 'case', ',', 'the', 'bucket', 'passes', 'the', 'S3', 'path', 'of', 'the', 'new', 'object', 'to', 'Aws', 'LAMBDA', '.', 'When', 'the', 'LAMBDA', 'function', 'is', 'called', ',', 'it', 'uses', 'the', 'Amazon', 'recognition', 'SDK', 'to', 'call', 'the', 'service', '.', 'Amazon', 'recognition', 'analyzes', 'the', 'image', 'detects', 'aspects', 'of', 'the', 'property', 'creates', 'labels', 'and', 'passes', 'the', 'information', 'back', 'to', 'LAMBDA', 'as', 'an', 'object', 'formatted', 'in', 'javascript', 'object', 'notation', 'or', 'json', 'LAMBDA', ',', 'then', 'stores', 'the', 'labels', 'and', 'confidence', 'score', 'in', 'Amazon', 'elastic', 'search', 'service', ',', 'which', 'is', 'also', 'known', 'as', 'Amazon', 'E', 'S', 'application', '.', 'Users', 'can', 'now', 'identify', 'aspects', 'of', 'a', 'property', 'using', 'the', 'objects', 'that', 'were', 'detected', 'in', 'the', 'image', '.', 'In', 'this', 'example', 'architecture', ',', 'the', 'system', 'checks', 'uploaded', 'images', 'for', 'inappropriate', 'content', '.', 'Like', 'the', 'previous', 'example', ',', 'processing', 'begins', 'when', 'the', 'user', 'uploads', 'content', '.', 'First', ',', 'the', 'user', 'uploads', 'an', 'image', 'to', 'Amazon', 'S3', '.', 'Second', ',', 'the', 'S3', 'bucket', 'is', 'configured', 'to', 'call', 'a', 'Lambda', 'function', '.', 'When', 'an', 'object', 'is', 'written', 'to', 'the', 'bucket', '.', 'Third', ',', 'lambda', 'calls', 'Amazon', 'recognition', 'via', 'the', 'S', 'D', 'K', 'Amazon', 'recognition', ',', 'then', 'analyzes', 'the', 'images', 'for', 'inappropriate', 'content', 'and', 'sends', 'the', 'response', 'back', 'to', 'LAMBDA', '.', 'Fourth', '.', 'If', 'the', 'content', 'is', 'appropriate', ',', 'the', 'content', 'is', 'approved', '.', 'Fifth', '.', 'If', 'the', 'content', 'is', \"n't\", 'appropriate', ',', 'the', 'content', 'can', 'be', 'sent', 'for', 'manual', 'inspection', '.', 'And', 'finally', ',', 'if', 'the', 'content', 'is', \"n't\", 'approved', ',', 'a', 'notification', 'is', 'sent', 'to', 'the', 'user', '.', 'In', 'this', 'final', 'use', 'case', ',', 'the', 'system', 'analyzes', 'a', 'video', 'feed', 'for', 'sentiment', 'analysis', '.', 'First', ',', 'an', 'in', 'store', 'camera', 'captures', 'video', 'that', \"'s\", 'then', 'sent', 'to', 'a', 'back', 'office', 'or', 'a', 'cloud', 'based', 'application', '.', 'Typically', 'an', 'application', 'like', 'this', 'uses', 'Amazon', 'kinesis', 'to', 'stream', 'the', 'video', '.', 'Second', ',', 'the', 'application', 'uses', 'the', 'S', 'D', 'K', 'to', 'send', 'the', 'video', 'to', 'Amazon', 'recognition', '.', 'For', 'further', 'analysis', ',', 'visual', 'sentiment', 'is', 'extracted', 'along', 'with', 'other', 'attributes', 'such', 'as', 'age', '.', 'Third', ',', 'the', 'discovered', 'attributes', 'are', 'sent', 'to', 'Amazon', 'kinesis', '.', 'Fourth', ',', 'a', 'lambda', 'function', 'extracts', 'the', 'data', 'from', 'the', 'stream', '.', 'Fifth', ',', 'the', 'data', 'is', 'then', 'written', 'to', 'S3', '.', 'Next', ',', 'the', 'data', 'is', 'loaded', 'into', 'Amazon', 'redshift', 'on', 'a', 'regular', 'basis', '.', 'And', 'finally', ',', 'tools', 'like', 'Amazon', 'Quicks', 'site', 'can', 'be', 'used', 'to', 'generate', 'reports', 'from', 'the', 'data', '.', 'Amazon', 'recognition', 'is', 'designed', 'to', 'integrate', 'into', 'your', 'applications', 'through', 'the', 'API', 'and', 'S', 'D', 'K', 'S', 'API', 'operations', 'are', 'provided', 'for', 'detecting', 'labels', ',', 'faces', ',', 'recognizing', 'celebrities', 'and', 'detecting', 'unsafe', 'images', 'to', 'perform', 'a', 'prediction', '.', 'Provide', 'the', 'service', 'with', 'an', 'image', 'object', 'in', 'Amazon', 'S3', 'or', 'upload', 'a', 'byte', 'stream', 'of', 'an', 'image', 'images', 'can', 'be', 'in', 'JPEG', 'or', 'P', 'and', 'G', 'formats', '.', 'Amazon', 'recognition', 'processes', '.', 'The', 'image', 'performs', 'the', 'prediction', 'and', 'returns', 'a', 'JSON', 'object', 'with', 'the', 'results', '.', 'When', 'Amazon', 'recognition', 'performs', 'predictions', ',', 'it', 'often', 'returns', 'multiple', 'labels', '.', 'Each', 'label', 'has', 'a', 'confidence', 'level', '.', 'This', 'confidence', 'level', 'indicates', 'how', 'likely', 'the', 'label', 'was', 'found', 'in', 'the', 'image', '.', 'Like', 'this', 'example', 'shows', 'labels', 'can', 'also', 'have', 'hierarchies', '.', 'When', 'you', 'find', 'instances', 'of', 'objects', 'you', 'need', 'to', 'understand', 'where', 'the', 'detected', 'object', 'is', 'in', 'the', 'image', '.', 'For', 'each', 'instance', ',', 'the', 'results', 'from', 'Amazon', 'recognition', 'include', 'a', 'bounding', 'box', 'that', 'contains', 'the', 'starting', 'coordinate', 'of', 'top', 'left', 'and', 'box', 'dimensions', 'of', 'width', 'height', '.', 'Like', 'the', 'example', ',', 'you', 'can', 'use', 'this', 'information', 'to', 'determine', 'the', 'location', 'of', 'the', 'detected', 'object', 'in', 'the', 'image', '.', 'It', \"'s\", 'important', 'to', 'note', 'that', 'all', 'findings', 'contain', 'a', 'confidence', 'score', '.', 'You', 'can', 'use', 'the', 'confidence', 'score', 'in', 'your', 'applications', 'to', 'tune', 'your', 'response', 'to', 'predictions', 'with', 'a', 'higher', 'score', '.', 'It', \"'s\", 'more', 'likely', 'that', 'the', 'object', 'was', 'correctly', 'labeled', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'one', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'two', 'where', 'we', \"'ll\", 'explore', 'facial', 'detection', '.']\n",
      "Welcome back in this section , we 'll explore image analysis in more detail . And in part two , we 'll take a closer look into video analysis to start . We 'll introduce the main Amazon service . We 'll be using Amazon recognition . Amazon recognition is a computer vision service that 's based on deep learning . You can use it to add image and video analysis to your application . There are many us for Amazon recognition including creating searchable image and video library . Amazon recognition make both image and stored video searchable so you can discover the object and scene that appear in them . You can use Amazon recognition to build a face based user verification system . So your application can confirm user identity by comparing their live image with a reference image . Amazon recognition interprets emotional expression such a happy , sad or surprise . It can also interpret demographic information from facial image such a gender . Amazon recognition can also detect inappropriate content in both image and stored video . And finally , Amazon recognition can recognize and extract text content from image before we go further . Here 's a quick note on security . You need to check if the application you build using Amazon recognition fall under any regulatory restriction a defined in your field or country security and compliance . For Amazon Recognition is a shared responsibility between Aws and the customer . For more information about this topic , see the Aws compliance page , Amazon Recognition is an Aws managed service with a managed service . Amazon host the machine learning model , maintains an API and scale out to meet demand for you . You can benefit from a set of model that constantly learn and improve . Also , you can focus on building application that use the API and optionally training the service to understand your unique business need . There are various resource you can use to access and interact with Amazon recognition such a API S S D K S and command for the AWS command line interface , which is also known a the AWS cli . The language supported by the S D K s include javascript , Python P H P dot net , Ruby , Java go no J S and C++ . Finally , Amazon recognition integrates with other Aws service . For example , if you need storage , you can use Amazon 's simple storage service or S3 for authentication and authorization . You can use Aws identity and access management , which is also known a I AM . This diagram illustrates an image search feature where user can take picture and get information about the real estate property they 're viewing first . The user take a picture with their mobile device . The user then initiate a search which cause the application to upload to Amazon S3 S3 is configured to call other service . When a write event occurs . In this case , the bucket pass the S3 path of the new object to Aws LAMBDA . When the LAMBDA function is called , it us the Amazon recognition SDK to call the service . Amazon recognition analyzes the image detects aspect of the property creates label and pass the information back to LAMBDA a an object formatted in javascript object notation or json LAMBDA , then store the label and confidence score in Amazon elastic search service , which is also known a Amazon E S application . Users can now identify aspect of a property using the object that were detected in the image . In this example architecture , the system check uploaded image for inappropriate content . Like the previous example , processing begin when the user uploads content . First , the user uploads an image to Amazon S3 . Second , the S3 bucket is configured to call a Lambda function . When an object is written to the bucket . Third , lambda call Amazon recognition via the S D K Amazon recognition , then analyzes the image for inappropriate content and sends the response back to LAMBDA . Fourth . If the content is appropriate , the content is approved . Fifth . If the content is n't appropriate , the content can be sent for manual inspection . And finally , if the content is n't approved , a notification is sent to the user . In this final use case , the system analyzes a video feed for sentiment analysis . First , an in store camera capture video that 's then sent to a back office or a cloud based application . Typically an application like this us Amazon kinesis to stream the video . Second , the application us the S D K to send the video to Amazon recognition . For further analysis , visual sentiment is extracted along with other attribute such a age . Third , the discovered attribute are sent to Amazon kinesis . Fourth , a lambda function extract the data from the stream . Fifth , the data is then written to S3 . Next , the data is loaded into Amazon redshift on a regular basis . And finally , tool like Amazon Quicks site can be used to generate report from the data . Amazon recognition is designed to integrate into your application through the API and S D K S API operation are provided for detecting label , face , recognizing celebrity and detecting unsafe image to perform a prediction . Provide the service with an image object in Amazon S3 or upload a byte stream of an image image can be in JPEG or P and G format . Amazon recognition process . The image performs the prediction and return a JSON object with the result . When Amazon recognition performs prediction , it often return multiple label . Each label ha a confidence level . This confidence level indicates how likely the label wa found in the image . Like this example show label can also have hierarchy . When you find instance of object you need to understand where the detected object is in the image . For each instance , the result from Amazon recognition include a bounding box that contains the starting coordinate of top left and box dimension of width height . Like the example , you can use this information to determine the location of the detected object in the image . It 's important to note that all finding contain a confidence score . You can use the confidence score in your application to tune your response to prediction with a higher score . It 's more likely that the object wa correctly labeled . That 's it . For part one of this section , we 'll see you again for part two where we 'll explore facial detection .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'image', 'analysis', 'with', 'a', 'closer', 'look', 'at', 'facial', 'detection', '.', 'Facial', 'detection', 'uses', 'a', 'model', 'that', 'was', 'tuned', 'to', 'perform', 'predictions', 'specifically', 'for', 'detecting', 'faces', 'and', 'facial', 'features', '.', 'Facial', 'detection', 'has', 'many', 'of', 'the', 'same', 'features', 'as', 'standard', 'object', 'detection', 'such', 'as', 'a', 'bounding', 'box', 'or', 'the', 'coordinates', 'of', 'the', 'box', 'surrounding', 'the', 'face', 'that', 'was', 'detected', '.', 'This', 'will', 'include', 'a', 'value', 'representing', 'the', 'confidence', 'that', 'the', 'bounding', 'box', 'contains', 'a', 'face', '.', 'There', 'will', 'be', 'a', 'list', 'of', 'attributes', 'if', 'found', 'such', 'as', 'if', 'the', 'face', 'has', 'a', 'beard', 'or', 'if', 'it', 'appears', 'to', 'be', 'male', 'or', 'female', ',', 'there', 'will', 'also', 'be', 'a', 'confidence', 'score', 'for', 'these', 'attributes', '.', 'It', 'can', 'also', 'detect', 'physical', 'emotions', 'like', 'whether', 'the', 'person', 'is', 'smiling', 'or', 'frowning', '.', 'It', \"'s\", 'important', 'to', 'understand', 'this', 'classification', 'is', 'based', 'only', 'on', 'visual', 'clues', '.', 'And', 'so', 'it', 'might', 'not', 'represent', 'the', 'actual', 'emotion', 'of', 'the', 'person', '.', 'Facial', 'landmarks', 'are', 'components', 'of', 'the', 'face', 'such', 'as', 'eyes', 'and', 'mouth', '.', 'Typical', 'landmarks', 'also', 'include', 'X', 'and', 'Y', 'coordinates', 'quality', 'describes', 'the', 'brightness', 'and', 'the', 'sharpness', 'of', 'the', 'face', 'and', 'pose', 'describes', 'the', 'rotation', 'of', 'the', 'face', 'inside', 'the', 'image', '.', 'Again', ',', 'confidence', 'is', 'a', 'feature', 'here', 'and', 'it', \"'s\", 'provided', 'for', 'each', 'detected', 'feature', '.', 'And', 'remember', 'the', 'feature', 'prediction', 'is', 'based', 'only', 'on', 'visual', 'observations', 'with', 'Amazon', 'recognition', '.', 'You', 'can', 'compare', 'two', 'images', 'to', 'determine', 'if', 'they', 'contain', 'the', 'same', 'person', '.', 'Comparisons', 'require', 'both', 'a', 'source', 'and', 'a', 'target', 'image', '.', 'The', 'results', 'will', 'include', 'all', 'the', 'faces', 'that', 'were', 'found', 'and', 'they', 'include', 'information', 'about', 'matching', 'and', 'non', 'matching', 'faces', '.', 'Again', ',', 'confidence', 'scores', 'indicate', 'how', 'likely', 'each', 'prediction', 'is', '.', 'Amazon', 'recognition', 'can', 'also', 'search', 'for', 'known', 'faces', 'to', 'use', 'this', 'feature', '.', 'You', 'need', 'to', 'train', 'the', 'model', 'by', 'providing', 'a', 'collection', 'of', 'images', 'to', 'use', '.', 'After', 'you', 'train', 'the', 'model', ',', 'you', 'can', 'then', 'detect', 'those', 'people', 'and', 'images', 'you', 'provide', 'to', 'find', 'known', 'faces', '.', 'First', 'create', 'a', 'collection', 'and', 'add', 'faces', 'to', 'the', 'collection', '.', 'Amazon', 'recognition', 'will', 'perform', 'facial', 'recognition', 'on', 'the', 'images', 'you', 'provide', ',', 'it', 'will', 'then', 'return', 'typical', 'information', 'like', 'the', 'bounding', 'box', 'coordinates', 'or', 'the', 'confidence', 'score', 'to', 'associate', 'faces', 'with', 'an', 'image', 'specify', 'an', 'image', 'ID', 'in', 'the', 'external', 'image', 'ID', 'request', 'parameter', '.', 'This', 'could', 'be', 'the', 'file', 'name', 'of', 'the', 'image', 'or', 'another', 'ID', 'that', 'you', 'create', '.', 'After', 'you', 'create', 'your', 'collection', ',', 'you', 'can', 'then', 'use', 'the', 'search', 'faces', 'by', 'image', 'operation', 'to', 'search', 'for', 'faces', 'from', 'the', 'collection', '.', 'The', 'return', 'data', 'contains', 'an', 'array', 'of', 'all', 'faces', 'that', 'matched', 'the', 'information', 'includes', 'bounding', 'boxes', ',', 'confidence', 'scores', 'and', 'the', 'external', 'image', 'ID', 'value', '.', 'You', 'can', 'then', 'use', 'this', 'ID', 'value', 'to', 'link', 'back', 'to', 'the', 'source', 'image', '.', 'Now', 'that', 'you', \"'ve\", 'learned', 'about', 'the', 'facial', 'detection', 'features', 'of', 'Amazon', 'recognition', '.', 'Here', \"'s\", 'a', 'summary', 'of', 'the', 'guidelines', 'we', \"'ve\", 'discussed', 'so', 'far', '.', 'When', 'Amazon', 'recognition', 'detects', 'a', 'human', 'face', ',', 'it', 'captures', 'a', 'bounding', 'box', 'that', 'shows', 'where', 'the', 'face', 'was', 'found', 'in', 'the', 'video', '.', 'It', 'can', 'also', 'detect', 'attributes', 'such', 'as', 'the', 'position', 'of', 'the', 'eyes', ',', 'nose', 'and', 'mouth', '.', 'It', 'can', 'detect', 'emotion', ',', 'the', 'quality', 'of', 'the', 'detection', 'and', 'any', 'landmarks', 'that', 'might', 'appear', '.', 'All', 'these', 'items', 'will', 'have', 'an', 'associated', 'confidence', 'score', '.', 'A', 'higher', 'score', 'means', 'that', 'the', 'model', 'has', 'greater', 'confidence', 'about', 'the', 'detection', 'gender', 'is', 'inferred', 'from', 'the', 'image', ',', 'not', 'inferred', 'from', 'identity', '.', 'Similarly', ',', 'emotion', 'is', 'also', 'determined', 'from', 'the', 'image', 'and', 'it', 'might', 'not', 'reflect', 'the', 'subject', \"'s\", 'actual', 'emotional', 'state', '.', 'How', 'should', 'I', 'apply', 'facial', 'recognition', 'responsibly', '?', 'Facial', 'recognition', 'should', 'never', 'be', 'used', 'in', 'a', 'way', 'that', 'violates', 'an', 'individual', \"'s\", 'rights', 'including', 'the', 'right', 'to', 'privacy', '.', 'It', 'should', 'also', 'never', 'be', 'used', 'to', 'make', 'autonomous', 'decisions', 'for', 'scenarios', 'that', 'require', 'a', 'human', 'being', 'to', 'analyze', 'them', '.', 'For', 'example', ',', 'suppose', 'that', 'a', 'bank', 'uses', 'tools', 'like', 'Amazon', 'recognition', 'in', 'a', 'financial', 'application', 'to', 'verify', 'their', 'customers', 'identities', '.', 'The', 'bank', 'should', 'always', 'clearly', 'disclose', 'the', 'use', 'of', 'the', 'technology', 'and', 'ask', 'the', 'customer', 'to', 'approve', 'their', 'terms', 'and', 'conditions', '.', 'For', 'more', 'information', 'about', 'this', 'topic', ',', 'see', 'the', 'Aws', 'web', 'page', 'about', 'the', 'facts', 'on', 'facial', 'recognition', 'with', 'artificial', 'intelligence', 'will', 'now', 'explain', 'how', 'you', 'can', 'use', 'Amazon', 'recognition', 'to', 'process', 'videos', '.', 'You', 'can', 'perform', 'video', 'processing', 'on', 'both', 'stored', 'videos', 'and', 'video', 'streams', ',', 'stored', 'videos', 'should', 'be', 'uploaded', 'and', 'stored', 'in', 'an', 'S3', 'bucket', '.', 'Each', 'type', 'of', 'detection', 'has', 'its', 'own', 'start', 'operation', '.', 'You', 'can', 'search', 'for', 'people', ',', 'faces', ',', 'labels', ',', 'celebrities', ',', 'text', 'and', 'inappropriate', 'content', '.', 'Amazon', 'recognition', 'publishes', 'a', 'completion', 'status', 'to', 'a', 'topic', 'in', 'Amazon', \"'s\", 'simple', 'notification', 'service', 'which', 'is', 'also', 'known', 'as', 'Amazon', 'S', 'N', 'S.', 'Then', 'S', 'N', 'S', 'can', 'route', 'these', 'messages', 'to', 'subscribers', 'for', 'durability', '.', 'It', \"'s\", 'a', 'best', 'practice', 'to', 'route', 'messages', 'to', 'a', 'message', 'queue', '.', 'In', 'Amazon', \"'s\", 'simple', 'QE', 'service', 'or', 'Amazon', 'S', 'Q', 'S', ',', 'your', 'application', 'should', 'monitor', 'the', 'SQS', 'Q', 'for', 'completion', '.', 'Each', 'start', 'operation', 'has', 'a', 'corresponding', 'get', 'operation', 'for', 'retrieving', 'the', 'results', '.', 'If', 'you', 'call', 'get', 'detection', 'results', ',', 'it', 'returns', 'an', 'array', 'of', 'labels', 'that', 'contain', 'information', 'about', 'any', 'labels', 'found', 'in', 'the', 'video', '.', 'The', 'label', 'information', 'includes', 'the', 'same', 'labels', 'as', 'image', 'detection', ',', 'but', 'it', 'also', 'includes', 'a', 'time', 'stamp', 'of', 'where', 'the', 'label', 'was', 'detected', 'in', 'milliseconds', 'from', 'the', 'start', 'of', 'the', 'video', '.', 'In', 'addition', 'to', 'stored', 'videos', ',', 'you', 'can', 'also', 'use', 'Amazon', 'recognition', 'video', 'to', 'detect', 'and', 'recognize', 'faces', 'in', 'streaming', 'video', '.', 'A', 'typical', 'use', 'case', 'for', 'this', 'is', 'detecting', 'a', 'known', 'face', 'in', 'a', 'video', 'stream', '.', 'Amazon', 'recognition', 'video', 'uses', 'Amazon', 'kinesis', 'video', 'streams', 'to', 'receive', 'and', 'process', 'a', 'video', 'stream', '.', 'The', 'analysis', 'results', 'are', 'output', 'from', 'Amazon', 'recognition', 'video', 'to', 'a', 'kinesis', 'data', 'stream', '.', 'They', 'are', 'then', 'read', 'by', 'your', 'client', 'application', '.', 'Amazon', 'recognition', 'video', 'provides', 'a', 'stream', 'processor', 'that', \"'s\", 'called', 'create', 'stream', 'processor', 'and', 'you', 'can', 'use', 'it', 'to', 'start', 'and', 'manage', 'the', 'analysis', 'of', 'the', 'streaming', 'video', 'to', 'use', 'Amazon', 'recognition', 'video', 'with', 'your', 'streaming', 'video', ',', 'your', 'application', 'must', 'implement', 'these', 'resources', '.', 'First', ',', 'you', 'need', 'a', 'kinesis', 'video', 'stream', 'to', 'send', 'streaming', 'video', 'to', 'Amazon', 'recognition', 'video', '.', 'Next', ',', 'you', 'need', 'an', 'Amazon', 'recognition', 'video', 'stream', 'processor', 'to', 'manage', 'the', 'streaming', 'video', 'analysis', '.', 'And', 'finally', ',', 'you', 'need', 'a', 'kinesis', 'data', 'stream', 'consumer', 'to', 'read', 'the', 'analysis', 'results', 'that', 'Amazon', 'recognition', 'video', 'sends', 'to', 'the', 'data', 'stream', '.', 'If', 'you', 'want', 'to', 'find', 'a', 'face', 'in', 'a', 'video', ',', 'you', 'need', 'to', 'create', 'a', 'collection', '.', 'This', 'process', 'is', 'the', 'same', 'as', 'creating', 'a', 'collection', 'for', 'still', 'images', '.', 'Amazon', 'recognition', 'video', 'places', 'a', 'JSON', 'frame', 'record', 'for', 'each', 'analyzed', 'frame', 'into', 'the', 'kinesis', 'output', 'stream', '.', 'Amazon', 'recognition', 'video', 'does', \"n't\", 'analyze', 'every', 'frame', 'that', \"'s\", 'passed', 'to', 'it', 'through', 'the', 'kinesis', 'video', 'stream', '.', 'A', 'frame', 'record', 'that', \"'s\", 'sent', 'to', 'a', 'kinesis', 'data', 'stream', 'contains', 'information', 'about', 'which', 'video', 'stream', 'fragment', 'the', 'frame', 'is', 'in', 'where', 'the', 'frame', 'is', 'in', 'the', 'fragment', 'and', 'faces', 'that', 'are', 'recognized', 'in', 'the', 'frame', '.', 'It', 'also', 'includes', 'status', 'information', 'for', 'the', 'stream', 'processor', 'before', 'we', 'wrap', 'up', '.', 'Here', \"'s\", 'a', 'quick', 'summary', '.', 'Amazon', 'recognition', 'is', 'a', 'computer', 'vision', 'service', 'that', \"'s\", 'based', 'on', 'deep', 'learning', '.', 'You', 'can', 'easily', 'add', 'image', 'and', 'video', 'analysis', 'to', 'your', 'applications', '.', 'Amazon', 'recognition', 'can', 'detect', 'faces', 'sentiment', ',', 'text', 'unsafe', 'content', 'and', 'library', 'search', 'in', 'both', 'images', 'and', 'video', '.', 'Amazon', 'recognition', 'is', 'integrated', 'with', 'other', 'Aws', 'services', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video']\n",
      "Hi , welcome back . We 'll continue exploring image analysis with a closer look at facial detection . Facial detection us a model that wa tuned to perform prediction specifically for detecting face and facial feature . Facial detection ha many of the same feature a standard object detection such a a bounding box or the coordinate of the box surrounding the face that wa detected . This will include a value representing the confidence that the bounding box contains a face . There will be a list of attribute if found such a if the face ha a beard or if it appears to be male or female , there will also be a confidence score for these attribute . It can also detect physical emotion like whether the person is smiling or frowning . It 's important to understand this classification is based only on visual clue . And so it might not represent the actual emotion of the person . Facial landmark are component of the face such a eye and mouth . Typical landmark also include X and Y coordinate quality describes the brightness and the sharpness of the face and pose describes the rotation of the face inside the image . Again , confidence is a feature here and it 's provided for each detected feature . And remember the feature prediction is based only on visual observation with Amazon recognition . You can compare two image to determine if they contain the same person . Comparisons require both a source and a target image . The result will include all the face that were found and they include information about matching and non matching face . Again , confidence score indicate how likely each prediction is . Amazon recognition can also search for known face to use this feature . You need to train the model by providing a collection of image to use . After you train the model , you can then detect those people and image you provide to find known face . First create a collection and add face to the collection . Amazon recognition will perform facial recognition on the image you provide , it will then return typical information like the bounding box coordinate or the confidence score to associate face with an image specify an image ID in the external image ID request parameter . This could be the file name of the image or another ID that you create . After you create your collection , you can then use the search face by image operation to search for face from the collection . The return data contains an array of all face that matched the information includes bounding box , confidence score and the external image ID value . You can then use this ID value to link back to the source image . Now that you 've learned about the facial detection feature of Amazon recognition . Here 's a summary of the guideline we 've discussed so far . When Amazon recognition detects a human face , it capture a bounding box that show where the face wa found in the video . It can also detect attribute such a the position of the eye , nose and mouth . It can detect emotion , the quality of the detection and any landmark that might appear . All these item will have an associated confidence score . A higher score mean that the model ha greater confidence about the detection gender is inferred from the image , not inferred from identity . Similarly , emotion is also determined from the image and it might not reflect the subject 's actual emotional state . How should I apply facial recognition responsibly ? Facial recognition should never be used in a way that violates an individual 's right including the right to privacy . It should also never be used to make autonomous decision for scenario that require a human being to analyze them . For example , suppose that a bank us tool like Amazon recognition in a financial application to verify their customer identity . The bank should always clearly disclose the use of the technology and ask the customer to approve their term and condition . For more information about this topic , see the Aws web page about the fact on facial recognition with artificial intelligence will now explain how you can use Amazon recognition to process video . You can perform video processing on both stored video and video stream , stored video should be uploaded and stored in an S3 bucket . Each type of detection ha it own start operation . You can search for people , face , label , celebrity , text and inappropriate content . Amazon recognition publishes a completion status to a topic in Amazon 's simple notification service which is also known a Amazon S N S. Then S N S can route these message to subscriber for durability . It 's a best practice to route message to a message queue . In Amazon 's simple QE service or Amazon S Q S , your application should monitor the SQS Q for completion . Each start operation ha a corresponding get operation for retrieving the result . If you call get detection result , it return an array of label that contain information about any label found in the video . The label information includes the same label a image detection , but it also includes a time stamp of where the label wa detected in millisecond from the start of the video . In addition to stored video , you can also use Amazon recognition video to detect and recognize face in streaming video . A typical use case for this is detecting a known face in a video stream . Amazon recognition video us Amazon kinesis video stream to receive and process a video stream . The analysis result are output from Amazon recognition video to a kinesis data stream . They are then read by your client application . Amazon recognition video provides a stream processor that 's called create stream processor and you can use it to start and manage the analysis of the streaming video to use Amazon recognition video with your streaming video , your application must implement these resource . First , you need a kinesis video stream to send streaming video to Amazon recognition video . Next , you need an Amazon recognition video stream processor to manage the streaming video analysis . And finally , you need a kinesis data stream consumer to read the analysis result that Amazon recognition video sends to the data stream . If you want to find a face in a video , you need to create a collection . This process is the same a creating a collection for still image . Amazon recognition video place a JSON frame record for each analyzed frame into the kinesis output stream . Amazon recognition video doe n't analyze every frame that 's passed to it through the kinesis video stream . A frame record that 's sent to a kinesis data stream contains information about which video stream fragment the frame is in where the frame is in the fragment and face that are recognized in the frame . It also includes status information for the stream processor before we wrap up . Here 's a quick summary . Amazon recognition is a computer vision service that 's based on deep learning . You can easily add image and video analysis to your application . Amazon recognition can detect face sentiment , text unsafe content and library search in both image and video . Amazon recognition is integrated with other Aws service . Thanks for watching . We 'll see you in the next video\n",
      "['In', 'this', 'section', ',', 'we', \"'ll\", 'look', 'at', 'preparing', 'custom', 'data', 'sets', 'for', 'computer', 'vision', 'so', 'you', 'can', 'detect', 'custom', 'objects', '.', 'One', 'challenge', 'of', 'using', 'a', 'prebuilt', 'model', 'is', 'that', 'it', 'will', 'only', 'find', 'images', 'it', 'was', 'trained', 'to', 'find', '.', 'Though', 'Amazon', 'recognition', 'was', 'trained', 'with', 'tens', 'of', 'millions', 'of', 'images', ',', 'it', 'ca', \"n't\", 'detect', 'objects', 'that', 'it', 'was', \"n't\", 'trained', 'on', '.', 'For', 'example', ',', 'consider', 'the', 'eight', 'of', 'hearts', 'playing', 'card', '.', 'If', 'you', 'run', 'this', 'card', 'through', 'Amazon', 'recognition', ',', 'the', 'results', 'show', 'various', 'attributes', '.', 'However', ',', 'none', 'of', 'the', 'labels', 'are', 'playing', 'card', 'or', 'eight', 'of', 'hearts', '.', 'If', 'you', 'want', 'Amazon', 'recognition', 'to', 'detect', 'images', 'in', 'your', 'problem', 'domain', ',', 'you', 'must', 'train', 'the', 'model', 'with', 'your', 'images', '.', 'So', 'in', 'this', 'section', ',', 'you', \"'ll\", 'learn', 'how', 'to', 'train', 'Amazon', 'recognition', 'with', 'images', 'from', 'your', 'problem', 'domain', '.', 'Though', 'you', \"'ll\", 'focus', 'only', 'on', 'using', 'Amazon', 'recognition', 'here', ',', 'you', \"'ll\", 'encounter', 'a', 'similar', 'process', '.', 'If', 'you', 'use', 'other', 'pre', 'trained', 'models', ',', 'training', 'a', 'computer', 'vision', 'algorithm', 'to', 'recognize', 'images', 'requires', 'a', 'large', 'input', 'data', 'set', 'which', 'is', \"n't\", 'practical', 'for', 'most', 'organizations', '.', 'Many', 'machine', 'learning', 'problems', 'today', 'can', 'be', 'solved', 'by', 'training', 'existing', 'models', '.', 'Or', 'you', 'can', 'use', 'a', 'managed', 'service', 'like', 'Amazon', 'recognition', ',', 'custom', 'labels', 'like', 'other', 'machine', 'learning', 'processes', '.', 'You', 'need', 'to', 'train', 'Amazon', 'recognition', '.', 'So', 'it', 'recognizes', 'scenes', 'and', 'objects', 'that', 'are', 'in', 'a', 'specific', 'domain', '.', 'You', \"'ll\", 'need', 'both', 'a', 'training', 'data', 'set', 'and', 'a', 'test', 'data', 'set', 'that', 'contain', 'labeled', 'images', '.', 'If', 'you', 'have', 'images', 'that', 'need', 'labels', ',', 'you', 'can', 'use', 'Amazon', 'recognition', 'custom', 'labels', 'to', 'simplify', 'your', 'labeling', 'tasks', '.', 'For', 'example', ',', 'it', 'provides', 'a', 'U', 'I', 'for', 'labeling', 'images', 'which', 'includes', 'a', 'feature', 'you', 'can', 'use', 'to', 'draw', 'bounding', 'boxes', 'around', 'images', '.', 'It', 'can', 'also', 'help', 'find', 'objects', 'and', 'scenes', 'that', 'are', 'unique', 'to', 'your', 'business', 'needs', '.', 'You', 'can', 'use', 'it', 'to', 'classify', 'images', 'or', 'detect', 'objects', 'within', 'an', 'image', '.', 'Say', 'you', 'want', 'to', 'identify', 'specific', 'machine', 'parts', 'and', 'images', 'such', 'as', 'turbo', 'chargers', 'or', 'torque', 'converters', '.', 'You', 'could', 'collect', 'pictures', 'of', 'each', 'kind', 'of', 'machine', 'part', 'and', 'use', 'them', 'to', 'train', 'your', 'model', '.', 'Amazon', 'recognition', '.', 'Custom', 'labels', 'also', 'includes', 'automated', 'machine', 'learning', 'capabilities', 'that', 'handle', 'the', 'machine', 'learning', 'process', 'for', 'you', '.', 'When', 'you', 'provide', 'training', 'images', ',', 'the', 'service', 'can', 'automatically', 'load', 'and', 'inspect', 'the', 'data', ',', 'select', 'the', 'correct', 'machine', 'learning', 'algorithms', ',', 'train', 'a', 'model', 'and', 'provide', 'model', 'performance', 'metrics', '.', 'When', 'you', 'finish', 'training', 'your', 'model', ',', 'you', 'can', 'then', 'evaluate', 'your', 'custom', 'models', 'performance', 'on', 'your', 'test', 'set', '.', 'Each', 'image', 'in', 'the', 'test', 'set', 'has', 'a', 'side', 'by', 'side', 'comparison', 'of', 'the', 'model', \"'s\", 'prediction', 'versus', 'the', 'label', 'it', 'assigned', '.', 'There', 'are', 'also', 'detailed', 'performance', 'metrics', 'for', 'you', 'to', 'review', '.', 'You', 'can', 'start', 'using', 'your', 'model', 'immediately', 'for', 'image', 'analysis', 'or', 'you', 'can', 'iterate', 'and', 'retrain', 'new', 'versions', 'with', 'more', 'images', 'to', 'refine', 'the', 'model', '.', 'After', 'you', 'start', 'using', 'your', 'model', ',', 'you', 'can', 'track', 'your', 'predictions', ',', 'correct', 'any', 'mistakes', 'and', 'use', 'the', 'feedback', 'data', 'to', 'retrain', 'new', 'model', 'versions', 'and', 'improve', 'their', 'performance', '.', 'So', 'how', 'do', 'you', 'label', 'images', '?', 'The', 'diagram', 'shows', 'a', 'typical', 'process', 'for', 'training', 'a', 'computer', 'vision', 'model', 'which', 'includes', 'the', 'Amazon', 'recognition', 'custom', 'labels', 'feature', '.', 'We', \"'ll\", 'step', 'through', 'this', 'in', 'some', 'detail', '.', 'The', 'process', 'of', 'developing', 'a', 'custom', 'model', 'to', 'analyze', 'images', 'requires', 'time', ',', 'expertise', 'and', 'resources', '.', 'It', 'often', 'takes', 'months', 'to', 'complete', '.', 'It', 'can', 'also', 'require', 'thousands', 'or', 'tens', 'of', 'thousands', 'of', 'hand', 'labeled', 'images', '.', 'So', 'the', 'model', 'has', 'enough', 'data', 'to', 'make', 'accurate', 'decisions', '.', 'It', 'can', 'take', 'months', 'to', 'generate', 'and', 'gather', 'this', 'data', 'and', 'it', 'can', 'require', 'large', 'teams', 'of', 'labelers', 'to', 'prepare', 'it', 'for', 'use', 'in', 'machine', 'learning', '.', 'Amazon', 'recognition', 'custom', 'labels', 'builds', 'on', 'the', 'existing', 'capabilities', 'of', 'Amazon', 'recognition', 'which', 'is', 'already', 'trained', 'on', 'tens', 'of', 'millions', 'of', 'images', 'across', 'many', 'categories', '.', 'Instead', 'of', 'thousands', 'of', 'images', ',', 'you', 'can', 'upload', 'a', 'small', 'set', 'of', 'training', 'images', 'that', 'are', 'specific', 'to', 'your', 'use', 'case', '.', 'Typically', ',', 'you', \"'d\", 'use', 'a', 'few', '100', 'images', 'for', 'this', '.', 'You', 'can', 'use', 'the', 'AWS', 'management', 'console', 'to', 'upload', 'training', 'images', '.', 'If', 'your', 'images', 'are', 'already', 'labeled', 'Amazon', 'recognition', ',', 'custom', 'labels', 'can', 'begin', 'training', 'your', 'model', '.', 'If', 'they', \"'re\", 'not', ',', 'you', 'can', 'label', 'the', 'images', 'directly', 'in', 'the', 'labeling', 'interface', 'or', 'you', 'can', 'use', 'Amazon', 'Sagemaker', 'ground', 'Truth', 'to', 'label', 'them', 'for', 'you', '.', 'There', \"'ll\", 'be', 'more', 'on', 'that', 'shortly', '.', 'Amazon', 'recognition', 'custom', 'labels', 'works', 'best', 'when', 'you', 'use', 'different', 'models', 'for', 'different', 'domains', '.', 'For', 'example', ',', 'if', 'you', 'need', 'to', 'detect', 'both', 'machine', 'parts', 'and', 'plant', 'health', ',', 'you', \"'d\", 'use', 'two', 'different', 'models', '.', 'Images', 'you', 'select', 'for', 'training', 'should', 'be', 'similar', 'to', 'the', 'images', 'that', 'will', 'be', 'used', 'for', 'inference', 'use', 'images', 'that', 'use', 'various', 'lighting', 'conditions', ',', 'backgrounds', 'and', 'resolutions', '.', 'Ideally', ',', 'your', 'training', 'images', 'will', 'mirror', 'images', '.', 'You', \"'d\", 'want', 'to', 'perform', 'detection', 'on', 'if', 'you', 'can', 'use', 'the', 'same', 'source', 'like', 'you', \"'d\", 'use', 'in', 'production', 'that', 'works', 'best', '.', 'The', 'documentation', 'includes', 'additional', 'guidelines', 'on', 'image', 'type', '.', 'So', 'whether', 'they', 'are', 'J', 'pegs', 'or', 'png', \"'s\", 'and', 'other', 'properties', 'like', 'image', 'size', 'and', 'resolution', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'one', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'two', 'where', 'we', \"'ll\", 'review', 'how', 'to', 'create', 'the', 'training', 'data', 'set', '.']\n",
      "In this section , we 'll look at preparing custom data set for computer vision so you can detect custom object . One challenge of using a prebuilt model is that it will only find image it wa trained to find . Though Amazon recognition wa trained with ten of million of image , it ca n't detect object that it wa n't trained on . For example , consider the eight of heart playing card . If you run this card through Amazon recognition , the result show various attribute . However , none of the label are playing card or eight of heart . If you want Amazon recognition to detect image in your problem domain , you must train the model with your image . So in this section , you 'll learn how to train Amazon recognition with image from your problem domain . Though you 'll focus only on using Amazon recognition here , you 'll encounter a similar process . If you use other pre trained model , training a computer vision algorithm to recognize image requires a large input data set which is n't practical for most organization . Many machine learning problem today can be solved by training existing model . Or you can use a managed service like Amazon recognition , custom label like other machine learning process . You need to train Amazon recognition . So it recognizes scene and object that are in a specific domain . You 'll need both a training data set and a test data set that contain labeled image . If you have image that need label , you can use Amazon recognition custom label to simplify your labeling task . For example , it provides a U I for labeling image which includes a feature you can use to draw bounding box around image . It can also help find object and scene that are unique to your business need . You can use it to classify image or detect object within an image . Say you want to identify specific machine part and image such a turbo charger or torque converter . You could collect picture of each kind of machine part and use them to train your model . Amazon recognition . Custom label also includes automated machine learning capability that handle the machine learning process for you . When you provide training image , the service can automatically load and inspect the data , select the correct machine learning algorithm , train a model and provide model performance metric . When you finish training your model , you can then evaluate your custom model performance on your test set . Each image in the test set ha a side by side comparison of the model 's prediction versus the label it assigned . There are also detailed performance metric for you to review . You can start using your model immediately for image analysis or you can iterate and retrain new version with more image to refine the model . After you start using your model , you can track your prediction , correct any mistake and use the feedback data to retrain new model version and improve their performance . So how do you label image ? The diagram show a typical process for training a computer vision model which includes the Amazon recognition custom label feature . We 'll step through this in some detail . The process of developing a custom model to analyze image requires time , expertise and resource . It often take month to complete . It can also require thousand or ten of thousand of hand labeled image . So the model ha enough data to make accurate decision . It can take month to generate and gather this data and it can require large team of labelers to prepare it for use in machine learning . Amazon recognition custom label build on the existing capability of Amazon recognition which is already trained on ten of million of image across many category . Instead of thousand of image , you can upload a small set of training image that are specific to your use case . Typically , you 'd use a few 100 image for this . You can use the AWS management console to upload training image . If your image are already labeled Amazon recognition , custom label can begin training your model . If they 're not , you can label the image directly in the labeling interface or you can use Amazon Sagemaker ground Truth to label them for you . There 'll be more on that shortly . Amazon recognition custom label work best when you use different model for different domain . For example , if you need to detect both machine part and plant health , you 'd use two different model . Images you select for training should be similar to the image that will be used for inference use image that use various lighting condition , background and resolution . Ideally , your training image will mirror image . You 'd want to perform detection on if you can use the same source like you 'd use in production that work best . The documentation includes additional guideline on image type . So whether they are J peg or png 's and other property like image size and resolution . That 's it . For part one of this section , we 'll see you again for part two where we 'll review how to create the training data set .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'video', 'analysis', 'by', 'reviewing', 'how', 'to', 'create', 'the', 'training', 'data', 'set', '.', 'Data', 'sets', 'contain', 'information', 'that', \"'s\", 'needed', 'to', 'train', 'and', 'test', 'an', 'Amazon', 'recognition', 'custom', 'labels', 'model', 'such', 'as', 'images', ',', 'labels', 'and', 'bounding', 'boxes', '.', 'You', 'can', 'use', 'images', 'from', 'Amazon', 'S3', 'or', 'you', 'can', 'upload', 'them', 'from', 'your', 'computer', 'to', 'S3', 'as', 'part', 'of', 'the', 'process', 'to', 'train', 'a', 'model', '.', 'Your', 'data', 'set', 'should', 'have', 'at', 'least', 'two', 'labels', 'with', 'at', 'least', '10', 'images', 'per', 'label', '.', 'Each', 'image', 'in', 'your', 'data', 'set', 'must', 'be', 'labeled', 'as', 'we', 'mentioned', 'earlier', '.', 'You', 'can', 'use', 'the', 'Amazon', 'recognition', 'custom', 'labels', 'console', 'or', 'Amazon', 'Sagemaker', 'Ground', 'Truth', 'to', 'label', 'your', 'images', 'again', 'to', 'train', 'an', 'Amazon', 'recognition', 'custom', 'labels', 'model', '.', 'Your', 'images', 'must', 'be', 'labeled', 'a', 'label', 'indicates', 'that', 'an', 'image', 'contains', 'an', 'object', 'scene', 'or', 'concept', '.', 'As', 'we', 'mentioned', 'earlier', ',', 'a', 'data', 'set', 'needs', 'at', 'least', 'two', 'defined', 'labels', '.', 'Also', ',', 'each', 'image', 'must', 'have', 'at', 'least', 'one', 'assigned', 'label', 'that', 'identifies', 'the', 'object', 'scene', 'or', 'concept', 'in', 'the', 'image', '.', 'When', 'you', 'apply', 'labels', 'to', 'an', 'image', 'as', 'a', 'whole', ',', 'these', 'labels', 'are', 'known', 'as', 'image', 'level', 'labels', '.', 'They', \"'re\", 'useful', 'for', 'identifying', 'scenes', 'or', 'concepts', 'that', 'you', 'want', 'to', 'detect', '.', 'For', 'example', ',', 'one', 'of', 'the', 'images', 'shows', 'a', 'beach', 'scene', 'from', 'Ko', 'Olena', '.', 'It', \"'s\", 'on', 'the', 'island', 'of', 'Oahu', 'in', 'the', 'US', 'state', 'of', 'Hawaii', 'to', 'train', 'a', 'model', 'to', 'detect', 'beaches', ',', 'you', \"'d\", 'add', 'a', 'beach', 'label', 'that', 'applies', 'to', 'the', 'entire', 'image', '.', 'You', 'can', 'also', 'apply', 'labels', 'to', 'specific', 'areas', 'of', 'an', 'image', 'that', 'contain', 'an', 'object', 'you', 'want', 'to', 'detect', '.', 'For', 'example', ',', 'if', 'you', 'want', 'your', 'model', 'to', 'detect', 'Amazon', 'echo', 'devices', ',', 'it', 'must', 'identify', 'the', 'different', 'types', 'of', 'echo', 'devices', 'in', 'an', 'image', '.', 'The', 'model', 'needs', 'information', 'about', 'where', 'the', 'devices', 'are', 'located', 'in', 'the', 'image', 'and', 'it', 'needs', 'a', 'corresponding', 'label', 'that', 'identifies', 'the', 'type', 'of', 'the', 'device', '.', 'This', 'information', 'is', 'known', 'as', 'localization', 'information', '.', 'The', 'location', 'of', 'the', 'device', 'is', 'expressed', 'as', 'a', 'bounding', 'box', '.', 'The', 'example', 'objects', 'with', 'bounding', 'boxes', ',', 'image', 'shows', 'a', 'bounding', 'box', 'that', 'surrounds', 'an', 'Amazon', 'echo', 'dot', 'The', 'image', 'also', 'contains', 'an', 'Amazon', 'echo', 'without', 'a', 'bounding', 'box', '.', 'The', 'output', 'of', 'the', 'labeling', 'process', 'will', 'be', 'a', 'manifest', 'file', '.', 'The', 'manifest', 'file', 'for', 'an', 'image', 'level', 'label', 'typically', 'contains', 'the', 'label', 'or', 'class', 'name', 'along', 'with', 'some', 'metadata', 'about', 'how', 'the', 'image', 'was', 'labeled', 'for', 'object', 'detection', '.', 'The', 'manifest', 'contains', 'information', 'about', 'each', 'labeled', 'image', '.', 'The', 'bounding', 'box', 'identifies', 'where', 'the', 'object', 'is', 'in', 'the', 'image', 'along', 'with', 'the', 'label', 'that', 'the', 'bounding', 'box', 'belongs', 'to', '.', 'We', \"'ve\", 'mentioned', 'Amazon', 'Sagemaker', 'ground', 'truth', 'a', 'few', 'times', 'we', 'now', 'look', 'at', 'what', 'it', 'is', 'and', 'how', 'it', 'might', 'help', 'you', 'with', 'Sagemaker', 'Ground', 'Truth', ',', 'you', 'can', 'build', 'high', 'quality', 'training', 'data', 'sets', 'for', 'your', 'machine', 'learning', 'models', 'to', 'use', 'it', ',', 'create', 'a', 'data', 'set', 'that', 'needs', 'labeling', '.', 'You', 'then', 'provide', 'detailed', 'instructions', 'on', 'what', 'needs', 'to', 'be', 'labeled', 'and', 'submit', 'the', 'job', '.', 'You', 'can', 'decide', 'who', 'processes', 'the', 'images', 'to', 'create', 'a', 'label', 'data', 'set', '.', 'You', 'can', 'use', 'workers', 'from', 'the', 'Amazon', 'Mechanical', 'Turk', 'service', ',', 'a', 'vendor', 'company', 'or', 'an', 'internal', 'workforce', 'with', 'machine', 'learning', '.', 'You', 'can', 'use', 'the', 'label', 'dataset', 'output', 'from', 'Sagemaker', 'Ground', 'Truth', 'to', 'train', 'your', 'own', 'models', 'or', 'you', 'can', 'also', 'use', 'it', 'with', 'Amazon', 'recognition', 'custom', 'labels', '.', 'Sagemaker', 'Ground', 'Truth', 'can', 'use', 'active', 'learning', 'to', 'automate', 'the', 'labeling', 'of', 'your', 'input', 'data', '.', 'Active', 'learning', 'is', 'a', 'machine', 'learning', 'technique', 'that', 'identifies', 'data', 'that', 'should', 'be', 'labeled', 'by', 'your', 'workers', 'in', 'Sagemaker', 'Ground', 'Truth', '.', 'This', 'functionality', 'is', 'called', 'automated', 'data', 'labeling', '.', 'Automated', 'data', 'labeling', 'can', 'reduce', 'the', 'time', 'and', 'cost', '.', 'It', 'takes', 'to', 'label', 'your', 'data', 'set', 'compared', 'to', 'using', 'only', 'human', 'workers', '.', 'When', 'you', 'use', 'automated', 'labeling', ',', 'you', 'incur', 'Amazon', 'sagemaker', 'training', 'and', 'inference', 'costs', '.', 'Yes', ',', 'we', 'just', 'said', 'that', 'you', 'can', 'use', 'machine', 'learning', 'to', 'label', 'the', 'images', 'that', 'you', \"'ll\", 'then', 'use', 'for', 'machine', 'learning', '.', 'We', \"'ll\", 'talk', 'through', 'how', 'this', 'works', '.', 'When', 'Sagemaker', 'Ground', 'Truth', 'starts', 'an', 'automated', 'data', 'labeling', 'job', ',', 'it', 'selects', 'a', 'random', 'sample', 'of', 'input', 'data', 'or', 'objects', 'and', 'sends', 'it', 'to', 'human', 'workers', 'when', 'the', 'label', 'data', 'is', 'returned', '.', 'Sagemaker', 'ground', 'Truth', 'uses', 'this', 'data', 'which', 'is', 'the', 'validation', 'data', 'to', 'validate', 'the', 'models', 'that', 'were', 'trained', 'for', 'automated', 'data', 'labeling', '.', 'Sagemaker', '.', 'Ground', 'truth', 'runs', 'a', 'batch', 'transform', 'job', 'using', 'the', 'validated', 'model', 'for', 'inference', 'on', 'the', 'validation', 'data', '.', 'Batch', 'inference', 'produces', 'a', 'confidence', 'score', 'and', 'quality', 'metric', 'for', 'each', 'object', '.', 'In', 'the', 'validation', 'data', ',', 'automated', 'labeling', 'determines', 'if', 'the', 'confidence', 'score', 'for', 'each', 'object', 'which', 'was', 'produced', 'in', 'step', 'five', 'meets', 'the', 'required', 'threshold', ',', 'which', 'was', 'determined', 'in', 'step', 'four', '.', 'If', 'the', 'confidence', 'score', 'meets', 'the', 'threshold', ',', 'the', 'expected', 'quality', 'of', 'automatic', 'labeling', 'exceeds', 'the', 'requested', 'level', 'of', 'accuracy', '.', 'The', 'object', 'is', 'then', 'considered', 'to', 'be', 'automatically', 'labeled', '.', 'Step', 'six', 'produces', 'a', 'data', 'set', 'of', 'unlabeled', 'data', 'with', 'confidence', 'scores', '.', 'Sagemaker', 'ground', 'truth', 'selects', 'data', 'points', 'with', 'low', 'confidence', 'scores', 'from', 'this', 'data', 'set', 'and', 'sends', 'them', 'to', 'human', 'workers', 'for', 'additional', 'labeling', '.', 'Sagemaker', 'ground', 'truth', 'then', 'uses', 'the', 'existing', 'human', 'label', 'data', 'and', 'the', 'additional', 'human', 'label', 'data', 'to', 'train', 'a', 'new', 'model', '.', 'The', 'process', 'is', 'repeated', 'until', 'the', 'data', 'set', 'is', 'fully', 'labeled', 'or', 'until', 'another', 'stopping', 'condition', 'is', 'met', '.', 'For', 'example', ',', 'automatic', 'labeling', 'can', 'stop', 'when', 'you', 'meet', 'your', 'budget', 'for', 'human', 'annotation', '.', 'We', 'recommend', 'using', 'automated', 'data', 'labeling', 'on', 'large', 'data', 'sets', '.', 'The', 'minimum', 'number', 'of', 'objects', 'allowed', 'for', 'automated', 'data', 'labeling', 'is', '1250', '.', 'However', ',', 'we', 'strongly', 'suggest', 'providing', 'a', 'minimum', 'of', '5000', 'objects', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'two', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'three', 'where', 'we', \"'ll\", 'review', 'how', 'to', 'evaluate', 'and', 'improve', 'your', 'model', '.']\n",
      "Hi , welcome back . We 'll continue exploring video analysis by reviewing how to create the training data set . Data set contain information that 's needed to train and test an Amazon recognition custom label model such a image , label and bounding box . You can use image from Amazon S3 or you can upload them from your computer to S3 a part of the process to train a model . Your data set should have at least two label with at least 10 image per label . Each image in your data set must be labeled a we mentioned earlier . You can use the Amazon recognition custom label console or Amazon Sagemaker Ground Truth to label your image again to train an Amazon recognition custom label model . Your image must be labeled a label indicates that an image contains an object scene or concept . As we mentioned earlier , a data set need at least two defined label . Also , each image must have at least one assigned label that identifies the object scene or concept in the image . When you apply label to an image a a whole , these label are known a image level label . They 're useful for identifying scene or concept that you want to detect . For example , one of the image show a beach scene from Ko Olena . It 's on the island of Oahu in the US state of Hawaii to train a model to detect beach , you 'd add a beach label that applies to the entire image . You can also apply label to specific area of an image that contain an object you want to detect . For example , if you want your model to detect Amazon echo device , it must identify the different type of echo device in an image . The model need information about where the device are located in the image and it need a corresponding label that identifies the type of the device . This information is known a localization information . The location of the device is expressed a a bounding box . The example object with bounding box , image show a bounding box that surround an Amazon echo dot The image also contains an Amazon echo without a bounding box . The output of the labeling process will be a manifest file . The manifest file for an image level label typically contains the label or class name along with some metadata about how the image wa labeled for object detection . The manifest contains information about each labeled image . The bounding box identifies where the object is in the image along with the label that the bounding box belongs to . We 've mentioned Amazon Sagemaker ground truth a few time we now look at what it is and how it might help you with Sagemaker Ground Truth , you can build high quality training data set for your machine learning model to use it , create a data set that need labeling . You then provide detailed instruction on what need to be labeled and submit the job . You can decide who process the image to create a label data set . You can use worker from the Amazon Mechanical Turk service , a vendor company or an internal workforce with machine learning . You can use the label dataset output from Sagemaker Ground Truth to train your own model or you can also use it with Amazon recognition custom label . Sagemaker Ground Truth can use active learning to automate the labeling of your input data . Active learning is a machine learning technique that identifies data that should be labeled by your worker in Sagemaker Ground Truth . This functionality is called automated data labeling . Automated data labeling can reduce the time and cost . It take to label your data set compared to using only human worker . When you use automated labeling , you incur Amazon sagemaker training and inference cost . Yes , we just said that you can use machine learning to label the image that you 'll then use for machine learning . We 'll talk through how this work . When Sagemaker Ground Truth start an automated data labeling job , it selects a random sample of input data or object and sends it to human worker when the label data is returned . Sagemaker ground Truth us this data which is the validation data to validate the model that were trained for automated data labeling . Sagemaker . Ground truth run a batch transform job using the validated model for inference on the validation data . Batch inference produce a confidence score and quality metric for each object . In the validation data , automated labeling determines if the confidence score for each object which wa produced in step five meet the required threshold , which wa determined in step four . If the confidence score meet the threshold , the expected quality of automatic labeling exceeds the requested level of accuracy . The object is then considered to be automatically labeled . Step six produce a data set of unlabeled data with confidence score . Sagemaker ground truth selects data point with low confidence score from this data set and sends them to human worker for additional labeling . Sagemaker ground truth then us the existing human label data and the additional human label data to train a new model . The process is repeated until the data set is fully labeled or until another stopping condition is met . For example , automatic labeling can stop when you meet your budget for human annotation . We recommend using automated data labeling on large data set . The minimum number of object allowed for automated data labeling is 1250 . However , we strongly suggest providing a minimum of 5000 object . That 's it . For part two of this section , we 'll see you again for part three where we 'll review how to evaluate and improve your model .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'video', 'analysis', 'by', 'reviewing', 'how', 'to', 'create', 'the', 'test', 'data', 'set', '.', 'The', 'final', 'step', 'before', 'you', 'train', 'your', 'model', 'is', 'to', 'identify', 'a', 'test', 'data', 'set', '.', 'You', 'will', 'use', 'this', 'test', 'data', 'set', 'to', 'validate', 'and', 'evaluate', 'the', 'model', \"'s\", 'performance', '.', 'You', \"'ll\", 'do', 'this', 'by', 'performing', 'an', 'inference', 'on', 'the', 'images', 'in', 'the', 'test', 'data', 'set', '.', 'You', \"'ll\", 'then', 'compare', 'the', 'results', 'with', 'the', 'labeling', 'information', 'that', \"'s\", 'in', 'the', 'training', 'data', 'set', '.', 'You', 'can', 'create', 'your', 'own', 'test', 'data', 'set', '.', 'Alternatively', ',', 'you', 'can', 'use', 'Amazon', 'recognition', 'custom', 'labels', 'to', 'split', 'your', 'training', 'data', 'set', 'into', 'two', 'data', 'sets', '.', 'By', 'using', 'an', '80', '20', 'split', '.', 'This', 'split', 'means', 'that', '80', '%', 'of', 'the', 'data', 'is', 'used', 'for', 'training', 'and', '20', '%', 'is', 'used', 'for', 'testing', '.', 'After', 'you', 'define', 'the', 'training', 'and', 'test', 'data', 'sets', '.', 'Amazon', 'recognition', 'custom', 'labels', 'can', 'automatically', 'train', 'the', 'model', 'for', 'you', '.', 'The', 'service', 'automatically', 'loads', 'and', 'inspects', 'the', 'data', ',', 'selects', 'the', 'correct', 'machine', 'learning', 'algorithms', 'trains', 'a', 'model', 'and', 'provides', 'model', 'performance', 'metrics', 'you', \"'re\", 'charged', 'for', 'the', 'amount', 'of', 'time', 'a', 'model', 'takes', 'to', 'train', 'a', 'data', 'set', 'that', 'contains', 'more', 'images', 'and', 'labels', 'will', 'take', 'longer', 'to', 'train', 'when', 'training', 'is', 'complete', '.', 'You', 'evaluate', 'the', 'performance', 'of', 'the', 'model', '.', 'During', 'testing', ',', 'Amazon', 'recognition', ',', 'custom', 'labels', 'predicts', 'if', 'a', 'test', 'image', 'contains', 'a', 'custom', 'label', '.', 'The', 'confidence', 'score', 'is', 'a', 'value', 'that', 'quantifies', 'the', 'certainty', 'of', 'the', 'model', \"'s\", 'prediction', '.', 'Because', 'this', 'is', 'a', 'classification', 'problem', ',', 'the', 'results', 'can', 'be', 'mapped', 'to', 'a', 'confusion', 'matrix', 'with', 'a', 'true', 'positive', '.', 'The', 'model', 'correctly', 'predicts', 'the', 'presence', 'of', 'the', 'custom', 'label', 'in', 'the', 'test', 'image', '.', 'That', 'is', 'the', 'predicted', 'label', 'is', 'also', 'a', 'ground', 'truth', 'label', 'for', 'that', 'image', '.', 'For', 'example', ',', 'Amazon', 'recognition', 'custom', 'labels', 'correctly', 'returns', 'a', 'cat', 'label', '.', 'When', 'a', 'cat', 'is', 'present', 'in', 'an', 'image', 'for', 'a', 'false', 'positive', ',', 'the', 'model', 'incorrectly', 'predicts', 'the', 'presence', 'of', 'a', 'custom', 'label', 'in', 'a', 'test', 'image', '.', 'That', 'is', 'the', 'predicted', 'label', 'is', \"n't\", 'a', 'ground', 'truth', 'label', 'for', 'the', 'image', '.', 'For', 'example', ',', 'Amazon', 'recognition', 'custom', 'labels', 'returns', 'a', 'cat', 'label', 'but', 'there', \"'s\", 'no', 'cat', 'label', 'in', 'the', 'ground', 'truth', 'for', 'that', 'image', 'for', 'a', 'false', 'negative', '.', 'The', 'model', 'does', \"n't\", 'predict', 'that', 'a', 'custom', 'label', 'is', 'present', 'in', 'the', 'image', '.', 'But', 'the', 'ground', 'truth', 'for', 'that', 'image', 'includes', 'this', 'label', '.', 'For', 'example', ',', 'Amazon', 'recognition', 'custom', 'labels', 'does', \"n't\", 'return', 'a', 'cat', 'custom', 'label', 'for', 'an', 'image', 'that', 'contains', 'a', 'cat', 'with', 'a', 'true', 'negative', '.', 'The', 'model', 'correctly', 'predicts', 'that', 'a', 'custom', 'label', 'is', \"n't\", 'present', 'in', 'the', 'test', 'image', '.', 'For', 'example', ',', 'Amazon', 'recognition', 'custom', 'labels', 'does', \"n't\", 'return', 'a', 'cat', 'label', 'for', 'an', 'image', 'that', 'does', \"n't\", 'contain', 'a', 'cat', '.', 'The', 'console', 'provides', 'access', 'to', 'true', 'positive', ',', 'false', 'positive', 'and', 'false', 'negative', 'values', 'for', 'each', 'image', 'in', 'your', 'test', 'data', 'set', '.', 'These', 'prediction', 'results', 'are', 'used', 'to', 'calculate', 'the', 'various', 'metrics', 'for', 'each', 'label', 'and', 'an', 'aggregate', 'of', 'metrics', 'for', 'your', 'entire', 'test', 'set', '.', 'The', 'same', 'definitions', 'apply', 'to', 'predictions', 'that', 'the', 'model', 'makes', 'at', 'the', 'bounding', 'box', 'level', '.', 'With', 'bounding', 'boxes', ',', 'all', 'metrics', 'are', 'calculated', 'over', 'each', 'bounding', 'box', 'in', 'each', 'test', 'image', 'regardless', 'of', 'whether', 'the', 'boxes', 'are', 'prediction', 'or', 'ground', 'truth', '.', 'To', 'help', 'you', ',', 'Amazon', 'recognition', 'custom', 'labels', 'provides', 'various', 'metrics', '.', 'For', 'example', ',', 'you', 'can', 'view', 'summary', 'metrics', 'and', 'evaluation', 'metrics', 'for', 'each', 'label', '.', 'It', 'also', 'provides', 'precision', 'metrics', 'for', 'each', 'label', 'and', 'an', 'average', 'precision', 'metric', 'for', 'the', 'entire', 'test', 'data', 'set', 'precision', 'is', 'the', 'proportion', 'of', 'positive', 'results', 'that', 'were', 'correctly', 'classified', '.', 'Amazon', 'recognition', '.', 'Custom', 'labels', 'provides', 'average', 'recall', 'metrics', 'for', 'each', 'label', 'and', 'an', 'average', 'recall', 'metric', 'for', 'the', 'entire', 'test', 'data', 'set', 'recall', 'is', 'the', 'fraction', 'of', 'your', 'test', 'set', 'labels', 'that', 'were', 'correctly', 'classified', 'using', 'the', 'previous', 'example', 'of', 'cats', '.', 'That', 'would', 'be', 'how', 'many', 'cats', 'were', 'correctly', 'classified', '.', 'The', 'service', 'also', 'provides', 'an', 'average', 'model', 'performance', 'score', 'for', 'each', 'label', 'and', 'an', 'average', 'model', 'performance', 'score', 'for', 'the', 'entire', 'test', 'data', 'set', '.', 'The', 'F', 'one', 'score', 'combines', 'precision', 'and', 'recall', 'together', 'to', 'give', 'you', 'just', 'one', 'number', 'that', 'quantifies', 'the', 'overall', 'performance', 'of', 'a', 'particular', 'machine', 'learning', 'algorithm', '.', 'You', 'might', 'use', 'the', 'F', 'one', 'score', 'when', 'you', 'have', 'a', 'class', 'imbalance', ',', 'but', 'you', 'also', 'want', 'to', 'preserve', 'the', 'equality', 'between', 'precision', 'and', 'sensitivity', 'a', 'higher', 'value', 'means', 'better', 'model', 'performance', 'for', 'both', 'recall', 'and', 'precision', '.', 'If', 'you', \"'re\", 'satisfied', 'with', 'the', 'accuracy', 'of', 'your', 'model', ',', 'you', 'can', 'start', 'using', 'it', '.', 'That', \"'s\", 'it', '.', 'For', 'part', 'three', 'of', 'this', 'section', ',', 'we', \"'ll\", 'see', 'you', 'again', 'for', 'part', 'four', 'where', 'we', \"'ll\", 'review', 'how', 'to', 'evaluate', 'and', 'improve', 'your', 'model', '.']\n",
      "Hi , welcome back . We 'll continue exploring video analysis by reviewing how to create the test data set . The final step before you train your model is to identify a test data set . You will use this test data set to validate and evaluate the model 's performance . You 'll do this by performing an inference on the image in the test data set . You 'll then compare the result with the labeling information that 's in the training data set . You can create your own test data set . Alternatively , you can use Amazon recognition custom label to split your training data set into two data set . By using an 80 20 split . This split mean that 80 % of the data is used for training and 20 % is used for testing . After you define the training and test data set . Amazon recognition custom label can automatically train the model for you . The service automatically load and inspects the data , selects the correct machine learning algorithm train a model and provides model performance metric you 're charged for the amount of time a model take to train a data set that contains more image and label will take longer to train when training is complete . You evaluate the performance of the model . During testing , Amazon recognition , custom label predicts if a test image contains a custom label . The confidence score is a value that quantifies the certainty of the model 's prediction . Because this is a classification problem , the result can be mapped to a confusion matrix with a true positive . The model correctly predicts the presence of the custom label in the test image . That is the predicted label is also a ground truth label for that image . For example , Amazon recognition custom label correctly return a cat label . When a cat is present in an image for a false positive , the model incorrectly predicts the presence of a custom label in a test image . That is the predicted label is n't a ground truth label for the image . For example , Amazon recognition custom label return a cat label but there 's no cat label in the ground truth for that image for a false negative . The model doe n't predict that a custom label is present in the image . But the ground truth for that image includes this label . For example , Amazon recognition custom label doe n't return a cat custom label for an image that contains a cat with a true negative . The model correctly predicts that a custom label is n't present in the test image . For example , Amazon recognition custom label doe n't return a cat label for an image that doe n't contain a cat . The console provides access to true positive , false positive and false negative value for each image in your test data set . These prediction result are used to calculate the various metric for each label and an aggregate of metric for your entire test set . The same definition apply to prediction that the model make at the bounding box level . With bounding box , all metric are calculated over each bounding box in each test image regardless of whether the box are prediction or ground truth . To help you , Amazon recognition custom label provides various metric . For example , you can view summary metric and evaluation metric for each label . It also provides precision metric for each label and an average precision metric for the entire test data set precision is the proportion of positive result that were correctly classified . Amazon recognition . Custom label provides average recall metric for each label and an average recall metric for the entire test data set recall is the fraction of your test set label that were correctly classified using the previous example of cat . That would be how many cat were correctly classified . The service also provides an average model performance score for each label and an average model performance score for the entire test data set . The F one score combine precision and recall together to give you just one number that quantifies the overall performance of a particular machine learning algorithm . You might use the F one score when you have a class imbalance , but you also want to preserve the equality between precision and sensitivity a higher value mean better model performance for both recall and precision . If you 're satisfied with the accuracy of your model , you can start using it . That 's it . For part three of this section , we 'll see you again for part four where we 'll review how to evaluate and improve your model .\n",
      "['Hi', ',', 'welcome', 'back', '.', 'We', \"'ll\", 'continue', 'exploring', 'video', 'analysis', 'by', 'reviewing', 'how', 'to', 'evaluate', 'and', 'improve', 'your', 'model', 'in', 'general', '.', 'You', 'can', 'improve', 'the', 'quality', 'of', 'your', 'model', 'with', 'larger', 'quantities', 'of', 'better', 'quality', 'data', '.', 'Use', 'training', 'images', 'that', 'clearly', 'show', 'the', 'object', 'or', 'seen', '.', 'And', 'do', \"n't\", 'include', 'many', 'things', 'that', 'you', \"'re\", 'not', 'interested', 'in', '.', 'For', 'bounding', 'boxes', 'around', 'objects', ',', 'use', 'training', 'images', 'that', 'show', 'the', 'object', 'as', 'fully', 'visible', 'and', 'not', 'hidden', 'by', 'other', 'objects', '.', 'Make', 'sure', 'that', 'your', 'training', 'and', 'test', 'data', 'sets', 'match', 'the', 'type', 'of', 'images', 'that', 'you', \"'ll\", 'eventually', 'run', 'inference', 'on', 'for', 'objects', 'where', 'you', 'have', 'just', 'a', 'few', 'training', 'examples', 'like', 'logos', ',', 'you', 'should', 'provide', 'bounding', 'boxes', 'around', 'the', 'logo', 'in', 'your', 'test', 'images', '.', 'These', 'images', 'represent', 'the', 'scenarios', 'you', 'want', 'to', 'localize', 'the', 'object', 'in', 'reducing', 'false', 'positives', ',', 'often', 'results', 'in', 'better', 'precision', 'to', 'reduce', 'false', 'positives', '.', 'First', 'check', 'if', 'increasing', 'the', 'confidence', 'threshold', 'enables', 'you', 'to', 'keep', 'the', 'correct', 'predictions', 'while', 'eliminating', 'false', 'positives', ',', 'increasing', 'the', 'confidence', 'threshold', 'eventually', 'results', 'in', 'diminishing', 'gains', 'because', 'of', 'the', 'trade', 'off', 'between', 'precision', 'and', 'recall', 'for', 'a', 'given', 'model', '.', 'Next', 'check', 'to', 'see', 'if', 'you', 'need', 'to', 'add', 'additional', 'classes', 'for', 'training', '.', 'For', 'example', ',', 'if', 'you', 'are', 'detecting', 'cats', 'but', 'often', 'dogs', 'are', 'being', 'flagged', 'as', 'cats', 'add', 'dog', 'as', 'a', 'label', 'to', 'your', 'training', 'data', 'set', 'along', 'with', 'the', 'images', 'of', 'dogs', 'that', 'you', 'got', 'the', 'false', 'positive', 'on', 'effectively', '.', 'You', \"'re\", 'helping', 'the', 'model', 'learn', 'to', 'predict', 'dog', 'and', 'not', 'cat', 'through', 'the', 'new', 'training', 'images', '.', 'You', 'might', 'find', 'that', 'the', 'model', 'is', 'confused', 'between', 'two', 'of', 'your', 'custom', 'labels', ',', 'cat', 'and', 'dog', '.', 'The', 'test', 'image', 'with', 'label', 'cat', 'is', 'predicted', 'as', 'having', 'labeled', 'dog', 'and', 'vice', 'versa', '.', 'In', 'this', 'case', ',', 'first', 'check', 'from', 'mislabeled', 'images', 'in', 'your', 'training', 'and', 'test', 'sets', '.', 'Also', 'adding', 'more', 'training', 'images', 'that', 'reflect', 'this', 'confusion', 'will', 'help', 'a', 'retrained', 'model', 'learn', 'to', 'better', 'discriminate', 'between', 'cat', 'and', 'dog', '.', 'Reducing', 'false', 'negatives', ',', 'often', 'results', 'in', 'better', 'recall', 'to', 'reduce', 'false', 'negatives', '.', 'First', ',', 'lower', 'the', 'confidence', 'threshold', '.', 'This', 'should', 'improve', 'recall', 'also', 'use', 'better', 'examples', 'to', 'model', 'the', 'variety', 'of', 'both', 'the', 'object', 'and', 'the', 'images', 'they', 'appear', 'in', '.', 'Finally', 'split', 'your', 'label', 'into', 'two', 'classes', 'that', 'are', 'easier', 'to', 'learn', '.', 'For', 'example', ',', 'instead', 'of', 'good', 'cookies', 'and', 'bad', 'cookies', ',', 'you', 'might', 'want', 'good', 'cookies', ',', 'burnt', 'cookies', 'and', 'broken', 'cookies', 'to', 'help', 'the', 'model', 'learn', 'each', 'unique', 'concept', 'better', '.', 'If', 'you', \"'re\", 'satisfied', 'with', 'the', 'performance', 'of', 'your', 'model', ',', 'you', 'can', 'make', 'it', 'available', 'for', 'use', 'by', 'starting', 'it', 'from', 'the', 'console', 'or', 'by', 'using', 'code', '.', 'After', 'the', 'model', 'is', 'running', ',', 'you', 'can', 'perform', 'an', 'inference', 'with', 'the', 'AWS', 'CLI', 'or', 'the', 'SDK', '.', 'When', 'you', 'call', 'the', 'API', 'you', 'specify', 'the', 'Amazon', 'resource', 'name', 'of', 'the', 'Amazon', 'recognition', 'custom', 'labels', 'model', 'that', 'you', 'want', 'to', 'use', '.', 'The', 'Amazon', 'resource', 'name', 'is', 'also', 'known', 'as', 'an', 'A', 'R', 'N.', 'You', \"'ll\", 'also', 'specify', 'the', 'image', 'you', 'want', 'the', 'model', 'to', 'make', 'a', 'prediction', 'with', '.', 'You', 'can', 'provide', 'an', 'input', 'image', 'as', 'an', 'image', 'byte', 'array', 'of', 'base', '64', 'encoded', 'image', 'bytes', 'or', 'as', 'an', 'S3', 'object', '.', 'Custom', 'labels', 'are', 'returned', 'in', 'an', 'array', 'of', 'custom', 'label', 'objects', '.', 'Each', 'custom', 'label', 'represents', 'a', 'single', 'object', 'seen', 'or', 'concept', 'that', \"'s\", 'found', 'in', 'the', 'image', '.', 'A', 'custom', 'label', 'includes', 'a', 'label', 'for', 'the', 'object', 'scene', 'or', 'concept', 'that', 'was', 'found', 'in', 'the', 'image', '.', 'It', 'also', 'includes', 'a', 'bounding', 'box', 'for', 'objects', 'that', 'were', 'found', 'in', 'the', 'image', '.', 'The', 'bounding', 'box', 'coordinates', 'show', 'where', 'the', 'object', 'is', 'located', 'on', 'the', 'source', 'image', '.', 'The', 'coordinate', 'values', 'are', 'a', 'ratio', 'of', 'the', 'overall', 'image', 'size', '.', 'Finally', ',', 'the', 'custom', 'label', 'includes', 'the', 'confidence', 'score', '.', 'This', 'represents', 'how', 'confident', 'Amazon', 'recognition', 'custom', 'labels', 'is', 'in', 'the', 'accuracy', 'of', 'the', 'label', 'and', 'bounding', 'box', '.', 'During', 'training', ',', 'a', 'model', 'calculates', 'a', 'threshold', 'value', 'that', 'determines', 'if', 'a', 'prediction', 'for', 'a', 'label', 'is', 'true', '.', 'By', 'default', ',', 'the', 'detect', 'custom', 'labels', 'operation', 'does', \"n't\", 'return', 'labels', 'with', 'a', 'confidence', 'value', '.', 'That', \"'s\", 'less', 'than', 'the', 'model', \"'s\", 'calculated', 'threshold', 'value', '.', 'To', 'filter', 'the', 'return', 'labels', 'specify', 'a', 'value', 'for', 'min', 'confidence', 'that', \"'s\", 'greater', 'than', 'the', 'model', \"'s\", 'calculated', 'threshold', '.', 'You', 'can', 'get', 'the', 'model', \"'s\", 'calculated', 'threshold', 'from', 'the', 'model', \"'s\", 'training', 'results', 'in', 'the', 'Amazon', 'recognition', 'custom', 'labels', 'console', 'to', 'get', 'all', 'the', 'labels', 'regardless', 'of', 'confidence', ',', 'specify', 'a', 'min', 'confidence', 'value', 'of', 'zero', '.', 'If', 'you', 'find', 'that', 'the', 'confidence', 'values', 'returned', 'by', 'the', 'detect', 'custom', 'labels', 'operation', 'are', 'too', 'low', '.', 'Consider', 'retraining', 'the', 'model', ',', 'you', 'can', 'restrict', 'the', 'number', 'of', 'custom', 'labels', 'that', 'are', 'returned', 'from', 'the', 'detect', 'custom', 'labels', 'operation', 'by', 'specifying', 'the', 'max', 'results', ',', 'input', 'parameter', '.', 'The', 'returned', 'results', 'are', 'sorted', 'from', 'the', 'highest', 'confidence', 'to', 'the', 'lowest', 'confidence', '.', 'Here', 'are', 'some', 'key', 'takeaways', 'from', 'this', 'section', 'of', 'the', 'module', 'models', 'must', 'be', 'trained', 'for', 'the', 'specific', 'domain', 'that', 'you', 'want', 'to', 'analyze', '.', 'If', 'you', \"'re\", 'looking', 'for', 'turbocharger', ',', 'you', \"'ll\", 'need', 'many', 'pictures', 'of', 'turbocharger', 'to', 'train', 'your', 'model', '.', 'You', 'can', 'set', 'custom', 'labeling', 'for', 'the', 'specific', 'business', 'case', '.', 'We', 'looked', 'at', 'the', 'custom', 'labeling', 'process', 'and', 'some', 'of', 'the', 'tools', 'you', 'can', 'use', '.', 'If', 'you', 'want', 'objects', 'to', 'be', 'detected', ',', 'you', 'need', 'to', 'label', 'images', 'and', 'create', 'bounding', 'boxes', 'for', 'these', 'objects', '.', 'You', 'can', 'use', 'Amazon', 'Sagemaker', 'Ground', 'Truth', 'to', 'build', 'training', 'data', 'sets', 'for', 'your', 'models', ',', 'which', 'can', 'also', 'use', 'machine', 'learning', 'to', 'label', 'your', 'images', '.', 'Thanks', 'for', 'watching', 'and', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Hi , welcome back . We 'll continue exploring video analysis by reviewing how to evaluate and improve your model in general . You can improve the quality of your model with larger quantity of better quality data . Use training image that clearly show the object or seen . And do n't include many thing that you 're not interested in . For bounding box around object , use training image that show the object a fully visible and not hidden by other object . Make sure that your training and test data set match the type of image that you 'll eventually run inference on for object where you have just a few training example like logo , you should provide bounding box around the logo in your test image . These image represent the scenario you want to localize the object in reducing false positive , often result in better precision to reduce false positive . First check if increasing the confidence threshold enables you to keep the correct prediction while eliminating false positive , increasing the confidence threshold eventually result in diminishing gain because of the trade off between precision and recall for a given model . Next check to see if you need to add additional class for training . For example , if you are detecting cat but often dog are being flagged a cat add dog a a label to your training data set along with the image of dog that you got the false positive on effectively . You 're helping the model learn to predict dog and not cat through the new training image . You might find that the model is confused between two of your custom label , cat and dog . The test image with label cat is predicted a having labeled dog and vice versa . In this case , first check from mislabeled image in your training and test set . Also adding more training image that reflect this confusion will help a retrained model learn to better discriminate between cat and dog . Reducing false negative , often result in better recall to reduce false negative . First , lower the confidence threshold . This should improve recall also use better example to model the variety of both the object and the image they appear in . Finally split your label into two class that are easier to learn . For example , instead of good cooky and bad cooky , you might want good cooky , burnt cooky and broken cooky to help the model learn each unique concept better . If you 're satisfied with the performance of your model , you can make it available for use by starting it from the console or by using code . After the model is running , you can perform an inference with the AWS CLI or the SDK . When you call the API you specify the Amazon resource name of the Amazon recognition custom label model that you want to use . The Amazon resource name is also known a an A R N. You 'll also specify the image you want the model to make a prediction with . You can provide an input image a an image byte array of base 64 encoded image byte or a an S3 object . Custom label are returned in an array of custom label object . Each custom label represents a single object seen or concept that 's found in the image . A custom label includes a label for the object scene or concept that wa found in the image . It also includes a bounding box for object that were found in the image . The bounding box coordinate show where the object is located on the source image . The coordinate value are a ratio of the overall image size . Finally , the custom label includes the confidence score . This represents how confident Amazon recognition custom label is in the accuracy of the label and bounding box . During training , a model calculates a threshold value that determines if a prediction for a label is true . By default , the detect custom label operation doe n't return label with a confidence value . That 's le than the model 's calculated threshold value . To filter the return label specify a value for min confidence that 's greater than the model 's calculated threshold . You can get the model 's calculated threshold from the model 's training result in the Amazon recognition custom label console to get all the label regardless of confidence , specify a min confidence value of zero . If you find that the confidence value returned by the detect custom label operation are too low . Consider retraining the model , you can restrict the number of custom label that are returned from the detect custom label operation by specifying the max result , input parameter . The returned result are sorted from the highest confidence to the lowest confidence . Here are some key takeaway from this section of the module model must be trained for the specific domain that you want to analyze . If you 're looking for turbocharger , you 'll need many picture of turbocharger to train your model . You can set custom labeling for the specific business case . We looked at the custom labeling process and some of the tool you can use . If you want object to be detected , you need to label image and create bounding box for these object . You can use Amazon Sagemaker Ground Truth to build training data set for your model , which can also use machine learning to label your image . Thanks for watching and we 'll see you in the next video .\n",
      "['It', \"'s\", 'now', 'time', 'to', 'summarize', 'some', 'of', 'the', 'main', 'points', 'in', 'this', 'module', '.', 'In', 'this', 'module', ',', 'you', 'learned', 'how', 'to', 'describe', 'the', 'use', 'cases', 'for', 'computer', 'vision', '.', 'Describe', 'the', 'Amazon', 'managed', 'machine', 'learning', 'services', 'available', 'for', 'image', 'and', 'video', 'analysis', 'list', '.', 'The', 'steps', 'required', 'to', 'prepare', 'a', 'custom', 'data', 'set', 'for', 'object', 'detection', '.', 'Describe', 'how', 'Amazon', 'Sagemaker', 'ground', 'truth', 'can', 'be', 'used', 'to', 'prepare', 'a', 'custom', 'data', 'set', 'and', 'use', 'Amazon', 'recognition', 'to', 'perform', 'facial', 'detection', '.', 'That', 'concludes', 'this', 'introduction', 'to', 'computer', 'vision', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'again', 'in', 'the', 'next', 'video', '.']\n",
      "It 's now time to summarize some of the main point in this module . In this module , you learned how to describe the use case for computer vision . Describe the Amazon managed machine learning service available for image and video analysis list . The step required to prepare a custom data set for object detection . Describe how Amazon Sagemaker ground truth can be used to prepare a custom data set and use Amazon recognition to perform facial detection . That concludes this introduction to computer vision . Thanks for watching . We 'll see you again in the next video .\n",
      "['Hi', 'and', 'welcome', 'to', 'module', 'six', 'of', 'Aws', 'Academy', 'Machine', 'learning', 'introduction', 'to', 'natural', 'language', 'processing', 'in', 'this', 'module', '.', 'We', \"'ll\", 'introduce', 'natural', 'language', 'processing', 'which', 'is', 'also', 'known', 'as', 'NLP', '.', 'This', 'section', 'includes', 'a', 'description', 'of', 'the', 'major', 'challenges', 'faced', 'by', 'N', 'LP', 'and', 'the', 'overall', 'development', 'process', 'for', 'N', 'LP', 'applications', '.', 'We', \"'ll\", 'then', 'review', 'five', 'aws', 'services', 'you', 'can', 'use', 'to', 'speed', 'up', 'the', 'development', 'of', 'N', 'LP', 'BASED', 'applications', '.', 'After', 'completing', 'this', 'module', ',', 'you', 'should', 'be', 'able', 'to', 'describe', 'the', 'N', 'LP', 'use', 'cases', 'that', 'are', 'solved', 'by', 'using', 'managed', 'Amazon', 'M', 'L', 'services', 'and', 'describe', 'the', 'managed', 'Amazon', 'M', 'L', 'services', 'available', 'for', 'N', 'LP', '.', 'Let', \"'s\", 'get', 'started', '.']\n",
      "Hi and welcome to module six of Aws Academy Machine learning introduction to natural language processing in this module . We 'll introduce natural language processing which is also known a NLP . This section includes a description of the major challenge faced by N LP and the overall development process for N LP application . We 'll then review five aws service you can use to speed up the development of N LP BASED application . After completing this module , you should be able to describe the N LP use case that are solved by using managed Amazon M L service and describe the managed Amazon M L service available for N LP . Let 's get started .\n",
      "['We', \"'ll\", 'get', 'started', 'by', 'reviewing', 'what', 'natural', 'language', 'processing', 'means', '.', 'Natural', 'language', 'processing', 'is', 'also', 'known', 'as', 'N', 'LP', '.', 'Before', 'we', 'explain', 'what', 'N', 'LP', 'is', ',', 'we', \"'ll\", 'consider', 'an', 'example', 'of', 'N', 'LP', '.', 'Amazon', ',', 'Alexa', ',', 'Alexa', 'works', 'by', 'having', 'a', 'device', 'such', 'as', 'an', 'Amazon', 'echo', ',', 'record', 'your', 'words', '.', 'The', 'recording', 'of', 'your', 'speech', 'is', 'sent', 'to', 'Amazon', 'servers', 'to', 'be', 'analyzed', 'more', 'efficiently', '.', 'Amazon', 'breaks', 'down', 'your', 'phrase', 'into', 'individual', 'sounds', '.', 'Then', 'it', 'connects', 'to', 'a', 'database', 'containing', 'the', 'pronunciation', 'of', 'various', 'words', 'to', 'find', 'which', 'words', 'most', 'closely', 'correspond', 'to', 'the', 'combination', 'of', 'individual', 'sounds', '.', 'Amazon', 'identifies', 'important', 'words', 'to', 'make', 'sense', 'of', 'the', 'tasks', 'and', 'carry', 'out', 'corresponding', 'functions', '.', 'For', 'instance', ',', 'if', 'Alexa', 'notices', 'words', 'like', 'outside', 'or', 'temperature', ',', 'it', 'will', 'open', 'the', 'weather', ',', 'Alexa', 'skill', '.', 'Amazon', 'servers', 'then', 'send', 'the', 'information', 'back', 'to', 'your', 'device', '.', 'And', 'Alexa', 'speaks', 'N', 'LP', 'is', 'a', 'broad', 'term', 'for', 'a', 'general', 'set', 'of', 'business', 'or', 'computational', 'problems', 'you', 'can', 'solve', 'with', 'machine', 'learning', 'or', 'M', 'L.', 'However', ',', 'N', 'LP', 'systems', 'predate', 'machine', 'learning', '.', 'For', 'example', ',', 'speech', 'to', 'text', 'on', 'older', 'pre', 'smartphone', 'cell', 'phones', 'used', 'N', 'LP', 'and', 'so', 'did', 'screen', 'readers', '.', 'Many', 'N', 'LP', 'systems', 'now', 'use', 'some', 'form', 'of', 'machine', 'learning', '.', 'N', 'LP', 'considers', 'the', 'hierarchical', 'structure', 'of', 'language', '.', 'Words', 'are', 'at', 'the', 'lowest', 'layer', 'in', 'a', 'hierarchy', '.', 'A', 'group', 'of', 'words', 'make', 'a', 'phrase', 'in', 'the', 'next', 'level', 'up', 'phrases', 'make', 'a', 'sentence', 'and', 'ultimately', 'sentences', 'convey', 'ideas', '.', 'NLP', 'systems', 'face', 'several', 'significant', 'challenges', '.', 'We', \"'ll\", 'look', 'at', 'its', 'challenges', '.', 'Next', 'language', 'is', \"n't\", 'precise', 'words', 'can', 'have', 'different', 'meanings', 'based', 'on', 'the', 'other', 'words', 'that', 'surround', 'them', '.', 'This', 'is', 'known', 'as', 'context', ',', 'often', 'the', 'same', 'words', 'or', 'phrases', 'can', 'have', 'multiple', 'meanings', '.', 'For', 'example', ',', 'consider', 'the', 'term', 'weather', '.', 'You', 'could', 'be', 'under', 'the', 'weather', 'which', 'has', 'a', 'colloquial', 'meaning', 'in', 'English', 'that', 'you', \"'re\", 'sick', 'or', 'you', 'could', 'say', 'there', \"'s\", 'wonderful', 'weather', 'outside', ',', 'which', 'means', 'the', 'weather', 'conditions', 'outside', 'are', 'good', '.', 'The', 'phrase', 'oh', 'really', 'could', 'convey', 'surprise', ',', 'disagreement', 'and', 'many', 'other', 'things', '.', 'It', 'depends', 'on', 'the', 'context', 'and', 'inflection', '.', 'Here', 'are', 'some', 'of', 'the', 'main', 'challenges', 'for', 'NLP', '.', 'One', 'challenge', 'is', 'discovering', 'the', 'structure', 'of', 'the', 'text', '.', 'One', 'of', 'the', 'first', 'tasks', 'of', 'any', 'N', 'LP', 'application', 'is', 'to', 'break', 'down', 'the', 'text', 'into', 'meaningful', 'units', 'such', 'as', 'words', ',', 'phrases', 'and', 'sentences', '.', 'Another', 'challenge', 'is', 'labeling', 'data', '.', 'After', 'the', 'system', 'converts', 'the', 'text', 'to', 'data', ',', 'it', 'must', 'apply', 'labels', 'representing', 'the', 'various', 'parts', 'of', 'speech', '.', 'Every', 'language', 'will', 'require', 'a', 'different', 'labeling', 'scheme', 'to', 'match', 'the', 'language', \"'s\", 'grammar', '.', 'NLP', 'also', 'faces', 'a', 'challenge', 'in', 'representing', 'context', 'because', 'word', 'meaning', 'can', 'depend', 'heavily', 'on', 'context', '.', 'Any', 'NLP', 'system', 'needs', 'a', 'way', 'to', 'represent', 'it', '.', 'This', 'is', 'a', 'large', 'challenge', 'because', 'there', 'are', 'many', 'contexts', 'and', 'it', \"'s\", 'difficult', 'to', 'convert', 'context', 'into', 'a', 'form', 'computers', 'can', 'understand', '.', 'Finally', ',', 'although', 'grammar', 'defines', 'a', 'structure', 'for', 'language', ',', 'the', 'application', 'of', 'grammar', 'is', 'indescribably', 'large', 'in', 'scope', 'handling', '.', 'The', 'variation', 'in', 'how', 'language', 'is', 'used', 'by', 'humans', 'is', 'a', 'major', 'challenge', 'for', 'N', 'LP', 'systems', '.', 'That', \"'s\", 'where', 'machine', 'learning', 'can', 'have', 'a', 'large', 'impact', '.', 'You', 'can', 'apply', 'NLP', 'to', 'a', 'range', 'of', 'problems', '.', 'Some', 'of', 'the', 'more', 'common', 'applications', 'include', 'search', 'applications', 'such', 'as', 'Google', 'or', 'Bing', 'human', 'machine', 'interactions', 'like', 'Alexa', 'sentiment', 'analysis', 'for', 'marketing', 'or', 'political', 'campaigns', ',', 'social', 'research', 'based', 'on', 'media', 'analysis', 'and', 'chat', 'bots', 'to', 'mimic', 'human', 'speech', 'and', 'applications', '.', 'You', 'can', 'apply', 'the', 'machine', 'learning', 'development', 'pipeline', 'you', \"'ve\", 'seen', 'throughout', 'this', 'course', 'when', 'developing', 'an', 'NLP', 'solution', ',', 'the', 'first', 'task', 'is', 'to', 'formulate', 'a', 'problem', 'then', 'collect', 'and', 'label', 'data', 'for', 'N', 'LP', '.', 'Collecting', 'data', 'consists', 'of', 'breaking', 'down', 'the', 'text', 'into', 'meaningful', 'subsets', 'and', 'labeling', 'the', 'sets', '.', 'Feature', 'engineering', 'is', 'a', 'major', 'part', 'of', 'N', 'LP', 'applications', '.', 'This', 'process', 'gets', 'complicated', 'when', 'you', \"'re\", 'dealing', 'with', 'highly', 'irregular', 'or', 'unstructured', 'text', '.', 'For', 'example', ',', 'say', 'you', \"'re\", 'building', 'an', 'application', 'to', 'classify', 'documents', ',', 'you', \"'d\", 'need', 'to', 'be', 'able', 'to', 'distinguish', 'between', 'the', 'words', 'with', 'common', 'terms', 'but', 'different', 'meanings', '.', 'Labeling', 'data', 'in', 'the', 'N', 'LP', 'domain', 'is', 'sometimes', 'also', 'called', 'tagging', '.', 'In', 'the', 'labeling', 'process', ',', 'you', 'assign', 'individual', 'text', 'strings', 'to', 'different', 'parts', 'of', 'speech', '.', 'There', 'are', 'specialized', 'tools', 'you', 'can', 'use', 'for', 'N', 'LP', '.', 'Labeling', '.', 'The', 'first', 'task', 'for', 'an', 'N', 'LP', 'application', 'is', 'to', 'convert', 'the', 'text', 'to', 'data', '.', 'So', 'it', 'can', 'be', 'analyzed', ',', 'you', 'convert', 'text', 'by', 'removing', 'words', 'that', 'are', \"n't\", 'needed', 'for', 'analysis', 'from', 'the', 'input', 'text', '.', 'In', 'the', 'example', ',', 'the', 'words', 'this', 'and', 'is', 'are', 'removed', 'to', 'leave', 'the', 'phrase', 'sample', 'text', '.', 'After', 'removing', 'stop', 'words', ',', 'you', 'can', 'normalize', 'text', 'by', 'converting', 'similar', 'words', 'into', 'a', 'common', 'form', '.', 'For', 'example', ',', 'the', 'words', 'run', ',', 'runner', ',', 'ran', 'and', 'running', 'are', 'all', 'different', 'forms', 'of', 'the', 'word', 'run', '.', 'You', 'can', 'normalize', 'all', 'instances', 'of', 'these', 'words', 'in', 'a', 'block', 'of', 'text', 'using', 'the', 'stemming', 'and', 'limitation', 'processes', ',', 'limitation', 'groups', ',', 'different', 'forms', 'of', 'a', 'word', 'into', 'a', 'single', 'term', 'lemon', 'of', 'the', 'versions', 'of', 'the', 'word', 'run', 'would', 'group', 'all', 'instances', 'of', 'those', 'forms', 'into', 'a', 'single', 'term', 'run', '.', 'Stemming', '.', 'On', 'the', 'other', 'hand', ',', 'removes', 'characters', 'that', 'the', 'stemming', 'algorithm', 'considers', 'unnecessary', 'stemming', 'might', 'not', 'work', 'with', 'the', 'run', 'example', 'as', 'the', 'form', 'ran', 'might', 'not', 'be', 'recognized', 'as', 'a', 'form', 'of', 'the', 'word', 'run', 'after', 'you', \"'ve\", 'normalized', 'the', 'text', '.', 'You', 'can', 'standardize', 'it', 'by', 'removing', 'words', 'that', 'are', \"n't\", 'in', 'the', 'dictionary', 'you', \"'re\", 'using', 'for', 'analysis', '.', 'For', 'example', ',', 'you', 'could', 'remove', 'acronyms', 'slang', 'in', 'special', 'characters', '.', 'The', 'natural', 'language', 'toolkit', 'is', 'also', 'known', 'as', 'N', 'L', 'T', 'K.', 'Their', 'Python', 'library', 'provides', 'functions', 'for', 'removing', 'stop', 'words', 'and', 'normalizing', 'text', '.', 'Another', 'first', 'step', 'in', 'creating', 'an', 'NLP', 'system', 'is', 'to', 'convert', 'the', 'text', 'into', 'a', 'data', 'collection', 'such', 'as', 'a', 'data', 'frame', '.', 'All', 'N', 'LP', 'libraries', 'provide', 'functions', 'to', 'assist', 'with', 'this', 'process', '.', 'The', 'example', 'shows', 'using', 'the', 'word', 'tokenize', 'function', 'from', 'the', 'N', 'L', 'T', 'K', 'library', '.', 'After', 'you', \"'ve\", 'cleaned', 'up', 'your', 'text', 'and', 'loaded', 'it', 'into', 'a', 'data', 'frame', ',', 'you', 'can', 'apply', 'one', 'of', 'the', 'N', 'LP', 'models', 'to', 'create', 'features', '.', 'Here', 'are', 'a', 'couple', 'of', 'common', 'models', '.', 'The', 'first', 'model', 'is', 'known', 'as', 'bag', 'of', 'words', '.', 'This', 'is', 'a', 'simple', 'model', 'for', 'capturing', 'the', 'frequency', 'of', 'words', 'in', 'a', 'document', '.', 'The', 'model', 'creates', 'a', 'key', 'for', 'each', 'word', '.', 'The', 'value', 'of', 'the', 'key', 'is', 'the', 'number', 'of', 'times', 'that', 'word', 'occurs', 'in', 'the', 'document', '.', 'The', 'second', 'model', 'is', 'term', 'frequency', 'and', 'inverse', 'document', 'frequency', 'which', 'is', 'also', 'known', 'as', 'T', 'F', 'ID', 'F', 'term', 'frequency', 'is', 'a', 'count', 'of', 'how', 'many', 'times', 'a', 'word', 'appears', 'in', 'a', 'document', '.', 'Inverse', 'document', 'frequency', 'is', 'the', 'number', 'of', 'times', 'a', 'word', 'occurs', 'in', 'a', 'group', 'of', 'documents', '.', 'These', 'two', 'values', 'are', 'used', 'together', 'to', 'calculate', 'a', 'weight', 'for', 'the', 'words', 'words', 'that', 'frequently', 'appear', 'in', 'many', 'documents', 'have', 'a', 'lower', 'weight', '.', 'There', 'are', 'many', 'established', 'models', 'in', 'the', 'NLP', 'field', '.', 'The', 'example', 'shows', 'a', 'bag', 'of', 'words', 'model', 'bag', 'of', 'words', 'is', 'a', 'vector', 'model', 'vector', 'models', 'convert', 'each', 'sentence', 'or', 'phrase', 'into', 'a', 'vector', 'which', 'is', 'a', 'mathematical', 'object', 'that', 'records', 'both', 'directionality', 'and', 'magnitude', '.', 'In', 'the', 'example', ',', 'a', 'simple', 'sentence', 'is', 'converted', 'into', 'a', 'vector', 'where', 'the', 'frequency', 'of', 'each', 'word', 'is', 'recorded', '.', 'The', 'word', 'is', 'has', 'a', 'value', 'of', 'two', 'because', 'it', 'appears', 'twice', 'in', 'the', 'sentence', 'bag', 'of', 'words', 'is', 'often', 'used', 'to', 'classify', 'documents', 'into', 'different', 'categories', '.', 'It', \"'s\", 'also', 'used', 'to', 'derive', 'attributes', 'that', 'feed', 'into', 'N', 'LP', 'applications', 'such', 'as', 'in', 'sentiment', 'analysis', '.', 'There', 'are', 'three', 'broad', 'categories', 'of', 'text', 'analysis', '.', 'First', ',', 'the', 'classification', 'of', 'text', 'is', 'similar', 'to', 'other', 'classification', 'systems', 'you', \"'ve\", 'seen', 'in', 'this', 'course', ',', 'text', 'provides', 'the', 'input', 'to', 'a', 'process', 'that', 'extracts', 'features', '.', 'Then', 'you', 'send', 'the', 'features', 'through', 'a', 'machine', 'learning', 'algorithm', 'that', 'interacts', 'with', 'a', 'classifier', 'model', 'and', 'infers', 'the', 'classification', '.', 'There', 'are', 'many', 'applications', 'for', 'text', 'matching', '.', 'For', 'example', ',', 'autocorrect', 'spelling', 'and', 'grammar', 'checking', 'are', 'based', 'on', 'text', 'matching', '.', 'The', 'algorithm', 'for', 'edit', 'distance', '.', 'Also', 'known', 'as', 'the', 'Levenstein', 'distance', 'is', 'frequently', 'used', '.', 'You', 'can', 'derive', 'relationships', 'between', 'different', 'words', 'or', 'phrases', 'in', 'the', 'text', 'using', 'a', 'process', 'called', 'co', 'reference', 'resolution', '.', 'Several', 'NLP', 'systems', 'provide', 'Python', 'libraries', 'for', 'deriving', 'relationships', '.', 'One', 'of', 'the', 'biggest', 'challenges', 'for', 'N', 'LP', 'is', 'how', 'to', 'describe', 'the', 'context', 'for', 'the', 'text', '.', 'Consider', 'this', 'example', 'where', 'a', 'user', 'is', 'searching', 'for', 'the', 'term', 'tablet', 'because', 'the', 'word', 'tablet', 'has', 'at', 'least', 'two', 'distinct', 'meanings', '.', 'The', 'search', 'engine', 'needs', 'to', 'know', 'which', 'meaning', 'the', 'user', 'has', 'in', 'mind', '.', 'Most', 'search', 'engines', 'rely', 'on', 'the', 'most', 'commonly', 'used', 'context', 'if', 'the', 'term', 'is', \"n't\", 'qualified', 'further', '.', 'For', 'example', ',', 'by', 'adding', 'another', 'term', 'like', 'medicine', 'or', 'computing', 'to', 'the', 'search', '.', 'The', 'process', 'of', 'extracting', 'entities', 'is', 'known', 'as', 'named', 'entity', 'recognition', 'or', 'N', 'er', 'and', 'any', 'R', 'model', 'has', 'the', 'following', 'functions', '.', 'First', ',', 'it', 'can', 'identify', 'noun', 'phrases', 'using', 'dependency', 'charts', 'and', 'part', 'of', 'speech', 'tagging', ',', 'it', 'can', 'classify', 'phrases', 'using', 'a', 'classification', 'algorithm', 'such', 'as', 'word', 'to', 'VC', '.', 'Finally', ',', 'it', 'can', 'disambiguate', 'entities', 'using', 'a', 'knowledge', 'graph', '.', 'Here', \"'s\", 'an', 'example', 'of', 'using', 'any', 'er', 'to', 'extract', 'the', 'entity', \"'s\", 'Titanic', 'and', 'North', 'Atlantic', 'from', 'the', 'text', '.', 'After', 'the', 'named', 'entities', 'are', 'extracted', ',', 'you', 'can', 'use', 'a', 'knowledge', 'graph', 'to', 'extract', 'meaning', 'a', 'knowledge', 'graph', 'combines', 'subject', 'matter', 'expertise', 'with', 'machine', 'learning', 'to', 'drive', '.', 'Meaning', 'the', 'Amazon', 'Recommendations', 'engine', 'is', 'an', 'example', 'of', 'a', 'knowledge', 'graph', '.', 'Here', 'are', 'the', 'main', 'points', 'to', 'remember', 'from', 'this', 'section', '.', 'First', 'N', 'LP', 'predates', 'machine', 'learning', '.', 'You', 'can', 'use', 'the', 'same', 'ML', 'workflow', 'that', 'you', \"'ve\", 'seen', 'in', 'other', 'modules', 'for', 'N', 'LP', '.', 'Some', 'of', 'the', 'main', 'use', 'cases', 'for', 'NLP', 'are', 'search', 'query', 'analysis', ',', 'human', 'machine', 'interaction', 'and', 'marketing', 'and', 'social', 'research', '.', 'N', 'LP', 'is', 'complicated', 'because', 'human', 'language', 'lacks', 'precision', '.', 'Thanks', 'for', 'watching', ',', 'we', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "We 'll get started by reviewing what natural language processing mean . Natural language processing is also known a N LP . Before we explain what N LP is , we 'll consider an example of N LP . Amazon , Alexa , Alexa work by having a device such a an Amazon echo , record your word . The recording of your speech is sent to Amazon server to be analyzed more efficiently . Amazon break down your phrase into individual sound . Then it connects to a database containing the pronunciation of various word to find which word most closely correspond to the combination of individual sound . Amazon identifies important word to make sense of the task and carry out corresponding function . For instance , if Alexa notice word like outside or temperature , it will open the weather , Alexa skill . Amazon server then send the information back to your device . And Alexa speaks N LP is a broad term for a general set of business or computational problem you can solve with machine learning or M L. However , N LP system predate machine learning . For example , speech to text on older pre smartphone cell phone used N LP and so did screen reader . Many N LP system now use some form of machine learning . N LP considers the hierarchical structure of language . Words are at the lowest layer in a hierarchy . A group of word make a phrase in the next level up phrase make a sentence and ultimately sentence convey idea . NLP system face several significant challenge . We 'll look at it challenge . Next language is n't precise word can have different meaning based on the other word that surround them . This is known a context , often the same word or phrase can have multiple meaning . For example , consider the term weather . You could be under the weather which ha a colloquial meaning in English that you 're sick or you could say there 's wonderful weather outside , which mean the weather condition outside are good . The phrase oh really could convey surprise , disagreement and many other thing . It depends on the context and inflection . Here are some of the main challenge for NLP . One challenge is discovering the structure of the text . One of the first task of any N LP application is to break down the text into meaningful unit such a word , phrase and sentence . Another challenge is labeling data . After the system convert the text to data , it must apply label representing the various part of speech . Every language will require a different labeling scheme to match the language 's grammar . NLP also face a challenge in representing context because word meaning can depend heavily on context . Any NLP system need a way to represent it . This is a large challenge because there are many context and it 's difficult to convert context into a form computer can understand . Finally , although grammar defines a structure for language , the application of grammar is indescribably large in scope handling . The variation in how language is used by human is a major challenge for N LP system . That 's where machine learning can have a large impact . You can apply NLP to a range of problem . Some of the more common application include search application such a Google or Bing human machine interaction like Alexa sentiment analysis for marketing or political campaign , social research based on medium analysis and chat bot to mimic human speech and application . You can apply the machine learning development pipeline you 've seen throughout this course when developing an NLP solution , the first task is to formulate a problem then collect and label data for N LP . Collecting data consists of breaking down the text into meaningful subset and labeling the set . Feature engineering is a major part of N LP application . This process get complicated when you 're dealing with highly irregular or unstructured text . For example , say you 're building an application to classify document , you 'd need to be able to distinguish between the word with common term but different meaning . Labeling data in the N LP domain is sometimes also called tagging . In the labeling process , you assign individual text string to different part of speech . There are specialized tool you can use for N LP . Labeling . The first task for an N LP application is to convert the text to data . So it can be analyzed , you convert text by removing word that are n't needed for analysis from the input text . In the example , the word this and is are removed to leave the phrase sample text . After removing stop word , you can normalize text by converting similar word into a common form . For example , the word run , runner , ran and running are all different form of the word run . You can normalize all instance of these word in a block of text using the stemming and limitation process , limitation group , different form of a word into a single term lemon of the version of the word run would group all instance of those form into a single term run . Stemming . On the other hand , remove character that the stemming algorithm considers unnecessary stemming might not work with the run example a the form ran might not be recognized a a form of the word run after you 've normalized the text . You can standardize it by removing word that are n't in the dictionary you 're using for analysis . For example , you could remove acronym slang in special character . The natural language toolkit is also known a N L T K. Their Python library provides function for removing stop word and normalizing text . Another first step in creating an NLP system is to convert the text into a data collection such a a data frame . All N LP library provide function to assist with this process . The example show using the word tokenize function from the N L T K library . After you 've cleaned up your text and loaded it into a data frame , you can apply one of the N LP model to create feature . Here are a couple of common model . The first model is known a bag of word . This is a simple model for capturing the frequency of word in a document . The model creates a key for each word . The value of the key is the number of time that word occurs in the document . The second model is term frequency and inverse document frequency which is also known a T F ID F term frequency is a count of how many time a word appears in a document . Inverse document frequency is the number of time a word occurs in a group of document . These two value are used together to calculate a weight for the word word that frequently appear in many document have a lower weight . There are many established model in the NLP field . The example show a bag of word model bag of word is a vector model vector model convert each sentence or phrase into a vector which is a mathematical object that record both directionality and magnitude . In the example , a simple sentence is converted into a vector where the frequency of each word is recorded . The word is ha a value of two because it appears twice in the sentence bag of word is often used to classify document into different category . It 's also used to derive attribute that feed into N LP application such a in sentiment analysis . There are three broad category of text analysis . First , the classification of text is similar to other classification system you 've seen in this course , text provides the input to a process that extract feature . Then you send the feature through a machine learning algorithm that interacts with a classifier model and infers the classification . There are many application for text matching . For example , autocorrect spelling and grammar checking are based on text matching . The algorithm for edit distance . Also known a the Levenstein distance is frequently used . You can derive relationship between different word or phrase in the text using a process called co reference resolution . Several NLP system provide Python library for deriving relationship . One of the biggest challenge for N LP is how to describe the context for the text . Consider this example where a user is searching for the term tablet because the word tablet ha at least two distinct meaning . The search engine need to know which meaning the user ha in mind . Most search engine rely on the most commonly used context if the term is n't qualified further . For example , by adding another term like medicine or computing to the search . The process of extracting entity is known a named entity recognition or N er and any R model ha the following function . First , it can identify noun phrase using dependency chart and part of speech tagging , it can classify phrase using a classification algorithm such a word to VC . Finally , it can disambiguate entity using a knowledge graph . Here 's an example of using any er to extract the entity 's Titanic and North Atlantic from the text . After the named entity are extracted , you can use a knowledge graph to extract meaning a knowledge graph combine subject matter expertise with machine learning to drive . Meaning the Amazon Recommendations engine is an example of a knowledge graph . Here are the main point to remember from this section . First N LP predates machine learning . You can use the same ML workflow that you 've seen in other module for N LP . Some of the main use case for NLP are search query analysis , human machine interaction and marketing and social research . N LP is complicated because human language lack precision . Thanks for watching , we 'll see you in the next video .\n",
      "['Welcome', 'back', 'in', 'this', 'section', '.', 'We', \"'ll\", 'review', 'five', 'managed', 'machine', 'learning', 'services', 'you', 'can', 'use', 'for', 'various', 'use', 'cases', '.', 'These', 'services', 'simplify', 'the', 'process', 'of', 'creating', 'a', 'machine', 'learning', 'application', '.', 'We', \"'ll\", 'start', 'by', 'looking', 'at', 'Amazon', 'transcribe', '.', 'You', 'can', 'use', 'Amazon', 'transcribe', 'to', 'recognize', 'speech', 'in', 'audio', 'files', 'and', 'produce', 'a', 'transcription', '.', 'It', 'can', 'recognize', 'specific', 'voices', 'in', 'an', 'audio', 'file', 'and', 'you', 'can', 'create', 'a', 'customized', 'vocabulary', 'for', 'terms', 'that', 'are', 'specialized', 'for', 'a', 'particular', 'domain', '.', 'You', 'can', 'also', 'add', 'a', 'transcription', 'service', 'to', 'your', 'applications', 'by', 'integrating', 'with', 'web', 'sockets', ',', 'an', 'internet', 'protocol', 'you', 'can', 'use', 'for', 'two', 'way', 'communication', 'between', 'an', 'application', 'and', 'Amazon', 'transcribe', '.', 'Here', 'are', 'some', 'of', 'the', 'more', 'common', 'use', 'cases', 'for', 'Amazon', 'transcribe', '.', 'First', 'medical', 'professionals', 'can', 'record', 'their', 'notes', 'and', 'Amazon', 'transcribe', 'can', 'capture', 'their', 'spoken', 'notes', 'as', 'text', '.', 'Also', 'video', 'production', 'organizations', 'can', 'generate', 'subtitles', 'automatically', 'from', 'video', '.', 'This', 'could', 'also', 'be', 'done', 'in', 'real', 'time', 'for', 'a', 'live', 'feed', 'to', 'add', 'closed', 'captioning', 'media', 'companies', 'can', 'use', 'Amazon', 'transcribe', 'to', 'capture', 'and', 'label', 'content', '.', 'They', 'can', 'then', 'feed', 'the', 'content', 'into', 'Amazon', 'comprehend', 'for', 'further', 'analysis', '.', 'Last', 'companies', 'can', 'record', 'customer', 'service', 'or', 'sales', 'calls', 'and', 'transcribe', 'them', '.', 'They', 'can', 'analyze', 'the', 'results', 'for', 'training', 'or', 'for', 'strategic', 'opportunities', '.', 'Amazon', 'Poly', 'can', 'convert', 'text', 'into', 'lifelike', 'speech', '.', 'You', 'can', 'input', 'either', 'plain', 'text', 'files', 'or', 'a', 'file', 'that', \"'s\", 'formatted', 'in', 'speech', 'synthesis', 'markup', 'language', 'or', 'S', 'S', 'M', 'L', 'S', 'S', 'M', 'L', 'is', 'a', 'markup', 'language', 'used', 'to', 'provide', 'special', 'instructions', 'for', 'how', 'speech', 'should', 'sound', '.', 'For', 'example', ',', 'if', 'you', 'want', 'to', 'introduce', 'a', 'pause', 'in', 'the', 'flow', 'of', 'speech', ',', 'you', 'can', 'add', 'an', 'S', 'S', 'M', 'L', 'tag', 'that', 'instructs', 'Amazon', 'Polly', 'to', 'pause', 'between', 'two', 'words', '.', 'You', 'can', 'also', 'output', 'speech', 'from', 'Amazon', 'Polly', 'to', 'P', 'three', 'V', 'B', 'and', 'PC', 'M', 'audio', 'stream', 'formats', '.', 'Amazon', 'POLY', 'is', 'eligible', 'for', 'use', 'with', 'certain', 'regulated', 'workloads', '.', 'For', 'example', ',', 'it', \"'s\", 'eligible', 'for', 'use', 'with', 'the', 'US', 'Health', 'insurance', 'Portability', 'and', 'Accountability', 'Act', 'of', '1996', 'or', 'IP', 'A.', 'Amazon', 'Polly', 'is', 'also', 'eligible', 'for', 'use', 'with', 'payment', 'card', 'industry', ',', 'data', 'security', 'standard', 'or', 'PC', 'I', 'DS', 'S.', 'Here', 'are', 'some', 'of', 'the', 'more', 'common', 'use', 'cases', 'for', 'Amazon', 'Polly', '.', 'As', 'a', 'first', 'example', ',', 'major', 'news', 'companies', 'are', 'using', 'Amazon', 'Polly', 'to', 'generate', 'vocal', 'content', 'directly', 'from', 'the', 'written', 'stories', '.', 'It', \"'s\", 'also', 'been', 'embedded', 'in', 'mapping', 'APIS', 'so', 'developers', 'can', 'add', 'voice', 'to', 'their', 'geo', 'based', 'applications', '.', 'Language', 'training', 'companies', 'have', 'used', 'Amazon', 'Polly', 'to', 'create', 'systems', 'for', 'learning', 'a', 'new', 'language', '.', 'Finally', ',', 'animators', 'have', 'used', 'it', 'to', 'add', 'voices', 'to', 'their', 'characters', 'with', 'Amazon', 'translate', ',', 'you', 'can', 'create', 'multi', 'language', 'experiences', 'in', 'your', 'applications', ',', 'you', 'can', 'create', 'systems', 'that', 'read', 'documents', 'in', 'one', 'language', 'and', 'then', 'render', 'or', 'store', 'them', 'in', 'another', 'language', '.', 'You', 'can', 'also', 'use', 'it', 'as', 'part', 'of', 'a', 'document', 'analysis', 'system', '.', 'Amazon', 'translate', 'is', 'fully', 'integrated', 'with', 'other', 'machine', 'learning', 'services', 'such', 'as', 'Amazon', 'comprehend', ',', 'Amazon', 'transcribe', 'and', 'Amazon', 'Poly', '.', 'With', 'this', 'integration', ',', 'you', 'can', 'extract', 'named', 'entities', ',', 'sentiment', 'and', 'key', 'phrases', 'by', 'integrating', 'it', 'with', 'Amazon', 'comprehend', 'create', 'Multilingual', 'subtitles', 'with', 'Amazon', 'transcribe', 'and', 'speak', 'translated', 'content', 'with', 'Amazon', 'poly', '.', 'Here', 'are', 'some', 'of', 'the', 'more', 'common', 'use', 'cases', 'for', 'Amazon', 'translate', '.', 'The', 'first', 'use', 'case', 'is', 'building', 'international', 'websites', '.', 'You', 'can', 'use', 'Amazon', 'translate', 'to', 'quickly', 'globalize', 'your', 'websites', '.', 'Amazon', 'translate', 'can', 'also', 'be', 'used', 'to', 'develop', 'Multilingual', 'chat', 'bots', '.', 'Chat', 'bots', 'are', 'used', 'to', 'create', 'a', 'more', 'human', 'like', 'interface', 'to', 'applications', 'with', 'Amazon', 'translate', '.', 'You', 'can', 'create', 'a', 'chat', 'bot', 'that', 'speaks', 'multiple', 'languages', '.', 'Another', 'use', 'case', 'is', 'software', 'localization', 'localization', 'is', 'a', 'major', 'cost', 'for', 'all', 'software', 'aimed', 'at', 'a', 'global', 'audience', '.', 'Amazon', 'translate', 'can', 'decrease', 'software', 'development', 'time', 'and', 'significantly', 'reduce', 'costs', 'for', 'localizing', 'software', '.', 'The', 'final', 'example', 'use', 'case', 'is', 'international', 'media', 'management', 'companies', 'that', 'manage', 'media', 'for', 'a', 'global', 'audience', 'have', 'used', 'Amazon', 'translate', 'to', 'reduce', 'their', 'costs', 'for', 'localization', '.', 'Amazon', 'comprehend', 'implements', 'many', 'of', 'the', 'NLP', 'techniques', 'that', 'we', 'reviewed', 'earlier', 'in', 'this', 'module', '.', 'You', 'can', 'extract', 'key', 'entities', 'perform', 'sentiment', 'analysis', 'and', 'tag', 'words', 'with', 'parts', 'of', 'speech', '.', 'Here', 'are', 'some', 'of', 'the', 'more', 'common', 'use', 'cases', 'for', 'Amazon', 'comprehend', '.', 'The', 'first', 'example', 'is', 'analyzing', 'legal', 'and', 'medical', 'documents', '.', 'Legal', 'insurance', 'and', 'medical', 'organizations', 'have', 'used', 'Amazon', 'comprehend', 'to', 'perform', 'many', 'of', 'the', 'N', 'LP', 'functions', 'we', 'reviewed', 'in', 'this', 'module', '.', 'Another', 'use', 'is', 'for', 'large', 'scale', 'mobile', 'app', 'analysis', '.', 'Mobile', 'app', 'developers', 'use', 'Amazon', 'comprehend', 'to', 'look', 'for', 'patterns', 'of', 'usage', 'with', 'their', 'apps', 'so', 'they', 'can', 'design', 'improvements', '.', 'Financial', 'fraud', 'detection', 'is', 'another', 'use', 'case', 'for', 'Amazon', 'comprehend', ',', 'banking', ',', 'financial', 'and', 'other', 'institutions', 'have', 'used', 'it', 'to', 'examine', 'very', 'large', 'data', 'sets', 'of', 'financial', 'transactions', 'to', 'uncover', 'fraud', 'and', 'look', 'for', 'patterns', 'of', 'illegal', 'transactions', '.', 'Finally', ',', 'it', 'can', 'be', 'used', 'for', 'content', 'management', ',', 'media', 'and', 'other', 'content', 'companies', 'can', 'use', 'Amazon', 'comprehend', 'to', 'tag', 'content', 'for', 'analysis', 'and', 'management', '.', 'With', 'Amazon', 'Lex', ',', 'you', 'can', 'add', 'a', 'human', 'language', 'front', 'end', 'to', 'your', 'applications', '.', 'Amazon', 'Lex', 'lets', 'you', 'use', 'the', 'same', 'conversational', 'engine', 'that', 'powers', 'Amazon', 'Alexa', '.', 'You', 'can', 'automatically', 'increase', 'capacity', 'for', 'your', 'Amazon', 'Lex', 'solution', 'by', 'creating', 'Aws', 'lambda', 'functions', 'that', 'scale', 'on', 'demand', '.', 'You', 'can', 'also', 'store', 'log', 'files', 'of', 'the', 'conversations', 'for', 'further', 'analysis', '.', 'Here', 'are', 'some', 'of', 'the', 'more', 'common', 'use', 'cases', 'for', 'Amazon', 'Lex', '.', 'The', 'first', 'use', 'case', 'is', 'building', 'front', 'end', 'interfaces', 'for', 'inventory', 'management', 'and', 'sales', '.', 'Voice', 'interfaces', 'are', 'becoming', 'more', 'common', '.', 'Companies', 'have', 'used', 'Amazon', 'Lex', 'to', 'add', 'chat', 'bots', 'to', 'their', 'inventory', 'and', 'sales', 'applications', '.', 'Another', 'use', 'for', 'Amazon', 'Lex', 'is', 'creating', 'customer', 'service', 'interfaces', '.', 'Human', 'like', 'voice', 'applications', 'are', 'quickly', 'becoming', 'the', 'standard', 'for', 'many', 'customer', 'service', 'applications', '.', 'Amazon', 'Lex', 'can', 'reduce', 'the', 'time', 'it', 'takes', 'to', 'develop', 'these', 'chat', 'bots', 'and', 'increase', 'their', 'quality', '.', 'Amazon', 'Lex', 'can', 'also', 'be', 'used', 'to', 'develop', 'interactive', 'assistance', 'by', 'combining', 'Amazon', 'Lex', 'with', 'other', 'ML', 'services', '.', 'Customers', 'are', 'creating', 'more', 'sophisticated', 'assistance', 'for', 'many', 'different', 'industries', '.', 'The', 'final', 'example', 'use', 'case', 'is', 'querying', 'databases', 'with', 'a', 'human', 'like', 'language', '.', 'Amazon', 'Lex', 'has', 'been', 'combined', 'with', 'other', 'AWS', 'database', 'services', 'to', 'create', 'sophisticated', 'data', 'analysis', 'applications', 'with', 'a', 'human', 'like', 'language', 'interface', '.', 'Here', 'are', 'some', 'of', 'the', 'main', 'points', 'you', 'should', 'take', 'away', 'from', 'this', 'module', '.', 'First', ',', 'Amazon', 'transcribe', 'can', 'automatically', 'convert', 'spoken', 'language', 'to', 'text', '.', 'Amazon', 'Polly', 'can', 'convert', 'written', 'text', 'to', 'spoken', 'language', '.', 'Amazon', 'translate', 'can', 'create', 'real', 'time', 'translation', 'between', 'languages', '.', 'Amazon', 'comprehend', 'automates', 'many', 'of', 'the', 'NLP', 'use', 'cases', 'reviewed', 'in', 'this', 'module', '.', 'And', 'finally', ',', 'Amazon', 'Lex', 'can', 'create', 'a', 'human', 'like', 'interface', 'to', 'your', 'applications', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'video', '.']\n",
      "Welcome back in this section . We 'll review five managed machine learning service you can use for various use case . These service simplify the process of creating a machine learning application . We 'll start by looking at Amazon transcribe . You can use Amazon transcribe to recognize speech in audio file and produce a transcription . It can recognize specific voice in an audio file and you can create a customized vocabulary for term that are specialized for a particular domain . You can also add a transcription service to your application by integrating with web socket , an internet protocol you can use for two way communication between an application and Amazon transcribe . Here are some of the more common use case for Amazon transcribe . First medical professional can record their note and Amazon transcribe can capture their spoken note a text . Also video production organization can generate subtitle automatically from video . This could also be done in real time for a live feed to add closed captioning medium company can use Amazon transcribe to capture and label content . They can then feed the content into Amazon comprehend for further analysis . Last company can record customer service or sale call and transcribe them . They can analyze the result for training or for strategic opportunity . Amazon Poly can convert text into lifelike speech . You can input either plain text file or a file that 's formatted in speech synthesis markup language or S S M L S S M L is a markup language used to provide special instruction for how speech should sound . For example , if you want to introduce a pause in the flow of speech , you can add an S S M L tag that instructs Amazon Polly to pause between two word . You can also output speech from Amazon Polly to P three V B and PC M audio stream format . Amazon POLY is eligible for use with certain regulated workload . For example , it 's eligible for use with the US Health insurance Portability and Accountability Act of 1996 or IP A. Amazon Polly is also eligible for use with payment card industry , data security standard or PC I DS S. Here are some of the more common use case for Amazon Polly . As a first example , major news company are using Amazon Polly to generate vocal content directly from the written story . It 's also been embedded in mapping APIS so developer can add voice to their geo based application . Language training company have used Amazon Polly to create system for learning a new language . Finally , animator have used it to add voice to their character with Amazon translate , you can create multi language experience in your application , you can create system that read document in one language and then render or store them in another language . You can also use it a part of a document analysis system . Amazon translate is fully integrated with other machine learning service such a Amazon comprehend , Amazon transcribe and Amazon Poly . With this integration , you can extract named entity , sentiment and key phrase by integrating it with Amazon comprehend create Multilingual subtitle with Amazon transcribe and speak translated content with Amazon poly . Here are some of the more common use case for Amazon translate . The first use case is building international website . You can use Amazon translate to quickly globalize your website . Amazon translate can also be used to develop Multilingual chat bot . Chat bot are used to create a more human like interface to application with Amazon translate . You can create a chat bot that speaks multiple language . Another use case is software localization localization is a major cost for all software aimed at a global audience . Amazon translate can decrease software development time and significantly reduce cost for localizing software . The final example use case is international medium management company that manage medium for a global audience have used Amazon translate to reduce their cost for localization . Amazon comprehend implement many of the NLP technique that we reviewed earlier in this module . You can extract key entity perform sentiment analysis and tag word with part of speech . Here are some of the more common use case for Amazon comprehend . The first example is analyzing legal and medical document . Legal insurance and medical organization have used Amazon comprehend to perform many of the N LP function we reviewed in this module . Another use is for large scale mobile app analysis . Mobile app developer use Amazon comprehend to look for pattern of usage with their apps so they can design improvement . Financial fraud detection is another use case for Amazon comprehend , banking , financial and other institution have used it to examine very large data set of financial transaction to uncover fraud and look for pattern of illegal transaction . Finally , it can be used for content management , medium and other content company can use Amazon comprehend to tag content for analysis and management . With Amazon Lex , you can add a human language front end to your application . Amazon Lex let you use the same conversational engine that power Amazon Alexa . You can automatically increase capacity for your Amazon Lex solution by creating Aws lambda function that scale on demand . You can also store log file of the conversation for further analysis . Here are some of the more common use case for Amazon Lex . The first use case is building front end interface for inventory management and sale . Voice interface are becoming more common . Companies have used Amazon Lex to add chat bot to their inventory and sale application . Another use for Amazon Lex is creating customer service interface . Human like voice application are quickly becoming the standard for many customer service application . Amazon Lex can reduce the time it take to develop these chat bot and increase their quality . Amazon Lex can also be used to develop interactive assistance by combining Amazon Lex with other ML service . Customers are creating more sophisticated assistance for many different industry . The final example use case is querying database with a human like language . Amazon Lex ha been combined with other AWS database service to create sophisticated data analysis application with a human like language interface . Here are some of the main point you should take away from this module . First , Amazon transcribe can automatically convert spoken language to text . Amazon Polly can convert written text to spoken language . Amazon translate can create real time translation between language . Amazon comprehend automates many of the NLP use case reviewed in this module . And finally , Amazon Lex can create a human like interface to your application . Thanks for watching . We 'll see you in the next video .\n",
      "['Welcome', 'back', '.', 'It', \"'s\", 'now', 'time', 'to', 'review', 'the', 'module', 'and', 'wrap', 'it', 'up', '.', 'In', 'summary', '.', 'In', 'this', 'module', ',', 'you', 'learn', 'how', 'to', 'describe', 'the', 'NLP', 'use', 'cases', 'that', 'are', 'solved', 'by', 'using', 'managed', 'Amazon', 'ML', 'services', 'and', 'describe', 'the', 'managed', 'M', 'L', 'services', 'available', 'for', 'N', 'LP', '.', 'Good', 'job', '.', 'Thanks', 'for', 'watching', '.', 'We', \"'ll\", 'see', 'you', 'in', 'the', 'next', 'module', '.']\n",
      "Welcome back . It 's now time to review the module and wrap it up . In summary . In this module , you learn how to describe the NLP use case that are solved by using managed Amazon ML service and describe the managed M L service available for N LP . Good job . Thanks for watching . We 'll see you in the next module .\n",
      "['Welcome', 'to', 'module', 'seven', 'course', '.', 'Wrap', 'up', '.', 'Congratulations', 'on', 'completing', 'the', 'Aws', 'Academy', 'Machine', 'Learning', 'course', '.', 'We', \"'ll\", 'take', 'a', 'few', 'minutes', 'to', 'review', 'what', 'you', \"'ve\", 'learned', 'and', 'where', 'you', 'can', 'go', 'from', 'here', '.', 'We', \"'re\", 'going', 'to', 'start', 'with', 'a', 'review', 'of', 'what', 'you', \"'ve\", 'learned', 'in', 'this', 'course', '.', 'You', 'learned', 'how', 'to', 'describe', 'machine', 'learning', ',', 'implement', 'a', 'machine', 'learning', 'pipeline', 'and', 'use', 'Amazon', 'machine', 'learning', 'services', 'for', 'forecasting', 'computer', 'vision', 'and', 'natural', 'language', 'processing', 'well', 'done', '.', 'Although', 'this', 'course', 'is', \"n't\", 'designed', 'to', 'prepare', 'you', 'to', 'become', 'certified', 'for', 'the', 'Aws', 'certified', 'machine', 'learning', 'specialty', '.', 'We', \"'ll\", 'review', 'how', 'you', 'can', 'continue', 'to', 'work', 'towards', 'that', 'certification', '.', 'Aws', 'certification', 'helps', 'you', 'build', 'credibility', 'and', 'confidence', 'by', 'validating', 'your', 'cloud', 'expertise', 'with', 'an', 'industry', 'recognized', 'credential', '.', 'It', 'also', 'helps', 'organizations', 'identify', 'skilled', 'professionals', 'who', 'can', 'lead', 'cloud', 'initiatives', 'by', 'using', 'Aws', '.', 'You', 'must', 'earn', 'a', 'passing', 'score', 'by', 'taking', 'a', 'proctored', 'exam', 'to', 'earn', 'an', 'Aws', 'certification', '.', 'After', 'receiving', 'a', 'passing', 'score', ',', 'you', \"'ll\", 'receive', 'your', 'certification', 'credentials', '.', 'Aws', 'certification', 'does', \"n't\", 'publish', 'a', 'list', 'of', 'all', 'services', 'or', 'features', 'that', 'are', 'covered', 'in', 'a', 'certification', 'exam', '.', 'However', ',', 'there', \"'s\", 'an', 'exam', 'guide', 'for', 'each', 'exam', 'and', 'it', 'lists', 'the', 'current', 'topic', 'areas', 'and', 'objectives', 'covered', 'in', 'the', 'exam', 'exam', 'guides', 'can', 'be', 'found', 'on', 'the', 'prepare', 'for', 'your', 'Aws', 'Certification', 'exam', 'web', 'page', '.', 'You', \"'ll\", 'be', 'required', 'to', 'update', 'your', 'certification', 'or', 'recertify', 'every', 'three', 'years', '.', 'View', 'the', 'Aws', 'Certification', 're', 'certification', 'page', '.', 'For', 'more', 'details', ',', 'the', 'information', 'on', 'this', 'slide', 'is', 'current', 'as', 'of', 'June', '2020', '.', 'However', ',', 'exams', 'are', 'frequently', 'updated', '.', 'Also', 'the', 'details', 'regarding', 'which', 'exams', 'are', 'available', 'and', 'what', 'topics', 'are', 'tested', 'by', 'each', 'exam', 'are', 'subject', 'to', 'change', '.', 'The', 'Aws', 'certified', 'machine', 'learning', 'specialty', 'means', 'you', 'can', 'select', 'and', 'justify', 'the', 'appropriate', 'machine', 'learning', 'approach', 'for', 'a', 'given', 'business', 'problem', '.', 'You', 'can', 'also', 'identify', 'appropriate', 'Aws', 'services', 'to', 'implement', 'machine', 'learning', 'solutions', '.', 'And', 'finally', ',', 'you', 'can', 'design', 'and', 'implement', 'scalable', 'cost', ',', 'optimized', ',', 'reliable', 'and', 'secure', 'machine', 'learning', 'solutions', '.', 'Before', 'sitting', 'for', 'the', 'Aws', 'Certified', 'Machine', 'learning', 'specialty', 'exam', ',', 'we', 'recommend', 'that', 'you', 'have', 'the', 'following', 'knowledge', 'and', 'experience', '.', 'First', ',', 'you', 'should', 'have', '1', 'to', '2', 'years', 'of', 'experience', 'developing', 'architect', 'or', 'running', 'M', 'L', 'or', 'deep', 'learning', 'workloads', 'on', 'the', 'Aws', 'cloud', '.', 'Your', 'experience', 'should', 'include', 'performing', 'basic', 'hyper', 'parameter', 'optimization', 'and', 'working', 'with', 'machine', 'learning', 'and', 'deep', 'learning', 'frameworks', '.', 'You', 'should', 'also', 'be', 'able', 'to', 'express', 'the', 'intuition', 'behind', 'basic', 'ML', 'algorithms', '.', 'Finally', ',', 'you', 'should', 'be', 'able', 'to', 'follow', 'best', 'practices', 'for', 'model', 'training', 'in', 'addition', 'to', 'best', 'practices', 'for', 'deployment', 'and', 'operations', '.', 'Thanks', 'for', 'watching', 'and', 'congratulations', 'on', 'completing', 'the', 'Aws', 'Academy', 'machine', 'learning', 'course', '.']\n",
      "Welcome to module seven course . Wrap up . Congratulations on completing the Aws Academy Machine Learning course . We 'll take a few minute to review what you 've learned and where you can go from here . We 're going to start with a review of what you 've learned in this course . You learned how to describe machine learning , implement a machine learning pipeline and use Amazon machine learning service for forecasting computer vision and natural language processing well done . Although this course is n't designed to prepare you to become certified for the Aws certified machine learning specialty . We 'll review how you can continue to work towards that certification . Aws certification help you build credibility and confidence by validating your cloud expertise with an industry recognized credential . It also help organization identify skilled professional who can lead cloud initiative by using Aws . You must earn a passing score by taking a proctored exam to earn an Aws certification . After receiving a passing score , you 'll receive your certification credential . Aws certification doe n't publish a list of all service or feature that are covered in a certification exam . However , there 's an exam guide for each exam and it list the current topic area and objective covered in the exam exam guide can be found on the prepare for your Aws Certification exam web page . You 'll be required to update your certification or recertify every three year . View the Aws Certification re certification page . For more detail , the information on this slide is current a of June 2020 . However , exam are frequently updated . Also the detail regarding which exam are available and what topic are tested by each exam are subject to change . The Aws certified machine learning specialty mean you can select and justify the appropriate machine learning approach for a given business problem . You can also identify appropriate Aws service to implement machine learning solution . And finally , you can design and implement scalable cost , optimized , reliable and secure machine learning solution . Before sitting for the Aws Certified Machine learning specialty exam , we recommend that you have the following knowledge and experience . First , you should have 1 to 2 year of experience developing architect or running M L or deep learning workload on the Aws cloud . Your experience should include performing basic hyper parameter optimization and working with machine learning and deep learning framework . You should also be able to express the intuition behind basic ML algorithm . Finally , you should be able to follow best practice for model training in addition to best practice for deployment and operation . Thanks for watching and congratulation on completing the Aws Academy machine learning course .\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "for k, v in alldata.items():\n",
    "    tokens = word_tokenize(v)\n",
    "    print(tokens)\n",
    "    normalized_text = \" \".join([lemmatizer.lemmatize(token) for token in tokens])\n",
    "    print(normalized_text)\n",
    "    alldata[k] = normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9c9a635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting yake\n",
      "  Downloading yake-0.4.8-py2.py3-none-any.whl (60 kB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting segtok\n",
      "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yake) (3.0)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yake) (1.22.3)\n",
      "Requirement already satisfied: click>=6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yake) (8.1.3)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: jellyfish in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from yake) (0.9.0)\n",
      "Requirement already satisfied: regex in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from segtok->yake) (2022.10.31)\n",
      "Installing collected packages: tabulate, segtok, yake\n",
      "Successfully installed segtok-1.5.11 tabulate-0.9.0 yake-0.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip install yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ccfdd7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " showing all the keywords from the videos demo1-job_name_1.1_1:\n",
      "Machine Learning\n",
      "Machine\n",
      "Learning\n",
      "Amazon machine learning\n",
      "Amazon\n",
      "Section\n",
      "machine learning pipeline\n",
      "Amazon machine\n",
      "data\n",
      "machine learning problems\n",
      "Machine Learning Foundations\n",
      "managed Amazon machine\n",
      "certified machine learning\n",
      "machine learning services\n",
      "Academy Cloud Foundations\n",
      "module\n",
      "Amazon Sagemaker\n",
      "learn\n",
      "machine learning specialty\n",
      "machine learning model\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_2:\n",
      "Aws Academy machine\n",
      "Aws Academy\n",
      "Academy machine learning\n",
      "machine learning\n",
      "Academy machine\n",
      "machine\n",
      "learning\n",
      "Aws\n",
      "Academy\n",
      "introduce machine learning\n",
      "introduce machine\n",
      "module\n",
      "machine learning terminology\n",
      "terminology\n",
      "introduce\n",
      "artificial intelligence\n",
      "business\n",
      "Describe\n",
      "artificial\n",
      "intelligence\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_3:\n",
      "machine learning\n",
      "learning\n",
      "Deep learning\n",
      "machine\n",
      "machine learning model\n",
      "artificial\n",
      "Deep\n",
      "machine learning fits\n",
      "neurons\n",
      "building machines\n",
      "human\n",
      "train machine learning\n",
      "machine learning capabilities\n",
      "spam\n",
      "artificial intelligence\n",
      "machine learning practitioners\n",
      "output\n",
      "models\n",
      "artificial neurons\n",
      "machines\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_4:\n",
      "Machine learning\n",
      "learning\n",
      "supervised learning\n",
      "machine learning program\n",
      "Machine\n",
      "reinforcement learning\n",
      "data\n",
      "learning reinforcement learning\n",
      "machine learning problems\n",
      "Aws Deep Racer\n",
      "learning program\n",
      "problems\n",
      "learning problems\n",
      "deep learning\n",
      "supervised\n",
      "unsupervised machine learning\n",
      "supervised learning problems\n",
      "machine learning reinforcement\n",
      "Aws Deep\n",
      "deep learning models\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_5:\n",
      "data\n",
      "model\n",
      "training data\n",
      "model data features\n",
      "training\n",
      "data sources\n",
      "features\n",
      "problem\n",
      "model data\n",
      "model performs\n",
      "process\n",
      "feature\n",
      "machine learning\n",
      "target\n",
      "data processing model\n",
      "test data\n",
      "evaluation data\n",
      "machine\n",
      "learning\n",
      "sources\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_6:\n",
      "machine learning\n",
      "Jupiter Lab\n",
      "Jupiter\n",
      "Jupiter notebooks\n",
      "Amazon\n",
      "machine\n",
      "learning\n",
      "data\n",
      "Amazon Sagemaker\n",
      "Python\n",
      "AWS\n",
      "Sagemaker\n",
      "Lab\n",
      "tools\n",
      "text\n",
      "notebooks\n",
      "Python library\n",
      "Jupiter notebooks code\n",
      "library\n",
      "frameworks\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_7:\n",
      "machine learning\n",
      "learning\n",
      "machine\n",
      "data\n",
      "machine learning problems\n",
      "machine learning challenges\n",
      "learning problems\n",
      "problems\n",
      "challenges\n",
      "model\n",
      "models\n",
      "business\n",
      "solve machine learning\n",
      "learning challenges\n",
      "machine learning solution\n",
      "services\n",
      "section\n",
      "YOLO\n",
      "back\n",
      "machine learning knowledge\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_8:\n",
      "machine learning\n",
      "learning\n",
      "machine\n",
      "time to review\n",
      "module\n",
      "looked\n",
      "problem machine learning\n",
      "defining machine learning\n",
      "review the module\n",
      "machine learning application\n",
      "machine learning applies\n",
      "machine learning pipeline\n",
      "time\n",
      "review\n",
      "machine learning terminology\n",
      "machine learning process\n",
      "learning applies learning\n",
      "Describe\n",
      "problem machine\n",
      "defining machine\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_9:\n",
      "Aws Academy\n",
      "Machine learning\n",
      "Amazon Sage Maker\n",
      "back to Aws\n",
      "Machine\n",
      "learning\n",
      "machine learning pipeline\n",
      "Sage Maker\n",
      "machine learning model\n",
      "Amazon Sage\n",
      "machine learning problem\n",
      "Amazon\n",
      "Aws\n",
      "Academy\n",
      "entire machine learning\n",
      "Academy of Machine\n",
      "learning pipeline\n",
      "module\n",
      "Sagemaker\n",
      "process\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_10:\n",
      "machine learning\n",
      "data\n",
      "data set\n",
      "problem\n",
      "learning\n",
      "machine\n",
      "section\n",
      "machine learning problem\n",
      "data sets\n",
      "model\n",
      "business\n",
      "set\n",
      "machine learning model\n",
      "module\n",
      "learning problem\n",
      "machine learning pipeline\n",
      "business problem\n",
      "sets\n",
      "learning model\n",
      "Irvine machine learning\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_11:\n",
      "data\n",
      "Amazon\n",
      "training\n",
      "training data\n",
      "Amazon FSX\n",
      "target\n",
      "Amazon Elastic\n",
      "Amazon Efs\n",
      "transaction\n",
      "Amazon Sagemaker\n",
      "model\n",
      "training jobs\n",
      "data sets\n",
      "data set\n",
      "fraud\n",
      "service\n",
      "fraudulent\n",
      "fraudulent transactions\n",
      "private data\n",
      "machine learning\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_12:\n",
      "aws glue\n",
      "aws glue data\n",
      "data\n",
      "aws\n",
      "glue\n",
      "glue data catalog\n",
      "glue data\n",
      "Aws glue API\n",
      "Aws glue services\n",
      "data catalog\n",
      "Aws GLUE console\n",
      "transform\n",
      "load data data\n",
      "Aws Glue consists\n",
      "extract\n",
      "Aws Glue enables\n",
      "Aws glue crawls\n",
      "data sources\n",
      "data data\n",
      "load\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_13:\n",
      "data\n",
      "Aws\n",
      "access\n",
      "Cloud Trail\n",
      "secure\n",
      "make\n",
      "Cloud\n",
      "Trail\n",
      "Aws account\n",
      "Aws Cloud Trail\n",
      "security\n",
      "service\n",
      "services\n",
      "account\n",
      "Aws Cloud\n",
      "encryption\n",
      "activity\n",
      "requirements\n",
      "Aws services\n",
      "SSL TLS\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_14:\n",
      "data\n",
      "data frame\n",
      "pandas\n",
      "column\n",
      "frame\n",
      "correct data\n",
      "data types\n",
      "correct data types\n",
      "types\n",
      "columns\n",
      "data type\n",
      "load\n",
      "section\n",
      "data set\n",
      "correct\n",
      "Amazon Sagemaker\n",
      "data analysis\n",
      "labels\n",
      "formats\n",
      "format\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_15:\n",
      "data\n",
      "data set\n",
      "statistics\n",
      "plot\n",
      "scatter plot\n",
      "features\n",
      "set\n",
      "variables\n",
      "descriptive statistics\n",
      "attributes\n",
      "feature data\n",
      "scatter\n",
      "feature\n",
      "model\n",
      "numerical data\n",
      "columns\n",
      "descriptive\n",
      "outliers\n",
      "numerical\n",
      "feature data set\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_16:\n",
      "correlation\n",
      "linear relationship\n",
      "relationship\n",
      "data\n",
      "linear\n",
      "variables\n",
      "minus\n",
      "correlation matrix\n",
      "data set\n",
      "acidity\n",
      "fixed acidity\n",
      "fixed\n",
      "set\n",
      "find correlations\n",
      "weak linear relationships\n",
      "back\n",
      "numerical\n",
      "proportional\n",
      "number\n",
      "acid\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_17:\n",
      "data\n",
      "feature\n",
      "features\n",
      "columns\n",
      "feature selection\n",
      "make\n",
      "feature extraction\n",
      "feature engineering\n",
      "categorical data\n",
      "numerical\n",
      "section\n",
      "text\n",
      "encode\n",
      "numerical data\n",
      "model\n",
      "multiple columns\n",
      "selection\n",
      "extraction\n",
      "machine\n",
      "learning\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_18:\n",
      "data\n",
      "missing\n",
      "missing data\n",
      "data set\n",
      "impute\n",
      "set\n",
      "variables\n",
      "variable\n",
      "data sets\n",
      "drop\n",
      "string data\n",
      "single\n",
      "row\n",
      "include\n",
      "single variable\n",
      "make\n",
      "scale\n",
      "rows\n",
      "variable describes\n",
      "column\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_19:\n",
      "feature\n",
      "data\n",
      "outlier\n",
      "model\n",
      "methods\n",
      "features\n",
      "outliers\n",
      "feature set\n",
      "set\n",
      "wrapper methods\n",
      "wrapper\n",
      "Multivariate outliers\n",
      "data set\n",
      "Filter\n",
      "Filter methods\n",
      "feature selection methods\n",
      "subset\n",
      "Filters\n",
      "feature engineering\n",
      "plot\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_20:\n",
      "Amazon Sagemaker\n",
      "data\n",
      "Amazon\n",
      "training data\n",
      "Amazon Sagemaker algorithms\n",
      "Sagemaker\n",
      "training\n",
      "Amazon Sagemaker provides\n",
      "Amazon sagemaker includes\n",
      "Amazon Sagemaker linear\n",
      "data set\n",
      "model\n",
      "Amazon Sagemaker container\n",
      "algorithms\n",
      "learning\n",
      "algorithm\n",
      "test data\n",
      "prebuilt Amazon Sagemaker\n",
      "test data set\n",
      "Sagemaker provides\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_21:\n",
      "model\n",
      "Sagemaker\n",
      "deploy\n",
      "models\n",
      "Amazon Sagemaker\n",
      "Amazon\n",
      "predictions\n",
      "data\n",
      "endpoint\n",
      "inference\n",
      "instances\n",
      "production\n",
      "multiple models\n",
      "deploy your model\n",
      "Amazon Sagemaker batch\n",
      "Amazon Sagemaker hosting\n",
      "deployment\n",
      "multi model\n",
      "host\n",
      "model endpoint\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_22:\n",
      "model\n",
      "cat\n",
      "class\n",
      "predicted\n",
      "data\n",
      "metric\n",
      "actual\n",
      "predicted class\n",
      "positive\n",
      "problem\n",
      "business\n",
      "metrics\n",
      "model metric\n",
      "outcome\n",
      "good\n",
      "actual class\n",
      "label\n",
      "negative\n",
      "business metric\n",
      "evaluate\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_23:\n",
      "cats\n",
      "model\n",
      "number\n",
      "identified\n",
      "correctly identified\n",
      "correctly\n",
      "total number\n",
      "number of actual\n",
      "true\n",
      "negatives\n",
      "positives\n",
      "positive\n",
      "total\n",
      "percentage\n",
      "make\n",
      "calculate\n",
      "metrics\n",
      "false\n",
      "actual\n",
      "making\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_24:\n",
      "model\n",
      "false positive rate\n",
      "positive rate\n",
      "positive\n",
      "false positive\n",
      "true positive\n",
      "true\n",
      "metrics\n",
      "metric\n",
      "models\n",
      "true positive rate\n",
      "false\n",
      "cats\n",
      "sensitivity\n",
      "rate\n",
      "positives\n",
      "squared\n",
      "precision\n",
      "threshold\n",
      "case\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_25:\n",
      "hyper parameter\n",
      "hyper parameter tuning\n",
      "hyper\n",
      "parameter\n",
      "parameters\n",
      "parameter tuning\n",
      "model hyper parameters\n",
      "sagemaker hyper parameter\n",
      "model\n",
      "tuning hyper parameters\n",
      "hyper parameter optimization\n",
      "tuning\n",
      "data\n",
      "training\n",
      "data hyper parameters\n",
      "Sagemaker\n",
      "hyper parameter ranges\n",
      "training job\n",
      "model tuning\n",
      "hyper parameters include\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_26:\n",
      "Amazon Sagemaker\n",
      "time to review\n",
      "knowledge check\n",
      "Amazon Sagemaker outline\n",
      "Amazon Sagemaker hyper\n",
      "Amazon\n",
      "Sagemaker\n",
      "build a Jupiter\n",
      "Jupiter notebook\n",
      "data\n",
      "machine learning\n",
      "model\n",
      "Sagemaker outline\n",
      "module\n",
      "machine learning build\n",
      "time\n",
      "review\n",
      "wrap\n",
      "knowledge\n",
      "check\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_27:\n",
      "Aws Academy Machine\n",
      "Academy Machine learning\n",
      "Aws Academy\n",
      "Academy Machine\n",
      "Machine learning\n",
      "Amazon forecast\n",
      "Amazon\n",
      "Aws\n",
      "Academy\n",
      "Machine\n",
      "forecast\n",
      "time series data\n",
      "time series\n",
      "series data\n",
      "module\n",
      "forecasting\n",
      "data\n",
      "learning\n",
      "series\n",
      "time\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_28:\n",
      "time\n",
      "forecasting\n",
      "data\n",
      "patterns\n",
      "time series\n",
      "series\n",
      "pattern\n",
      "series data\n",
      "time series data\n",
      "variable\n",
      "important\n",
      "component\n",
      "time component\n",
      "opportunities\n",
      "year\n",
      "trend\n",
      "seasonal\n",
      "section\n",
      "applications\n",
      "Multivariate data\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_29:\n",
      "data\n",
      "time\n",
      "time series data\n",
      "time series\n",
      "time stamp\n",
      "series data\n",
      "missing\n",
      "sales data\n",
      "sales\n",
      "missing data\n",
      "series\n",
      "time time series\n",
      "data set\n",
      "grained time\n",
      "model\n",
      "time stamp data\n",
      "data points\n",
      "stamp\n",
      "finely grained time\n",
      "missing sales data\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_30:\n",
      "time series data\n",
      "time series\n",
      "data\n",
      "time\n",
      "series\n",
      "series data\n",
      "auto correlation\n",
      "correlation\n",
      "auto\n",
      "seasonality\n",
      "data set\n",
      "data sets\n",
      "algorithms\n",
      "series data seasonality\n",
      "series data sets\n",
      "sales\n",
      "forecast\n",
      "Amazon forecast\n",
      "set\n",
      "wrangling time series\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_31:\n",
      "Amazon forecast\n",
      "data\n",
      "forecast\n",
      "Amazon\n",
      "time\n",
      "time series data\n",
      "model\n",
      "quantile\n",
      "forecasts\n",
      "related data\n",
      "time series\n",
      "Amazon forecast predictors\n",
      "quantile loss\n",
      "series data\n",
      "gloves\n",
      "data set\n",
      "item\n",
      "predictor\n",
      "Amazon forecast provides\n",
      "Amazon forecast creates\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_32:\n",
      "Amazon forecast\n",
      "Amazon\n",
      "forecast\n",
      "back\n",
      "module\n",
      "describe\n",
      "time\n",
      "describe the business\n",
      "solved by Amazon\n",
      "review\n",
      "wrap\n",
      "Describe the challenges\n",
      "list\n",
      "time series\n",
      "business problem\n",
      "problem solved\n",
      "learned\n",
      "business\n",
      "problem\n",
      "solved\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_33:\n",
      "Aws Academy Machine\n",
      "Academy Machine Learning\n",
      "Aws Academy\n",
      "Academy Machine\n",
      "Academy\n",
      "Machine Learning\n",
      "Amazon\n",
      "Computer vision\n",
      "Aws\n",
      "back to Aws\n",
      "detection\n",
      "module\n",
      "custom data set\n",
      "data\n",
      "Computer\n",
      "vision\n",
      "machine learning services\n",
      "managed machine learning\n",
      "Amazon managed machine\n",
      "data set\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_34:\n",
      "computer vision\n",
      "computer\n",
      "vision\n",
      "image\n",
      "images\n",
      "Object\n",
      "computer vision technologies\n",
      "analysis\n",
      "detection\n",
      "objects\n",
      "computer vision machines\n",
      "introduce computer vision\n",
      "image analysis\n",
      "trained computer vision\n",
      "data\n",
      "video\n",
      "computer vision problems\n",
      "Object detection\n",
      "Object segmentation\n",
      "classification\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_35:\n",
      "Amazon recognition\n",
      "Amazon\n",
      "recognition\n",
      "image\n",
      "Amazon recognition SDK\n",
      "calls Amazon recognition\n",
      "Amazon recognition analyzes\n",
      "Amazon recognition interprets\n",
      "Aws\n",
      "Amazon recognition performs\n",
      "Amazon recognition integrates\n",
      "LAMBDA\n",
      "Amazon recognition include\n",
      "Amazon recognition including\n",
      "Amazon recognition makes\n",
      "lambda calls Amazon\n",
      "images\n",
      "Amazon recognition fall\n",
      "content\n",
      "Amazon recognition processes\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_36:\n",
      "Amazon recognition video\n",
      "Amazon recognition\n",
      "Amazon\n",
      "video\n",
      "recognition video\n",
      "recognition\n",
      "faces\n",
      "video stream\n",
      "image\n",
      "stream\n",
      "face\n",
      "facial recognition\n",
      "facial\n",
      "detection\n",
      "streaming video\n",
      "Amazon kinesis video\n",
      "Amazon recognition detects\n",
      "confidence\n",
      "images\n",
      "recognition video stream\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_37:\n",
      "Amazon recognition\n",
      "Amazon recognition custom\n",
      "Amazon\n",
      "images\n",
      "recognition\n",
      "model\n",
      "train Amazon recognition\n",
      "training\n",
      "custom\n",
      "recognition custom labels\n",
      "custom labels\n",
      "recognition custom\n",
      "labels\n",
      "machine\n",
      "data\n",
      "train Amazon\n",
      "machine learning\n",
      "set\n",
      "training images\n",
      "labeled Amazon recognition\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_38:\n",
      "Sagemaker Ground Truth\n",
      "Ground Truth\n",
      "Sagemaker Ground\n",
      "data\n",
      "Amazon Sagemaker Ground\n",
      "Amazon\n",
      "data set\n",
      "image\n",
      "label\n",
      "Amazon Sagemaker\n",
      "Ground\n",
      "Sagemaker\n",
      "Truth\n",
      "Amazon recognition custom\n",
      "automated data labeling\n",
      "labels\n",
      "Amazon recognition\n",
      "labeling\n",
      "data labeling\n",
      "label data\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_39:\n",
      "test data set\n",
      "Amazon recognition custom\n",
      "data set\n",
      "test data\n",
      "recognition custom labels\n",
      "label\n",
      "custom labels\n",
      "Amazon recognition\n",
      "data\n",
      "test\n",
      "model\n",
      "custom\n",
      "set\n",
      "labels\n",
      "Amazon\n",
      "image\n",
      "recognition custom\n",
      "test image\n",
      "entire test data\n",
      "recognition\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_40:\n",
      "custom labels\n",
      "model\n",
      "custom\n",
      "images\n",
      "label\n",
      "labels\n",
      "training\n",
      "image\n",
      "training images\n",
      "confidence\n",
      "Amazon recognition custom\n",
      "Amazon\n",
      "object\n",
      "recognition custom labels\n",
      "detect custom labels\n",
      "custom labels operation\n",
      "objects\n",
      "custom labels model\n",
      "custom label includes\n",
      "threshold\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_41:\n",
      "time to summarize\n",
      "main points\n",
      "module\n",
      "custom data set\n",
      "Amazon\n",
      "computer vision\n",
      "prepare a custom\n",
      "custom data\n",
      "Amazon Sagemaker ground\n",
      "Amazon Sagemaker\n",
      "describe\n",
      "data set\n",
      "vision\n",
      "time\n",
      "summarize\n",
      "main\n",
      "points\n",
      "Amazon managed machine\n",
      "prepare\n",
      "custom\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_42:\n",
      "Academy Machine learning\n",
      "Aws Academy Machine\n",
      "Machine learning introduction\n",
      "Academy Machine\n",
      "natural language processing\n",
      "Machine learning\n",
      "Aws Academy\n",
      "natural language\n",
      "language processing\n",
      "learning introduction\n",
      "introduce natural language\n",
      "Academy\n",
      "Machine\n",
      "managed Amazon\n",
      "language\n",
      "introduction to natural\n",
      "natural\n",
      "processing\n",
      "Amazon\n",
      "module\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_43:\n",
      "words\n",
      "text\n",
      "word\n",
      "NLP\n",
      "machine learning\n",
      "Alexa\n",
      "Amazon\n",
      "machine\n",
      "language\n",
      "natural language processing\n",
      "data\n",
      "term\n",
      "model\n",
      "learning\n",
      "word run\n",
      "NLP systems\n",
      "natural language\n",
      "phrases\n",
      "context\n",
      "applications\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_44:\n",
      "Amazon\n",
      "Amazon Lex\n",
      "Amazon Polly\n",
      "Amazon transcribe\n",
      "Amazon comprehend\n",
      "Amazon translate\n",
      "Lex\n",
      "Polly\n",
      "transcribe\n",
      "cases for Amazon\n",
      "Amazon Lex solution\n",
      "combining Amazon Lex\n",
      "instructs Amazon Polly\n",
      "language\n",
      "create\n",
      "comprehend\n",
      "Amazon comprehend create\n",
      "applications\n",
      "translate\n",
      "cases\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_45:\n",
      "back\n",
      "module\n",
      "describe the NLP\n",
      "describe\n",
      "managed Amazon\n",
      "managed\n",
      "services\n",
      "NLP use cases\n",
      "time to review\n",
      "NLP\n",
      "Amazon\n",
      "time\n",
      "review\n",
      "wrap\n",
      "summary\n",
      "Amazon ML services\n",
      "Good job\n",
      "Good\n",
      "job\n",
      "watching\n",
      "\n",
      " showing all the keywords from the videos demo1-job_name_1.1_46:\n",
      "Machine Learning\n",
      "Aws\n",
      "Aws certification\n",
      "Aws certified machine\n",
      "Aws Academy Machine\n",
      "Learning\n",
      "Machine\n",
      "certification\n",
      "Aws certified\n",
      "certified machine learning\n",
      "Aws Academy\n",
      "Academy Machine Learning\n",
      "exam\n",
      "machine learning specialty\n",
      "certified machine\n",
      "Academy Machine\n",
      "Aws Certification exam\n",
      "Amazon machine learning\n",
      "machine learning solutions\n",
      "certified\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yake\n",
    "with open('./combined_output.json') as f:\n",
    "    data = json.load(f)\n",
    "language = \"en\"\n",
    "kw_extractor = yake.KeywordExtractor(lan=language )\n",
    "for key, value in data.items():\n",
    "    key_words = kw_extractor.extract_keywords(value)\n",
    "    print(f\" showing all the keywords from the videos {key}:\")\n",
    "    for k in key_words:\n",
    "        print(k[0])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0c614af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: spacy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (3.5.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (8.1.9)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (1.1.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (65.6.3)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (1.10.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (1.22.3)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.8)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->spacy) (2.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75d1ae07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting en-core-web-sm==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.5.0/en_core_web_sm-3.5.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from en-core-web-sm==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.22.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.28.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (65.6.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging>=20.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==3.5.0) (2.1.1)\n",
      "\u001b[38;5;2m Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dda4c270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video: demo1-job_name_1.1_1\n",
      "All the text from the video: Hi and welcome to Amazon Academy of Machine Learning Foundations in this module, you'll learn about the course objectives, various job roles in the machine learning domain and where you can go to learn more about machine learning. After completing this module, you should be able to identify course prerequisites and objectives indicate the role of the data scientist in business and identify resources for further learning. We're now going to look at the prerequisites for taking this course. Before you take this course, we recommend that you first complete Aws Academy Cloud Foundations. You should also have some general technical knowledge of it including foundational computer literacy skills like basic computer concepts, email file management and a good understanding of the internet. We also recommend that you have intermediate skills with Python programming and a general knowledge of applied statistics. Finally, general business knowledge is important for this course. This includes insight into how information technology is used in business. It's also important to have business related skill sets such as communication skills, leadership skills, and an orientation towards customer service. In this course, you'll be introduced to the key concepts of machine learning, its tools and its uses you'll also be introduced to and work with some of the AWS services for machine learning. You'll learn how to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology. Identify how machine learning can be used to solve a business problem. Describe the machine learning process. List the tools available to data scientists and identify when to use machine learning instead of traditional software development methods. As part of this course, you'll also learn how to implement a machine learning pipeline. This includes how to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupiter notebook by using Amazon Sagemaker outline the process for evaluating data. Explain why data needs to be pre processed and use open source tools to examine and prep process data. You will also use Amazon's Sage maker to train and host a machine learning model, use cross validation to test the performance of a machine learning model. Use a hosted model for inference, create an Amazon Sage maker hyper parameter tuning job to optimize a model's effectiveness. And finally how to use managed Amazon machine learning services to solve specific machine learning problems in forecasting computer vision and natural language processing. We'll now review the course outline to achieve the course objectives. You'll complete the following modules to start in module two, you'll get an introduction to machine learning in module three. You'll learn how to implement a machine learning pipeline with Amazon Sagemaker modules 45 and six describe how to apply managed Amazon machine learning services for problems in forecasting computer vision and natural language processing. Finally, module seven is a summary of the course. It also includes an overview of steps you can take to work towards the AWS certified machine learning specialty. The next five slides provide more detail about the subtopics covered in each module. The purpose of module two is to introduce you to major concepts for understanding machine learning. Section one describes the overall field of machine learning and how machine learning relates to artificial intelligence and deep learning. In section two, you'll learn about some of the most common business problems you can solve with machine learning. Section three describes the general workflow for solving machine learning problems. You'll also learn some of the more common machine learning terms. In section four, you'll review some of the commonly used tools by machine learning professionals. And lastly in section five, you'll get an overview of some of the common challenges you'll face when working with machine learning problems. In module three, you'll get an introduction to Amazon's sagemaker and how you can use it to implement a machine learning pipeline. The module focuses on the application of machine learning to solve problems with several public domain data sets. As examples of the machine learning pipeline. Section one introduces you to defining business problems and the data sets we will use during this module. Section two through eight described the phases of the machine learning pipeline by using computer vision as an example application. In section two, you'll learn how to collect and secure data. Section three describes different techniques for evaluating data. In section four, you'll learn about the process of feature engineering. Section five described the steps he'll take to train a model with Sagemaker. In section six, you'll get an overview of the options in Sagemaker for hosting and using a model. Finally, section seven and eight cover how to evaluate and tune your model with Sagemaker. In this module, you'll be introduced to using machine learning to create forecasts based on a time series data. In section one, you'll be introduced to forecasting in some of its common applications. Section two outlined some of the pitfalls of using time series data to make forecasts. Finally, in section three, you'll get an overview of how to use Amazon forecast in this module, you'll learn about using machine learning for computer vision. Section one describes the general problems you can solve with computer vision. In section two, you'll learn about the process for analyzing images and videos. And in section three, you'll learn the steps you'll need to take to prepare data sets for computer vision. In this module, you'll be introduced to natural language processing with machine learning. In section one, you'll learn about the general set of problems you can solve with natural language processing. Section two reviews some of the managed Amazon machine learning services you can use to address natural language processing problems. These services include Amazon, transcribe, Amazon translate, Amazon, Lex, Amazon comprehend and Amazon Poly module seven is the final module of the course. In this module, you'll review what you've learned throughout this course. You'll also be introduced to the next steps you should take. If you want to achieve the AWS Certified Machine learning specialty section. One of this module summarizes the topics you've covered in this course. In section two, you'll learn more about the Aws documentation. You'll also review two common frameworks for applying Aws services. And finally, section three describes the steps you should take. If you want to continue working towards the Aws certified machine learning specialty in this section, you'll learn about some of the more common job roles for machine learning professionals. If you're interested in a data scientist role, focus on developing analytical statistical and programming skills. As a data scientist, you'll use those skills to collect analyze and interpret large data sets. Some universities now offer degrees in data science, but data scientists often have degrees in related fields like statistics, math, computer science or economics. As a data scientist, you'll need technical competencies in statistics, machine learning programming languages and data analytics. If you'd like to have a career as a machine learning engineer, the skills you'll need will be similar to a data scientist skill set like data scientists, machine learning engineers also require technical competencies in statistics and machine learning. However, you'll focus more on programming skills and software architecture than analysis and interpretation. As a machine learning engineer, you'll apply those programming and architecture skills to design and develop machine learning systems. Machine learning engineers often have previous experience with software development and they rely more heavily on programming and software engineering than other machine learning roles. You might also be interested in a career in science where you can apply machine learning technology to your field. Machine learning is having an impact in everything from astronomy to zoology. So there are many different paths open to you. As an applied science researcher, your primary focus will be on the type of science you're working on. You'll need some of the same skills as a data scientist, but you'll also need to know how to apply those skills to your chosen domain. Thus, applied science roles also require technical competencies in statistics and machine learning. Many software developers are now integrating machine learning into their applications. If you're interested in a career as a software developer, you should also include machine learning technology in your studies. As a machine learning developer, your primary focus will be software development skills, but you'll also need some of the same skills as a data scientist. So make sure you take course work in statistics and applied mathematics. And here's a final note for this module, we recommend reviewing your student guide in your student guide, you'll find links to documentation and other resources you'll use throughout the course. That's it for this introduction. Thanks for watching. We'll see you in the next video.\n",
      "All the key phrases from the video: ['Amazon Academy', 'Machine Learning Foundations', 'this module', 'you', 'the course objectives', 'various job roles', 'the machine learning domain', 'you', 'machine learning', 'this module', 'you', 'course prerequisites', 'objectives', 'the role', 'the data scientist', 'business', 'resources', 'further learning', 'We', 'the prerequisites', 'this course', 'you', 'this course', 'we', 'you', 'Aws Academy Cloud Foundations', 'You', 'some general technical knowledge', 'it', 'foundational computer literacy skills', 'basic computer concepts', 'email file management', 'a good understanding', 'the internet', 'We', 'you', 'intermediate skills', 'Python programming', 'a general knowledge', 'applied statistics', 'general business knowledge', 'this course', 'This', 'insight', 'information technology', 'business', 'It', 'business related skill sets', 'communication skills', 'leadership skills', 'an orientation', 'customer service', 'this course', 'you', 'the key concepts', 'machine learning', 'its tools', 'its uses', 'you', 'some', 'the AWS services', 'machine learning', 'You', 'machine learning', 'deep learning', 'part', 'artificial intelligence', 'artificial intelligence and machine learning terminology', 'machine learning', 'a business problem', 'the machine learning process', 'the tools', 'data scientists', 'traditional software development methods', 'part', 'this course', 'you', 'a machine learning pipeline', 'This', 'a problem', 'a business request', 'data', 'machine learning', 'a Jupiter notebook', 'Amazon Sagemaker', 'the process', 'data', 'data', 'open source tools', 'prep process data', 'You', \"Amazon's Sage maker\", 'a machine learning model', 'cross validation', 'the performance', 'a machine learning model', 'a hosted model', 'inference', 'an Amazon Sage maker hyper parameter', 'job', \"a model's effectiveness\", 'managed Amazon machine learning services', 'specific machine learning problems', 'computer vision', 'natural language processing', 'We', 'the course outline', 'the course objectives', 'You', 'the following modules', 'module', 'you', 'an introduction', 'machine learning', 'module', 'You', 'a machine learning pipeline', 'Amazon Sagemaker', 'managed Amazon machine learning services', 'problems', 'computer vision', 'natural language processing', 'module', 'a summary', 'the course', 'It', 'an overview', 'steps', 'you', 'the AWS certified machine learning specialty', 'The next five slides', 'more detail', 'the subtopics', 'each module', 'The purpose', 'module', 'you', 'major concepts', 'machine learning', 'Section', 'the overall field', 'machine learning', 'machine learning', 'artificial intelligence', 'deep learning', 'section', 'you', 'some', 'the most common business problems', 'you', 'machine learning', 'Section', 'the general workflow', 'machine learning problems', 'You', 'some', 'the more common machine learning terms', 'section', 'you', 'some', 'the commonly used tools', 'machine learning professionals', 'section', 'you', 'an overview', 'some', 'the common challenges', 'you', 'machine learning problems', 'module', 'you', 'an introduction', \"Amazon's sagemaker\", 'you', 'it', 'a machine learning pipeline', 'The module', 'the application', 'problems', 'several public domain data sets', 'examples', 'the machine learning pipeline', 'Section', 'you', 'business problems', 'the data sets', 'we', 'this module', 'Section', 'the phases', 'the machine learning pipeline', 'computer vision', 'an example application', 'section', 'you', 'data', 'Section', 'different techniques', 'data', 'section', 'you', 'the process', 'feature engineering', 'Section', 'the steps', 'he', 'a model', 'Sagemaker', 'section', 'you', 'an overview', 'the options', 'Sagemaker', 'a model', 'section', 'your model', 'Sagemaker', 'this module', 'you', 'machine learning', 'forecasts', 'a time series data', 'section', 'you', 'some', 'its common applications', 'Section', 'some', 'the pitfalls', 'time series data', 'forecasts', 'section', 'you', 'an overview', 'Amazon forecast', 'this module', 'you', 'machine learning', 'computer vision', 'Section', 'the general problems', 'you', 'computer vision', 'section', 'you', 'the process', 'images', 'videos', 'section', 'you', 'the steps', 'you', 'data sets', 'computer vision', 'this module', 'you', 'natural language processing', 'machine learning', 'section', 'you', 'the general set', 'problems', 'you', 'natural language processing', 'Section', 'some', 'the managed Amazon machine learning services', 'you', 'natural language processing problems', 'These services', 'Amazon', 'Amazon', 'Lex', 'Amazon', 'Amazon Poly module', 'the final module', 'the course', 'this module', 'you', 'what', 'you', 'this course', 'You', 'the next steps', 'you', 'you', 'the AWS', 'specialty section', 'this module', 'the topics', 'you', 'this course', 'section', 'you', 'the Aws documentation', 'You', 'two common frameworks', 'Aws services', 'section', 'the steps', 'you', 'you', 'the Aws certified machine learning specialty', 'this section', 'you', 'some', 'the more common job roles', 'machine learning professionals', 'you', 'a data scientist role', 'analytical statistical and programming skills', 'a data scientist', 'you', 'those skills', 'large data sets', 'Some universities', 'degrees', 'data science', 'data scientists', 'degrees', 'related fields', 'statistics', 'math', 'computer science', 'economics', 'a data scientist', 'you', 'technical competencies', 'statistics', 'machine learning programming languages', 'data analytics', 'you', 'a career', 'a machine learning engineer', 'the skills', 'you', 'a data scientist skill', 'data scientists', 'machine learning engineers', 'technical competencies', 'statistics', 'machine learning', 'you', 'programming skills', 'software architecture', 'analysis', 'interpretation', 'a machine learning engineer', 'you', 'those programming and architecture skills', 'machine learning systems', 'Machine learning engineers', 'previous experience', 'software development', 'they', 'programming and software engineering', 'other machine learning roles', 'You', 'a career', 'science', 'you', 'machine learning technology', 'your field', 'Machine learning', 'an impact', 'everything', 'astronomy', 'zoology', 'many different paths', 'you', 'an applied science researcher', 'your primary focus', 'the type', 'science', 'you', 'You', 'some', 'the same skills', 'a data scientist', 'you', 'those skills', 'your chosen domain', 'applied science roles', 'technical competencies', 'statistics', 'machine learning', 'Many software developers', 'their applications', 'you', 'a career', 'a software developer', 'you', 'machine learning technology', 'your studies', 'a machine learning developer', 'your primary focus', 'software development skills', 'you', 'some', 'the same skills', 'a data scientist', 'you', 'work', 'statistics', 'applied mathematics', 'a final note', 'this module', 'we', 'your student guide', 'your student guide', 'you', 'links', 'documentation', 'other resources', 'you', 'the course', 'That', 'it', 'this introduction', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_2\n",
      "All the text from the video: Hi and welcome to module two of Aws Academy machine learning in this module, we're going to introduce machine learning. We'll first look at the business problems that can be solved by machine learning. We'll then talk about terminology, process tools and some of the challenges you'll face. After completing this module, you should be able to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology. Identify how machine learning can be used to solve a business problem. Describe the machine learning process list the tools available to data scientists and identify when to use machine learning instead of traditional software development methods. You're now ready to get started with. Section one. See you in the next video.\n",
      "All the key phrases from the video: ['this module', 'we', 'machine learning', 'We', 'the business problems', 'that', 'machine learning', 'We', 'terminology', 'process tools', 'some', 'the challenges', 'you', 'this module', 'you', 'machine learning', 'deep learning', 'part', 'artificial intelligence', 'artificial intelligence and machine learning terminology', 'machine learning', 'a business problem', 'the machine learning process', 'the tools', 'data scientists', 'traditional software development methods', 'You', 'Section', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_3\n",
      "All the text from the video: Hi and welcome to section one in this section. We're going to talk about what machine learning is. This course is an introduction to machine learning, which is also known as M L. But first we'll discuss where machine learning fits into the larger picture. Machine learning is a subset of artificial intelligence or A I. This is a broad branch of computer science that's focused on building machines that can do human tasks. Deep learning is a subdomain of machine learning to understand where these all fit together. We'll discuss each one. As we just mentioned, machine learning is a subset of a broader computer science field known as artificial intelligence. A I focuses on building machines that can perform tasks. A human would typically perform in contemporary popular culture. You've probably seen A I S in movies, television or works of fiction. For example, you might have seen A I S that control the world around them or that start acting on their own initiative. These A I S started as computer agents that perceive their environments and take actions to achieve a specific goal though maybe not the outcome the creators originally wished for other fictional A I S interact extensively with humans as helpers or workers and they generally do a better job working with humanity. But they're more general in purpose. These kinds of A I S are examples of artificial general intelligence or A G I. They have the capacity to learn or understand any task that a human can A I problems typically span many fields of research such as natural language processing, reasoning, knowledge, representation, learning, perception and physical environment, interaction. A I isn't yet a reality for us unless we are all truly living in a simulation. But every year we move closer to it. In each of those areas, you might have also read or seen commentary on the ethics of creating A I. Not all views are positive, perhaps partly in fear of the malicious fictional A I S that want to destroy humanity or use them as power sources or perhaps they're concerned about the risk of mass unemployment because an intelligent machine could work 24 7 and not need any breaks. Don't worry though we're not going to build the next ro A I in this course. Maybe in the next one, if you do a search, you'll probably find many definitions of machine learning. There isn't a universally agreed upon definition. So we'll start by looking at a couple of definitions. For example, we could say machine learning is the scientific study of algorithms and statistical models to perform a task by using inference instead of instructions. This isn't a bad starting point. The key point here is using algorithms and statistical models instead of instructions to help you better understand this. We'll apply this idea to a concrete example. Suppose you need to write an application that determines if an email message is spam or not. Without machine learning, you'd need to write a complex series of decision statements using if and statements. You'd also need to use words in the subject or body, the number of links and the length of the message to determine if an email message is spam, it would be hard and labor intensive to build a large set of rules covering every possibility with machine learning. However, you could use a list of email messages that were marked as spam or not spam and train a machine learning model. The model would then learn which patterns of words length and other attributes are good indicators of spam messages. If you presented the model with an email message, it hadn't seen before. The model would perform a prediction to say whether the message was spam or not spam. Deep learning represents a significant leap forward in the capabilities for artificial intelligence and machine learning. The theory behind deep learning was created from how the human brain works. An artificial neural network or A N N is inspired by the biological neurons found in the brain. Although the implementation has become very different, artificial neurons have one or more inputs. And a single output. These neurons fire or activate their outputs based on a transformation of the inputs. A neural network is composed of layers of these artificial neurons with connections between the layers. There are typically input output and hidden layers in the network. The output of a single neuron connects to the inputs of all the neurons. In the next layer, the network is then asked to solve a problem. The input layer is populated from the training data and the neurons activate throughout the layers until an answer is presented in the output layer. The accuracy of the output is then measured if the output doesn't meet your threshold, the training is repeated. But with slight changes to the weights of the connections between neurons, the neural network will do this repeatedly each time it strengthens the connections that lead to success and diminishes the connections that lead to failure. As you'll see in this course, machine learning practitioners spend a lot of time optimizing the ML models, selecting the best data features to train with and selecting the models with the best results. In contrast, deep learning practitioners spend almost no time on those tasks. Instead, they spend their time modeling data with different A N N architectures. Though the theory for deep learning goes back decades, the hardware needed to run deep learning problems wasn't generally accessible until recently. But now that it's available, you can use deep learning to address problems that are more complex than the problems you could have worked on before mainstream machine learning is a recent occurrence, rapid advancements in machine and deep learning only started around the mid two thousands. This is partly because Moore's law and the rise of cloud computing resulted in easier access to larger, faster and cheaper compute and storage capabilities. You can now rent computing power for a few hours for pennies before this, you needed substantial investments to buy and operate large scale compute clusters on your own. In 2012 neural networks started to be used in the image net large scale visual recognition challenge and machine learning competition for image recognition. The accuracy rate jumped up to about 82% and has been steadily climbing ever since. In fact, it exceeded human performance in 2015. Here are some of the key takeaways for this section. First artificial intelligence is the broad field of building machines to perform human tasks. Also, machine learning is a subset of A I. It focuses on using data to train machine learning models so they can make predictions. Deep learning is a technique inspired from human biology. It uses layers of artificial neurons to build networks that solve problems. And last advancements in technology, cloud computing and algorithm development have led to a corresponding advance in machine learning capabilities and applications. That's it for this section, we'll see you in the next video.\n",
      "All the key phrases from the video: ['Hi and welcome', 'section', 'this section', 'We', 'what', 'machine learning', 'This course', 'an introduction', 'machine learning', 'which', 'M L.', 'we', 'machine learning', 'the larger picture', 'Machine learning', 'a subset', 'artificial intelligence', 'This', 'a broad branch', 'computer science', 'that', 'building machines', 'that', 'human tasks', 'Deep learning', 'a subdomain', 'these all', 'We', 'each one', 'we', 'machine learning', 'a subset', 'a broader computer science field', 'artificial intelligence', 'A', 'I', 'machines', 'that', 'tasks', 'A human', 'contemporary popular culture', 'You', 'A I S', 'movies', 'television', 'works', 'fiction', 'example', 'you', 'A I S', 'that', 'the world', 'them', 'that', 'their own initiative', 'These A I S', 'computer agents', 'that', 'their environments', 'actions', 'a specific goal', 'the creators', 'other fictional A', 'I S', 'humans', 'helpers', 'workers', 'they', 'a better job', 'humanity', 'they', 'purpose', 'These kinds', 'A I S', 'examples', 'artificial general intelligence', 'They', 'the capacity', 'any task', 'I', 'many fields', 'research', 'natural language processing', 'reasoning', 'knowledge', 'representation', 'learning', 'perception', 'physical environment', 'interaction', 'A', 'I', 'a reality', 'us', 'we', 'a simulation', 'we', 'it', 'each', 'those areas', 'you', 'commentary', 'the ethics', 'A I.', 'Not all views', 'fear', 'the malicious fictional A I S', 'that', 'humanity', 'them', 'power sources', 'they', 'the risk', 'mass unemployment', 'an intelligent machine', 'any breaks', 'we', 'A', 'I', 'this course', 'the next one', 'you', 'a search', 'you', 'many definitions', 'machine learning', 'a', 'definition', 'we', 'a couple', 'definitions', 'example', 'we', 'machine learning', 'the scientific study', 'algorithms', 'statistical models', 'a task', 'inference', 'instructions', 'This', 'a bad starting point', 'The key point', 'algorithms', 'statistical models', 'instructions', 'you', 'this', 'We', 'this idea', 'a concrete example', 'you', 'an application', 'that', 'an email message', 'spam', 'machine learning', 'you', 'a complex series', 'decision statements', 'You', 'words', 'the subject', 'body', 'the number', 'links', 'the length', 'the message', 'an email message', 'spam', 'it', 'labor', 'a large set', 'rules', 'every possibility', 'machine learning', 'you', 'a list', 'email messages', 'that', 'spam', 'a machine learning model', 'The model', 'which patterns', 'words length', 'other attributes', 'good indicators', 'spam messages', 'you', 'the model', 'an email message', 'it', 'The model', 'a prediction', 'the message', 'spam', 'not spam', 'Deep learning', 'a significant leap', 'the capabilities', 'artificial intelligence', 'machine learning', 'The theory', 'deep learning', 'the human brain', 'An artificial neural network', 'A N N', 'the biological neurons', 'the brain', 'the implementation', 'artificial neurons', 'one or more inputs', 'And a single output', 'These neurons', 'their outputs', 'a transformation', 'the inputs', 'A neural network', 'layers', 'these artificial neurons', 'connections', 'the layers', 'input output', 'hidden layers', 'the network', 'The output', 'a single neuron', 'the inputs', 'all the neurons', 'the next layer', 'the network', 'a problem', 'The input layer', 'the training data', 'the neurons', 'the layers', 'an answer', 'the output layer', 'The accuracy', 'the output', 'the output', 'your threshold', 'the training', 'slight changes', 'the weights', 'the connections', 'neurons', 'the neural network', 'this', 'it', 'the connections', 'that', 'success', 'the connections', 'that', 'failure', 'you', 'this course', 'machine learning practitioners', 'a lot', 'time', 'the ML models', 'the best data features', 'the models', 'the best results', 'contrast', 'deep learning practitioners', 'almost no time', 'those tasks', 'they', 'their time', 'data', 'different A N N architectures', 'the theory', 'deep learning', 'the hardware', 'deep learning problems', 'it', 'you', 'deep learning', 'problems', 'that', 'the problems', 'you', 'mainstream machine learning', 'a recent occurrence', 'rapid advancements', 'machine', 'deep learning', 'the mid two thousands', 'This', \"Moore's law\", 'the rise', 'cloud computing', 'easier access', 'larger, faster and cheaper compute and storage capabilities', 'You', 'computing power', 'a few hours', 'pennies', 'this', 'you', 'substantial investments', 'large scale compute clusters', '2012 neural networks', 'the image', 'net large scale visual recognition challenge and machine learning competition', 'image recognition', 'The accuracy rate', 'about 82%', 'fact', 'it', 'human performance', 'some', 'the key takeaways', 'this section', 'First artificial intelligence', 'the broad field', 'building machines', 'human tasks', 'machine learning', 'a subset', 'A I.', 'It', 'data', 'machine learning models', 'they', 'predictions', 'Deep learning', 'a technique', 'human biology', 'It', 'layers', 'artificial neurons', 'networks', 'that', 'problems', 'last advancements', 'technology', 'cloud computing', 'algorithm development', 'a corresponding advance', 'machine learning capabilities', 'applications', 'That', 'it', 'this section', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_4\n",
      "All the text from the video: Hi and welcome back in this section. We're going to look at the types of business problems. Machine learning can help you solve. Machine learning is used all across your digital lives. Your email spam filter is the result of a machine learning program that was trained with examples of spam and regular email messages based on books. You're reading or products you bought machine learning programs can predict other books or products you're likely to be interested in. Again, the machine learning program was trained with data from other readers habits and purchases. When detecting credit card fraud, the machine learning program was trained on examples of transactions that turned out to be fraud along with normal transactions. You can probably think of many more examples from social media applications using facial detection to group your photos to detecting brain tumors in brain scans or finding anomalies in x rays. There are three main types of machine learning. There's supervised learning where a model uses known inputs and outputs to generalize future outputs. There's unsupervised learning where the model doesn't know inputs or outputs. So it finds patterns in the data without help. And there's reinforcement learning where the model interacts with its environment and learns to take actions that will maximize rewards. It's important to know the different types of M L because the type will guide you towards selecting algorithms that make sense for solving your business problem. Let's look more into each of these types. Supervised learning is a popular type of M L because it's widely applicable, it's called supervised learning because there needs to be a supervisor, a teacher who can show the right answers, so to speak. Like any student, a supervised algorithm needs to learn. By example, essentially it needs a teacher who uses training data to help it determine the patterns and relationships between the inputs and outputs. If you want to build an application to detect credit card fraud, you'd need training data that includes examples of fraud and examples of normal transactions within supervised learning. There are different types of problems, classification and regression. There are two subtypes of classification problems. The first is binary classification. Think back to the example with identifying fraudulent transactions, the target variable in this example is limited to two options, fraudulent or not fraudulent. This is a binary classification problem. There are also multi class classification problems. These M L problems classify an observation into one of three or more categories. Say that you have an M L model that predicts why a customer is calling your store. So you can reduce the number of transfers needed before the customer gets to the correct customer support department. In this case, the different customer support departments represent the variety of potential target variables, which could be many different departments much more than just two. There are also regression problems in a regression problem. You're no longer mapping an input to a defined number of categories. Instead you're mapping an input to a continuous value like an integer. One example of an M L regression problem is predicting the price of a company's stock. Computer vision is a good example of supervised learning. Is this a cat or a dog? Is there a tumor in this x-ray computer vision is often built with deep learning models. It automates the extraction analysis, classification and understanding of useful information from a single image or a sequence of images. Computer vision enables machines to identify people, places and things and images with accuracy at or above human levels and with greater speed and efficiency. The image data can take many forms such as single images, video sequences, views from multiple cameras or three dimensional data. You'll learn more about computer vision. Later in this course, we'll now discuss unsupervised machine learning. Sometimes all you have is the data. There's no supervisor in the room in unsupervised learning labels aren't provided like they are with supervised learning. You don't know all the variables and patterns in these instances. The machine has to uncover and create the labels itself. These models use the data they're presented with to detect emerging properties of the entire data set. Then they construct patterns from those properties. Clustering is a common subcategory of unsupervised learning. This kind of algorithm groups data into different clusters based on similar features. It does this to better understand the attributes of a specific cluster. For example, by analyzing customer purchasing habits, unsupervised algorithms can identify groups of customers that are associated with the size tier of a company. The advantage of unsupervised algorithms is that they enable you to see patterns in the data that you weren't aware of before natural language processing is also known as N LP. This is another area of machine learning that's experiencing growth. If you've ever used Alexa or any other voice assistant, they'll use N LP to try and answer your question. N LP isn't just about speech, it's also about written text. N LP shows up in many applications. For example, N LP is used with chat or call center bots which are automated systems that help you get your bank balance or order food from a restaurant. You can use N LP and translation tools which convert text between languages. For example, you might use applications that translate menus in real time. N LP is also used in voice to text translations which converts spoken words into text. And finally NLP can be used in sentiment analysis which you can use to analyze the sentiment of comments and reviews of products, music and movies. These sentiments could be used to give the movie an audience rating, you'll learn more about N LP later in this course. Another kind of machine learning that's been gaining popularity recently is reinforcement learning. Unlike other machine learning reinforcement learning continuously improves its model by mining feedback from previous iterations. In reinforcement learning, an agent continuously learns through trial and error as it interacts in an environment. Reinforcement learning is broadly useful when the reward of a desired outcome is known. But the path to achieving it isn't. And that path requires a lot of trial and error to discover. Take the example of Aws Deep Racer in the Aws Deep Racer simulator. The agent is the virtual car. The environment is a virtual race track. The actions are throttle and steering inputs to the car. And the goal is completing the race track as quickly as possible. Without deviating from the track. The car needs to learn the desired driving behavior to reach the goal of completing the track for the car to learn this. Aws Deep racer teams use rewards to incentivize their model to learn the desired driving behavior. In reinforcement learning the thing driving, the learning is called the agent. In this case, it's the Aws deep racer car. The environment is the place where the agent learns, which in this example would be the marked racetrack. When the agent does something in the environment that provokes a response such as crossing a boundary, it shouldn't cross, that's called an action, that response is called a reward or penalty. Depending on whether the agent did something to be reinforced or discouraged in the model. As the agent moves within the environment, its action should start receiving more rewards and fewer penalties until it meets the desired business outcome. Self driving vehicles, bring together many machine and deep learning algorithms and models to solve the problem of driving from point A to point B. Two of its main tasks are the continuous detection of the environment and forecasting changes. These involve detecting objects and localizing and predicting the movement of the detected objects. The outputs of these findings act as inputs to other systems that make decisions on what they should do with the vehicle's various controls. There are use cases in self driving vehicles that require real time responses to the environment. For example, if a previously hidden pedestrian walks out from behind an obstacle, the vehicle brakes need to be applied immediately, there can be no latency or room for error with these actions. Not every problem should be solved with machine learning. Sometimes regular programming will work well for your needs. If you're interested in exploring a potential machine learning solution, look for the existence of large data sets and a large number of variables. Machine learning is often the best choice if you're uncertain of the business logic or procedures needed to obtain an answer or accomplish a task, machine learning systems can be complex. The supporting infrastructure management support and technical expertise need to be in place to help ensure the project's success. Here are the key takeaways for this section where we explored some machine learning applications that are already part of everyday life. First, machine learning problems can be grouped into three categories. Supervised learning is where you have training data where you already know the answer, unsupervised learning is where you have data but are looking for insights within the data reinforcement learning is where the model learns based on experience and feedback. Most business problems are supervised learning problems. Well, that's it for this section. We'll see you in the next video.\n",
      "All the key phrases from the video: ['this section', 'We', 'the types', 'business problems', 'Machine learning', 'you', 'Machine learning', 'your digital lives', 'Your email spam filter', 'the result', 'a machine learning program', 'that', 'examples', 'spam', 'regular email messages', 'books', 'You', 'products', 'you', 'machine learning programs', 'other books', 'products', 'you', 'the machine learning program', 'data', 'other readers habits', 'purchases', 'credit card fraud', 'the machine learning program', 'examples', 'transactions', 'that', 'fraud', 'normal transactions', 'You', 'many more examples', 'social media applications', 'facial detection', 'your photos', 'brain tumors', 'brain scans', 'anomalies', 'rays', 'three main types', 'machine learning', 'supervised learning', 'a model', 'known inputs', 'outputs', 'future outputs', 'unsupervised learning', 'the model', 'inputs', 'outputs', 'it', 'patterns', 'the data', 'help', 'reinforcement learning', 'the model', 'its environment', 'actions', 'that', 'rewards', 'It', 'the different types', 'M L', 'the type', 'you', 'algorithms', 'that', 'sense', 'your business problem', \"'s\", 'each', 'these types', 'Supervised learning', 'a popular type', 'M L', 'it', 'it', 'supervised learning', 'a supervisor', 'a teacher', 'who', 'the right answers', 'any student', 'a supervised algorithm', 'example', 'it', 'a teacher', 'who', 'training data', 'it', 'the patterns', 'relationships', 'the inputs', 'outputs', 'you', 'an application', 'credit card fraud', 'you', 'training data', 'that', 'examples', 'fraud', 'examples', 'normal transactions', 'supervised learning', 'different types', 'problems', 'classification', 'regression', 'two subtypes', 'classification problems', 'binary classification', 'the example', 'fraudulent transactions', 'the target variable', 'this example', 'two options', 'This', 'a binary classification problem', 'multi class classification problems', 'These M L problems', 'an observation', 'three or more categories', 'you', 'an M L model', 'that', 'a customer', 'your store', 'you', 'the number', 'transfers', 'the customer', 'the correct customer support department', 'this case', 'the different customer support departments', 'the variety', 'potential target variables', 'which', 'many different departments', 'regression problems', 'a regression problem', 'You', 'an input', 'a defined number', 'categories', 'you', 'an input', 'a continuous value', 'an integer', 'One example', 'an M L regression problem', 'the price', \"a company's stock\", 'Computer vision', 'a good example', 'supervised learning', 'this', 'a cat', 'a dog', 'a tumor', 'this x-ray computer vision', 'deep learning models', 'It', 'the extraction analysis', 'classification', 'understanding', 'useful information', 'a single image', 'a sequence', 'images', 'Computer vision', 'machines', 'people', 'places', 'things', 'images', 'accuracy', 'human levels', 'greater speed', 'efficiency', 'The image data', 'many forms', 'single images', 'video sequences', 'views', 'multiple cameras', 'three dimensional data', 'You', 'computer vision', 'this course', 'we', 'unsupervised machine learning', 'all', 'you', 'the data', 'no supervisor', 'the room', 'unsupervised learning labels', 'they', 'supervised learning', 'You', 'all the variables', 'patterns', 'these instances', 'The machine', 'the labels', 'itself', 'These models', 'the data', 'they', 'emerging properties', 'the entire data set', 'they', 'patterns', 'those properties', 'Clustering', 'a common subcategory', 'unsupervised learning', 'This kind', 'algorithm groups', 'different clusters', 'similar features', 'It', 'this', 'the attributes', 'a specific cluster', 'example', 'customer purchasing habits', 'unsupervised algorithms', 'groups', 'customers', 'that', 'the size tier', 'a company', 'The advantage', 'unsupervised algorithms', 'they', 'you', 'patterns', 'the data', 'that', 'you', 'before natural language processing', 'N LP', 'This', 'another area', 'that', 'growth', 'you', 'Alexa', 'any other voice assistant', 'they', 'N LP', 'your question', 'N LP', 'speech', 'it', 'written text', 'N LP', 'many applications', 'example', 'N LP', 'chat or call center bots', 'which', 'automated systems', 'that', 'you', 'your bank balance or order food', 'a restaurant', 'You', 'N LP and translation tools', 'which', 'text', 'languages', 'example', 'you', 'applications', 'that', 'menus', 'real time', 'N LP', 'voice', 'text translations', 'which', 'words', 'text', 'NLP', 'sentiment analysis', 'which', 'you', 'the sentiment', 'comments', 'reviews', 'products', 'music', 'movies', 'These sentiments', 'the movie', 'an audience rating', 'you', 'more about N LP', 'this course', 'Another kind', 'that', 'popularity', 'reinforcement learning', 'other machine', 'reinforcement learning', 'its model', 'mining feedback', 'previous iterations', 'reinforcement learning', 'an agent', 'trial', 'error', 'it', 'an environment', 'Reinforcement learning', 'the reward', 'a desired outcome', 'the path', 'it', 'that path', 'a lot', 'trial', 'error', 'the example', 'Aws Deep Racer', 'the Aws Deep Racer simulator', 'The agent', 'the virtual car', 'The environment', 'a virtual race track', 'The actions', 'inputs', 'the car', 'the goal', 'the race track', 'the track', 'The car', 'the desired driving behavior', 'the goal', 'the track', 'the car', 'this', 'Deep racer teams', 'rewards', 'their model', 'the desired driving behavior', 'reinforcement', 'the thing', 'the learning', 'the agent', 'this case', 'it', 'the Aws deep racer car', 'The environment', 'the place', 'the agent', 'which', 'this example', 'the marked racetrack', 'the agent', 'something', 'the environment', 'that', 'a response', 'a boundary', 'it', 'that', 'an action', 'that response', 'a reward', 'penalty', 'the agent', 'something', 'the model', 'the agent', 'the environment', 'its action', 'more rewards', 'fewer penalties', 'it', 'the desired business outcome', 'Self driving vehicles', 'many machine', 'deep learning algorithms', 'models', 'the problem', 'point A', 'its main tasks', 'the continuous detection', 'the environment and forecasting changes', 'These', 'objects', 'the movement', 'the detected objects', 'The outputs', 'these findings', 'inputs', 'other systems', 'that', 'decisions', 'what', 'they', \"the vehicle's various controls\", 'use cases', 'self driving vehicles', 'that', 'real time responses', 'the environment', 'example', 'a previously hidden pedestrian', 'an obstacle', 'the vehicle brakes', 'no latency', 'room', 'error', 'these actions', 'Not every problem', 'machine learning', 'regular programming', 'your needs', 'you', 'a potential machine learning solution', 'the existence', 'large data sets', 'a large number', 'variables', 'Machine learning', 'the best choice', 'you', 'the business logic', 'procedures', 'an answer', 'a task', 'machine learning systems', 'The supporting infrastructure management support', 'technical expertise', 'place', \"the project's success\", 'the key takeaways', 'this section', 'we', 'some machine learning applications', 'that', 'part', 'everyday life', 'machine learning problems', 'three categories', 'Supervised learning', 'you', 'training data', 'you', 'the answer', 'unsupervised learning', 'you', 'data', 'insights', 'the data reinforcement learning', 'the model', 'experience', 'feedback', 'Most business problems', 'learning problems', 'that', 'it', 'this section', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_5\n",
      "All the text from the video: Hi and welcome back. This is section three and we're going to give you a quick high level overview of machine learning terminology and a typical workflow. We will cover these topics in more detail later in this course. But for now, we'll focus on the larger picture. So to begin, you should always start with the business problem. You or your team believe could benefit from machine learning from there. You want to do some problem formulation in this phase. One task is to articulate your business problem and convert it to an M L problem. After you've formulated the problem, you move on to the data preparation and preprocessing phase. You'll pull data from one or more data sources. These data sources might have differences in data or types that need to be reconciled. So you can form a single cohesive view of your data. You'll need to visualize your data and use statistics to determine if the data is consistent and can be used for machine learning. We'll look at some of the data sources later in the course. This example data has four columns containing data from three different data sources. The sources had slightly different ways of representing data and the results are shown in the table. In ML problems, columns represent features and rows represent instances. There are some issues here with the data. In some of the instances. In some cases, you'll need a subject matter expert or a functional expert to understand the authenticity of the data. For example, the date that's represented as 11 to 1969 could be November 2nd or February 11th in the year 1969 someone who owns or manages the data pool would be able to clarify this ambiguity. Also the word mail can probably be attributed to an import issue where cells shifted position, but there could be an outside chance where it's the actual location, male, a city that's the capital of the Republic of Maldives at times. This error identification isn't as simple and you'll need an sme to review the data. You'll learn about the role of experts later in this course. Remember that one of the largest impacts you can have on the success of a machine learning project is to have consistent and correct data. After your data is in good shape, it's time to train your model. This is where the process gets very iterative and fluid. You'll likely go through many multiple passes of feature engineering training, evaluating and tuning before you find a model that meets your business goals. Feature engineering is the process of selecting or creating the features. Your model will be trained with features are the columns of data you have in your data set. The goal of the model is to correctly estimate the target value for the new data. The M L algorithm will use the features to predict the target. In this example, the target data is the average number of steps taken in a week. Selecting the correct features can involve adding, removing or calculating new features. You might want to make the data formats consistent, the consistent formats could be later used in the model or you can make these changes for cosmetic reasons. Depending on the problem, you want to solve this data, you might not even need to include the name feature in the example data. What about the country feature? If this were a traditional database, you might want to move country to a look up table, then reference it. Most M L algorithms want the data for an instance in a single row M L algorithms need numerical data to process. You could consider turning the country text into the country's iso code. However, the model might interpret the numerical value as having meaning. So the UK S iso code value of 44 would be more significant than the iso code value of the US, which is 01. In this case, splitting the data into multiple columns is fine. This is known as categorical encoding and you'll learn about this later in the course. For other types of data, you could convert the text value into a numerical value. For example, you could use zero or one to represent male or female. These numeric values can be used more easily by the model. What about the remaining features like age, birth month, which is shown as B M in the table or day of week, which is shown as D O W extracting the age birth month and day of week might be appropriate depending on the problem you're trying to solve. Does age impact the target variable? What about which day of the week they were born on? Don't worry if this sounds complicated, you'll learn more about feature engineering later in this course after your data is cleaned and you've identified the features you want to use. It's time to train a model. You won't use all the data to train your model. In fact, you need to hold some of the data. So you have some data to test with. Typically you'll use about 80% of the data to train with and you'll save the rest of the data for testing next. You'll train a model with training data in the diagram. The model uses the X G boost algorithm. The model itself has some parameters. You can set these parameters will alter how the algorithm works and they're known as hyper parameters. The output of the training job will be a trained model with the trained model. You can use some of the test data to see how well the model performs. You'll take an instance, the model hasn't seen and use it to perform a prediction because you already know the target in your test data. You can compare the two values from these comparisons. You can calculate metrics which give you data on how well the model is performing. You'll then make changes to the model's data features or hyper parameters until you find the model that yields the best results. When training your model, there's a real danger of overfitting or under fitting the model, your model is overfitting your training data. When you see the model performs well on the training data but doesn't perform well on the evaluation data. This is because the model is memorizing the data it saw and can't generalize to unseen examples. Your model is under fitting the training data when the model performs poorly on the training data. This is because the model can't capture the relationship between the input examples, which are often called X and the target values, which are often called Y. Understanding model fit is important for understanding the root cause of poor model accuracy. This understanding will guide you to take corrective steps. You can determine whether a predictive model is under fitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data, we'll show you steps you can take to avoid this later in this course. After you've retrained the model and you're satisfied with the results, you deploy your model to deliver the best possible predictions. Later in this course, we'll walk you through these different phases and give you hands on experience with each of them. But knowing the process is also useful when using the managed services will also explore later where certain Amazon M L services will do the bulk of the work for you. Here are some of the key takeaways for this section. First, we looked at how the machine learning pipeline process can guide you through the process of training and evaluating a model. The iterative process can be broken into three broad steps, data processing model training and model evaluation. That's it for this video. We'll see you in the next one.\n",
      "All the key phrases from the video: ['This', 'section', 'we', 'you', 'a quick high level overview', 'machine learning terminology', 'a typical workflow', 'We', 'these topics', 'more detail', 'this course', 'we', 'the larger picture', 'you', 'the business problem', 'You', 'your team', 'You', 'some problem formulation', 'this phase', 'One task', 'your business problem', 'it', 'an M L problem', 'you', 'the problem', 'you', 'the data preparation', 'preprocessing phase', 'You', 'data', 'one or more data sources', 'These data sources', 'differences', 'data', 'types', 'that', 'you', 'a single cohesive view', 'your data', 'You', 'your data', 'statistics', 'the data', 'machine learning', 'We', 'some', 'the data sources', 'the course', 'This example data', 'four columns', 'data', 'three different data sources', 'The sources', 'slightly different ways', 'data', 'the results', 'the table', 'ML problems', 'columns', 'features', 'rows', 'instances', 'some issues', 'the data', 'some', 'the instances', 'some cases', 'you', 'a subject matter expert', 'a functional expert', 'the authenticity', 'the data', 'example', 'the date', 'that', 'November 2nd', 'February 11th', 'the year', '1969 someone', 'who', 'the data pool', 'this ambiguity', 'the word mail', 'an import issue', 'cells', 'position', 'an outside chance', 'it', 'the actual location', 'male', 'a city', 'that', 'the capital', 'the Republic', 'Maldives', 'times', 'This error identification', 'you', 'an sme', 'the data', 'You', 'the role', 'experts', 'this course', 'the largest impacts', 'you', 'the success', 'a machine learning project', 'consistent and correct data', 'your data', 'good shape', 'it', 'time', 'your model', 'This', 'the process', 'You', 'many multiple passes', 'feature engineering training', 'you', 'a model', 'that', 'your business goals', 'Feature engineering', 'the process', 'the features', 'Your model', 'features', 'the columns', 'data', 'you', 'your data', 'The goal', 'the model', 'the target value', 'the new data', 'The M L algorithm', 'the features', 'the target', 'this example', 'the target data', 'the average number', 'steps', 'a week', 'the correct features', 'new features', 'You', 'the data formats', 'the consistent formats', 'the model', 'you', 'these changes', 'cosmetic reasons', 'the problem', 'you', 'this data', 'you', 'the name feature', 'the example data', 'the country feature', 'this', 'a traditional database', 'you', 'country', 'a look up table', 'it', 'Most M L algorithms', 'the data', 'an instance', 'a single row', 'numerical data', 'You', 'the country text', \"the country's iso code\", 'the model', 'the numerical value', 'meaning', 'the UK S iso code value', 'the iso code value', 'the US', 'which', 'this case', 'the data', 'multiple columns', 'This', 'categorical encoding', 'you', 'this', 'the course', 'other types', 'data', 'you', 'the text value', 'a numerical value', 'example', 'you', 'male', 'These numeric values', 'the model', 'the remaining features', 'age', 'birth month', 'which', 'B M', 'the table', 'day', 'week', 'which', 'D O W', 'the age birth month', 'week', 'the problem', 'you', 'age', 'the target', 'What', 'which day', 'the week', 'they', 'this', 'you', 'feature engineering', 'this course', 'your data', 'you', 'the features', 'you', 'It', 'time', 'a model', 'You', 'all the data', 'your model', 'fact', 'you', 'some', 'the data', 'you', 'some data', 'you', 'about 80%', 'the data', 'you', 'the rest', 'the data', 'You', 'a model', 'training data', 'the diagram', 'The model', 'the X G', 'algorithm', 'The model', 'itself', 'some parameters', 'You', 'these parameters', 'the algorithm', 'they', 'hyper parameters', 'The output', 'the training job', 'a trained model', 'the trained model', 'You', 'some', 'the test data', 'the model', 'You', 'an instance', 'the model', 'it', 'a prediction', 'you', 'the target', 'your test data', 'You', 'the two values', 'these comparisons', 'You', 'metrics', 'which', 'you', 'data', 'the model', 'You', 'changes', \"the model's data features\", 'hyper parameters', 'you', 'the model', 'that', 'the best results', 'your model', 'a real danger', 'overfitting', 'the model', 'your model', 'your training data', 'you', 'the model', 'the training data', 'the evaluation data', 'This', 'the model', 'the data', 'it', 'unseen examples', 'Your model', 'the training data', 'the model', 'the training data', 'This', 'the model', 'the relationship', 'the input examples', 'which', 'X', 'the target values', 'which', 'Y. Understanding model fit', 'the root cause', 'poor model accuracy', 'This understanding', 'you', 'corrective steps', 'You', 'a predictive model', 'the training data', 'the prediction error', 'the training data', 'the evaluation data', 'we', 'you', 'steps', 'you', 'this', 'this course', 'you', 'the model', 'you', 'the results', 'you', 'your model', 'the best possible predictions', 'this course', 'we', 'you', 'these different phases', 'you', 'hands', 'experience', 'each', 'them', 'the process', 'the managed services', 'certain Amazon M L services', 'the bulk', 'the work', 'you', 'some', 'the key takeaways', 'this section', 'we', 'the machine learning pipeline process', 'you', 'the process', 'training', 'a model', 'The iterative process', 'three broad steps', 'data processing model training', 'model evaluation', 'That', 'it', 'this video', 'We', 'you']\n",
      "Video: demo1-job_name_1.1_6\n",
      "All the text from the video: Welcome back in this section. We'll look at some of the tools you'll be using throughout the rest of this course. Before we start this list isn't an exhaustive list of all the tools available today. We're really going to cover them at a high level, but it's a good place to get started. First, there's the Jupiter notebook. The Jupiter notebook is an open source web application you can use to create and share documents that contain live code equations, visualizations and narrative text uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning. And much more. Jupiter Lab is a web based interactive development environment for Jupiter notebooks code and data. Jupiter lab is flexible. You can use it to configure and arrange the user interface to support a wide range of workflows in data science, scientific computing and machine learning. Jupiter lab is extensible and modular. You can write plug ins that add new components and integrate with existing ones later in this course, you'll use Amazon Sagemaker which hosts both Jupiter notebooks and Jupiter Lab. Pandas is an open source Python library. It's used for data handling and analysis. Pandas represents data in a table similar to a spreadsheet. This table is known as a pandas data frame mat plot lib is a Python library for creating scientific static animated and interactive visualizations. In Python, you'll use it to generate plots of your data later in this course. Seaborne is another data visualization library for Python that's built on map plot lib. It provides a high level interface for drawing attractive and informative statistical graphs. Numb Pi is one of the fundamental scientific computing packages in Python. It contains functions for n dimensional array objects. It also has useful math functions such as linear algebra four year transform and random number capabilities. Side kit learn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting data, preprocessing model selection and evaluation and many other utilities Psy Pit Learn is built on numpy sci pi and map plot lib. It's a good tool for exploring machine learning. Although you'll only use it to borrow a few functions in this course, you might want to consider exploring it after you complete this course. Moving up from individual libraries and packages. There are also tools that contain production ready frameworks. We already mentioned side kit learn, which is a good library for machine learning. The frameworks supported on AWS such as tensor flow and Caris also include libraries you can use for machine learning. All the frameworks listed here are supported on AWS and can be used from Amazon. Sagemaker. Aws also provides compute instances that are tuned for machine learning in both the cloud and at the edge compute instances can be optimized for learning and inference. Another AWS resource you can use are certain Amazon machine images or Ami S we offer prepackaged AMI S that contain many of the popular frameworks. Finally, there's Amazon's Sagemaker which is an AWS service with many capabilities. First, Sagemaker can deploy machine learning instances, running Jupiter notebooks and Jupiter lab. It manages the deployment of these compute resources. So you only need to connect to the Jupiter environment. Sagemaker also provides tools for labeling data training models and hosting trained models. Aws marketplace also provides a selection of ready to use model packages and algorithms from third party machine learning developers. Aws also provides a set of managed machine learning servicess and you can integrate them into your applications even if you don't have substantial machine learning experience for computer vision. Amazon recognition provides object and facial recognition for both image and video. Also, Amazon textt can extract text from images. Speech services include Amazon Polly which can speak text. Another speech service is Amazon transcribed which converts spoken audio to text for language. Amazon comprehend uses N LP to find insights and relationships in text. Also, Amazon translate can translate text into different languages if you want to work with chat bots, Amazon Lex helps you build interactive conversational applications that use voice or text for forecasting. Amazon forecast uses machine learning to combine time series data with additional variables. So you can build forecasts. And finally, if you'd like to work with recommendations, Amazon personalized can help you create individual personalized recommendations for customers. These managed services have already been trained in many aspects of the problem domain. You only need to provide your specific data to get started. We're going to look at many of these managed services in the second half of this course. After you learn how to do things on your own, the key takeaways for this section include these points. First, Python is the most popular language for performing machine learning tasks. Jupiter notebooks provide you with a web based hosted development environment for machine learning. You'll use Jupiter notebooks frequently in machine learning. There are a large number of open source tools such as Penda that you'll use often as a machine learning practitioner. Finally, depending upon your requirements, you might start with low level frameworks to create your own solution. You might also use tools such as Amazon Sagemaker to help with some of the heavy lifting or you could simply use and adapt one of the managed Amazon ML services for your specific problem domain. That's it for this video. We'll see you in the next one.\n",
      "All the key phrases from the video: ['this section', 'We', 'some', 'the tools', 'you', 'the rest', 'this course', 'we', 'this list', 'an exhaustive list', 'all the tools', 'We', 'them', 'a high level', 'it', 'a good place', 'the Jupiter notebook', 'The Jupiter notebook', 'an open source web application', 'you', 'documents', 'that', 'live code equations', 'visualizations', 'narrative text uses', 'data cleaning', 'transformation', 'numerical simulation', 'statistical modeling', 'data visualization', 'machine learning', 'Jupiter Lab', 'a web based interactive development environment', 'Jupiter', 'code', 'data', 'Jupiter lab', 'You', 'it', 'the user interface', 'a wide range', 'workflows', 'data science', 'scientific computing', 'machine learning', 'Jupiter lab', 'You', 'plug ins', 'that', 'new components', 'existing ones', 'this course', 'you', 'Amazon Sagemaker', 'which', 'both Jupiter notebooks', 'Jupiter Lab', 'Pandas', 'an open source', 'Python library', 'It', 'data handling', 'analysis', 'Pandas', 'data', 'a table', 'a spreadsheet', 'This table', 'a pandas data frame', 'mat plot lib', 'a Python library', 'scientific static animated and interactive visualizations', 'Python', 'you', 'it', 'plots', 'your data', 'this course', 'Seaborne', 'another data visualization library', 'Python', 'that', 'map plot lib', 'It', 'a high level interface', 'attractive and informative statistical graphs', 'Numb Pi', 'the fundamental scientific computing packages', 'Python', 'It', 'functions', 'n dimensional array objects', 'It', 'useful math functions', 'four year transform', 'random number capabilities', 'Side kit learn', 'an open source machine learning library', 'that', 'learning', 'It', 'various tools', 'model fitting data', 'model selection', 'evaluation', 'many other utilities', 'Psy Pit Learn', 'numpy sci pi', 'plot lib', 'It', 'a good tool', 'machine learning', 'you', 'it', 'a few functions', 'this course', 'you', 'it', 'you', 'this course', 'individual libraries', 'packages', 'tools', 'that', 'production ready frameworks', 'We', 'side kit learn', 'which', 'a good library', 'machine learning', 'The frameworks', 'AWS', 'tensor flow', 'Caris', 'libraries', 'you', 'machine learning', 'All the frameworks', 'AWS', 'Amazon', 'Sagemaker', 'Aws', 'compute instances', 'that', 'machine learning', 'both the cloud', 'the edge compute instances', 'learning', 'inference', 'Another AWS resource', 'you', 'certain Amazon machine images', 'Ami S', 'we', 'AMI S', 'that', 'the popular frameworks', \"Amazon's Sagemaker\", 'which', 'an AWS service', 'many capabilities', 'Sagemaker', 'machine learning instances', 'Jupiter notebooks', 'Jupiter lab', 'It', 'the deployment', 'these compute resources', 'you', 'the Jupiter environment', 'Sagemaker', 'tools', 'data training models', 'trained models', 'Aws marketplace', 'a selection', 'model packages', 'algorithms', 'developers', 'Aws', 'a set', 'managed machine', 'servicess', 'you', 'them', 'your applications', 'you', 'substantial machine learning experience', 'computer vision', 'Amazon recognition', 'object', 'facial recognition', 'both image', 'video', 'Amazon textt', 'text', 'images', 'Speech services', 'Amazon Polly', 'which', 'text', 'Another speech service', 'which', 'text', 'language', 'Amazon', 'N LP', 'insights', 'relationships', 'text', 'Amazon', 'text', 'different languages', 'you', 'chat bots', 'Amazon Lex', 'you', 'interactive conversational applications', 'that', 'voice', 'text', 'forecasting', 'Amazon forecast', 'time series data', 'additional variables', 'you', 'forecasts', 'you', 'recommendations', 'Amazon', 'you', 'individual personalized recommendations', 'customers', 'These managed services', 'many aspects', 'the problem domain', 'You', 'your specific data', 'We', 'these managed services', 'the second half', 'this course', 'you', 'things', 'the key takeaways', 'this section', 'these points', 'Python', 'the most popular language', 'machine learning tasks', 'Jupiter notebooks', 'you', 'a web based hosted development environment', 'machine learning', 'You', 'Jupiter', 'machine learning', 'a large number', 'open source tools', 'Penda', 'that', 'you', 'a machine learning practitioner', 'your requirements', 'you', 'low level frameworks', 'your own solution', 'You', 'tools', 'Amazon Sagemaker', 'some', 'the heavy lifting', 'you', 'the managed Amazon ML services', 'your specific problem domain', 'That', 'it', 'this video', 'We', 'you']\n",
      "Video: demo1-job_name_1.1_7\n",
      "All the text from the video: Hi, welcome back. This is section five and we're going to discuss challenges with machine learning. You'll come across many challenges in machine learning. There are a lot of poor quality and inconsistent data available. A significant portion of your job will be getting access to or generating enough good data. That's representative of the problem. You want to solve a key issue to watch out for is under or overfitting the model. It's not all about the data although it mostly is. Do you have data science experience? Is staffing a team of data scientists cost effective. Does management support using machine learning? What does the business landscape look like? Are the problems too complex to formulate into a machine learning problem? Can the resulting model be explained to the business? If it can't be explained, it might not get adopted. What's the cost of building updating and operating a machine learning solution? Finally, how does the technology map? Does the business unit have access to the data that's needed? Can the data be secured to meet any regulatory requirements? What tools and frameworks will be used? How will the solution integrate with other systems? These are important questions to be successful, you'll need to be able to answer and address them. Many machine learning problems can be solved today by using existing models and without substantial machine learning knowledge, we've already talked about the Aws Managed services for machine learning. You can add sophisticated machine learning capabilities to your applications. With only some basic developer skills for calling API S. There are other prebuilt models you can use or adapt. One example is YOLO, which means you only look once YOLO is a popular computer vision model. In addition to these scenarios, you can use the AWS marketplace if you'd prefer to buy models and services from independent software vendors instead of developing your own. Here are the key takeaways for this section. First, you'll face many machine learning challenges. The biggest ones that you can directly influence are related to data, you should consider managed services to solve machine learning problems within the domains they support such as using Amazon recognition for computer vision problems. That's it. For this section. We'll see you in the next video.\n",
      "All the key phrases from the video: ['This', 'section', 'we', 'challenges', 'machine learning', 'You', 'many challenges', 'machine learning', 'a lot', 'poor quality and inconsistent data', 'A significant portion', 'your job', 'access', 'enough good data', 'That', 'representative', 'the problem', 'You', 'a key issue', 'the model', 'It', 'the data', 'it', 'you', 'data science experience', 'a team', 'data scientists', 'management support', 'machine learning', 'What', 'the business landscape', 'the problems', 'a machine learning problem', 'the resulting model', 'the business', 'it', 'it', 'What', 'the cost', 'building', 'a machine learning solution', 'the technology', 'the business unit', 'access', 'the data', 'that', 'the data', 'any regulatory requirements', 'What tools', 'frameworks', 'the solution', 'other systems', 'These', 'important questions', 'you', 'them', 'Many machine learning problems', 'existing models', 'substantial machine learning knowledge', 'we', 'the Aws Managed services', 'machine learning', 'You', 'sophisticated machine learning capabilities', 'your applications', 'only some basic developer skills', 'API S.', 'other prebuilt models', 'you', 'One example', 'YOLO', 'which', 'you', 'YOLO', 'a popular computer vision model', 'addition', 'these scenarios', 'you', 'the AWS marketplace', 'you', 'models', 'services', 'independent software vendors', 'the key takeaways', 'this section', 'you', 'many machine learning challenges', 'The biggest ones', 'that', 'you', 'data', 'you', 'managed services', 'machine learning problems', 'the domains', 'they', 'Amazon recognition', 'computer vision problems', 'That', 'it', 'this section', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_8\n",
      "All the text from the video: It's now time to review the module. Here are the main takeaways for this module. First, we looked at defining machine learning and how it fits into the broader A I landscape. We also looked at the types of problem machine learning can help us solve and how machine learning applies learning algorithms to develop models from large data sets. We then looked at the machine learning pipeline and the different stages for developing a machine learning application. Finally, we introduce some of the tools and services you can use before discussing some of the challenges with machine learning. In summary, in this module, you learned how to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology identify how machine learning can be used to solve a business problem. Describe the machine learning process list the tools available to data. Scientists identify when to use machine learning instead of traditional software development methods. Thanks for watching, we'll see you in the next video.\n",
      "All the key phrases from the video: ['It', 'time', 'the module', 'the main takeaways', 'this module', 'we', 'machine learning', 'it', 'A I landscape', 'We', 'the types', 'problem machine learning', 'us', 'algorithms', 'models', 'large data sets', 'We', 'the machine learning pipeline', 'the different stages', 'a machine learning application', 'we', 'some', 'the tools', 'services', 'you', 'some', 'the challenges', 'machine learning', 'summary', 'this module', 'you', 'machine learning', 'deep learning', 'part', 'artificial intelligence', 'artificial intelligence and machine learning terminology', 'machine learning', 'a business problem', 'the machine learning process', 'the tools', 'data', 'Scientists', 'traditional software development methods', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_9\n",
      "All the text from the video: Welcome back to Aws Academy of Machine learning. This is module three and we're going to work through the entire machine learning pipeline by using Amazon Sage Maker. This module will discuss a typical process for handling a machine learning problem. The machine learning pipeline can be applied to many machine learning problems. The focus is on supervised learning but the process you learn in this module can be adapted to other types of machine learning as well. This is a large module and we'll be covering a lot of material. At the end of this module, you'll be able to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupiter notebook by using Amazon Sagemaker outline the process for evaluating data. Explain why data needs to be pre processed, use open source tools to examine and prep process data. Use Amazon Sagemaker to train and host a machine learning model. Use cross validation to test the performance of a machine learning model, use a hosted model for inference and finally create an Amazon Sagemaker hyper parameter tuning job to optimize a model's effectiveness. We're ready to get started. See you in the next video.\n",
      "All the key phrases from the video: ['Aws Academy', 'Machine learning', 'This', 'module', 'we', 'the entire machine', 'pipeline', 'Amazon Sage Maker', 'This module', 'a typical process', 'a machine learning problem', 'The machine learning pipeline', 'many machine learning problems', 'The focus', 'supervised learning', 'the process', 'you', 'this module', 'other types', 'This', 'a large module', 'we', 'a lot', 'material', 'the end', 'this module', 'you', 'a problem', 'a business request', 'data', 'machine learning', 'a Jupiter notebook', 'Amazon Sagemaker', 'the process', 'data', 'data', 'open source tools', 'prep process data', 'Amazon Sagemaker', 'a machine learning model', 'validation', 'the performance', 'a machine learning model', 'a hosted model', 'inference', 'an Amazon Sagemaker hyper parameter tuning', 'job', \"a model's effectiveness\", 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_10\n",
      "All the text from the video: Hi and welcome back to module three. This is section one and we're going to take a look at some of the data sets we'll use in this module. We'll also look at guidance for how to formulate a business problem before we get started. Here's a reminder of the machine learning pipeline we looked at in the previous module and how that maps to the sections in this module. This section section one will cover how to formulate a problem. It will also cover the data sets we'll use throughout this module. Section two will discuss how to obtain and secure data for your machine learning activities. In section three, we'll show you tools and techniques for gaining an understanding of your data. Then in section four, we'll look at pre processing your data. So it's ready to train a model. Section five will cover selecting and training an appropriate machine learning model. Section six will show you how to deploy a model. So you can make a prediction. Section seven will examine the process of evaluating the performance of a machine learning module. And finally, in section eight, we'll look at tuning the model, the machine learning pipeline is an iterative process. When you work on a real world problem, you might find yourself iterating many times before you arrive at a solution that meets your business' needs. In this first section, we'll examine how to think about turning a business requirement into a machine learning problem. The first step in this phase is to simply define the problem you want to solve. And the goal you want to reach understanding the business goal is key because you'll use it to measure the performance of your solution. It's not unusual to solidify the business problem before you can begin targeting a solution. There are a lot of other questions you could ask to develop a good understanding of the problem with more information about the problem. You can begin framing an approach. First, can the problem even be solved by machine learning or would a traditional approach make more sense? Is this a supervised or unsupervised machine learning problem? Do you have labeled data to train a supervised model? There are many questions you could ask yourself and the business ultimately, you should try to validate the use of machine learning and you should make sure you have access to the right people and data. You should also try to come up with the simplest solution to the problem. Here's an example, you want to identify fraudulent credit card transactions so you can stop the transaction before it processes. That's your problem. Now, what's the business goal or outcome driving this problem statement in this case, say that the intended outcome is a reduction in the number of customers who end their membership to the credit card as a result of a fraudulent transaction. From a business perspective, how do you define success given this problem and the desired outcome? This is the stage where you need to move from qualitative statements to quantitative statements that can be easily measured. Continuing with the example, a metric you could use to define success with. This problem might be a 10% reduction in the number of customers who file claims for fraudulent transactions within a six month period. So now you've defined the business side of your problem. It's time to start thinking about this in terms of your machine learning model itself. What's the actual output you want to see from your model? You want to be specific here. It should be a statement that reflects what an M L model could actually output. An example might be. The model will output whether or not a credit card transaction is fraudulent or not fraudulent. Now that you know what you want your M L model to actually achieve, you can use this information to determine the type of M L you're working with. If you have historical data, where customers filed reports for fraud transactions, you can use this data for your machine learning purpose. This historical data falls under the supervised learning approach where the labels are already defined. Recall from earlier in this course that supervised ML types are categorized into two groups, classification and regression. In the credit card example, the desired output of categorizing a transaction is fraud or not fraud. So you can see that you're dealing with a binary classification problem throughout this module. You'll see several data sets being used. You can access these data sets and many more from the UC Irvine machine learning repository. The first data set contains numerical information about the composition of wine along with the quality of the wine. The question you might want to ask on this data set is based on the composition of the wine. Could we predict the quality and therefore the price. In addition to that question, we'll also use this data set to view statistics deal with outliers and scale numerical data. The second data set is a car evaluation database. This data set is heavily text based. This enables you to explore the encoding categorical data which converts the text values into numbers that can be processed by machine learning. The third data set is a biomedical data set which you'll also use in the labs. The question to answer for this data set is based on the biomechanical features. Can you predict if a patient has an abnormality? This data set will take you through the entire end to end process. You'll end with a trained model that's been tuned and that you can use to make a prediction. In this section, we looked at how business problems need to be converted into an M L problem. We also looked at some of the key questions to ask, which are what is defining success? Can you measure the outcome or impact if your solution is implemented? Most business problems fall into one of two categories. The first category is classification which can be binary or multi class. Ask yourself does the target belong to a class? And the second category is regression for this. Ask yourself, can you predict a numerical value? That's it? For section one, we'll see you in the next video.\n",
      "All the key phrases from the video: ['module', 'This', 'section', 'we', 'a look', 'some', 'the data sets', 'we', 'this module', 'We', 'guidance', 'a business problem', 'we', 'a reminder', 'the machine learning pipeline', 'we', 'the previous module', 'the sections', 'this module', 'This section section', 'one', 'a problem', 'It', 'the data sets', 'we', 'this module', 'Section', 'data', 'your machine learning activities', 'section', 'we', 'you', 'tools', 'techniques', 'an understanding', 'your data', 'section', 'we', 'pre', 'your data', 'it', 'a model', 'Section', 'an appropriate machine learning model', 'Section', 'you', 'a model', 'you', 'a prediction', 'Section', 'the process', 'the performance', 'a machine learning module', 'section', 'we', 'the model', 'the machine learning pipeline', 'an iterative process', 'you', 'a real world problem', 'you', 'yourself', 'many times', 'you', 'a solution', 'that', \"your business' needs\", 'this first section', 'we', 'a business requirement', 'a machine learning problem', 'The first step', 'this phase', 'the problem', 'you', 'the goal', 'you', 'the business goal', 'you', 'it', 'the performance', 'your solution', 'It', 'the business problem', 'you', 'a solution', 'a lot', 'other questions', 'you', 'a good understanding', 'the problem', 'more information', 'the problem', 'You', 'an approach', 'the problem', 'machine learning', 'a traditional approach', 'more sense', 'this', 'a supervised or unsupervised machine learning problem', 'you', 'data', 'a supervised model', 'many questions', 'you', 'yourself', 'the business', 'you', 'the use', 'machine learning', 'you', 'you', 'access', 'the right people', 'data', 'You', 'the simplest solution', 'the problem', 'an example', 'you', 'fraudulent credit card transactions', 'you', 'the transaction', 'it', 'That', 'your problem', 'what', 'the business goal', 'this problem statement', 'this case', 'the intended outcome', 'a reduction', 'the number', 'customers', 'who', 'their membership', 'the credit card', 'a result', 'a fraudulent transaction', 'a business perspective', 'you', 'success', 'this problem', 'the desired outcome', 'This', 'the stage', 'you', 'qualitative statements', 'quantitative statements', 'that', 'the example', 'you', 'success', 'This problem', 'a 10% reduction', 'the number', 'customers', 'who', 'claims', 'fraudulent transactions', 'a six month period', 'you', 'the business side', 'your problem', 'It', 'time', 'this', 'terms', 'your machine learning model', 'itself', 'What', 'the actual output', 'you', 'your model', 'You', 'It', 'a statement', 'that', 'what', 'an M L model', 'An example', 'The model', 'not a credit card transaction', 'you', 'what', 'you', 'your M L model', 'you', 'this information', 'the type', 'M L', 'you', 'you', 'historical data', 'customers', 'reports', 'fraud transactions', 'you', 'this data', 'your machine learning purpose', 'This historical data', 'the supervised learning approach', 'the labels', 'Recall', 'this course', 'that', 'ML types', 'two groups', 'classification', 'regression', 'the credit card example', 'the desired output', 'a transaction', 'fraud', 'not fraud', 'you', 'you', 'a binary classification problem', 'this module', 'You', 'several data sets', 'You', 'these data sets', 'the UC Irvine machine learning repository', 'The first data', 'numerical information', 'the composition', 'wine', 'the quality', 'the wine', 'The question', 'you', 'this data set', 'the composition', 'the wine', 'we', 'the quality', 'therefore the price', 'addition', 'that question', 'we', 'this data', 'statistics', 'outliers', 'scale numerical data', 'The second data', 'a car evaluation database', 'This data', 'heavily text', 'This', 'you', 'the encoding categorical data', 'which', 'the text values', 'numbers', 'that', 'machine learning', 'The third data', 'a biomedical data', 'which', 'you', 'the labs', 'The question', 'this data set', 'the biomechanical features', 'you', 'a patient', 'an abnormality', 'This data', 'you', 'the entire end', 'end process', 'You', 'a trained model', 'that', 'you', 'a prediction', 'this section', 'we', 'business problems', 'an M L problem', 'We', 'some', 'the key questions', 'which', 'what', 'success', 'you', 'the outcome', 'impact', 'your solution', 'Most business problems', 'two categories', 'The first category', 'classification', 'which', 'binary or multi class', 'yourself', 'the target', 'a class', 'the second category', 'regression', 'this', 'yourself', 'you', 'a numerical value', 'That', 'it', 'section', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_11\n",
      "All the text from the video: Hi, welcome back. We're now going to look at a few ways you can collect and secure data in this section. We'll explore some of the techniques and challenges associated with collecting and securing the data that's needed for machine learning. Consider again the original example about predicting credit card fraud, you've further formulated the problem. But what data do you need to actually train your model so you can get the desired output and subsequently achieve your intended business outcome. Do you have access to the data? If so, how much data do you have? And where is it? What solution can you use to bring all this data into one centralized repository? The answers to these questions are essential at this stage. The good news for a budding data scientist is that there are many places where you can obtain data, private data from you or your existing customer already exists, including everything from log files to customer invoice databases, private data can be useful depending on the problem you're trying to solve. In many cases, private data is found in many different systems. We'll look at how to bring these sources together shortly. Sometimes you want to use data that was collected and made available by a commercial organization companies such as Reuters, Change Healthcare, Dun and Bradstreet and Foursquare Maintain Databases. You can subscribe to. They include curated news stories, anonymized health care transactions, global business records and location data. If you supplement your own data with commercial data, you can get useful insights you wouldn't have gotten. Otherwise, there are also many open source data sets ranging from wine quality to movie reviews. These data sets are made available for use in research or for teaching purposes, Aws Cale and the U C I machine learning repositories are good places to find open source data sets. Government and health organizations are other sources of data that could be useful supervised machine learning problems need a lot of data. These are also called observations and you already need to know the target answer or prediction for that data. This kind of data where you already know the target answer or prediction is called labeled data. Each observation in your data is made up of two elements, the target and the features, the target is the answer you want to predict. So in the credit card transaction example, the target of any given observation is either fraud or not fraud. A feature is an attribute of the example that you can use to identify patterns for predicting the target answer. A feature in the credit card example could be the date of the transaction, the vendor or the amount in dollars of the transaction. You might wonder if the source of the target is fraud or not fraud. Typically, this information is discovered only after the transaction is complete and the actual card owner notices a fraudulent transaction on their statement. This information would be recorded with the transaction for exactly the purpose of using it to train a future model. So given what you know about the elements of an ML data set, we'll return to one of the original questions. What data do you need to actually train your model to reach the desired output and subsequently your intended business outcome. This is an example of a stage in the M L pipeline. When it's crucial to get domain expertise to help you answer this question with domain knowledge, you can start determining the features and target data. Your model will need to make accurate predictions. Your data should be representative of the data you'll have when you're using the model to make a prediction. For example, if you want to predict credit card fraud, you need to collect data for positive or fraudulent transactions. You also need to collect data for negative or non fraudulent transactions. You need both types of data. So the machine learning algorithm can find patterns that will distinguish between the two types. Suppose your average amount of fraudulent transactions is actually 3%. But your training data set only includes a very small fraction of fraudulent observations say 0.4%. In this case, it will be difficult for your model to truly learn patterns related to fraudulent transactions that it might encounter in production. There are many different services in Aws where you could find or store your data. Here are some key services you might use. Amazon's simple storage service is also known as Amazon S3. It provides object level storage with S3. You can store as much data as you want in the form of objects which you can think of as files. They could be CS V files or files of other formats you need S3 can be accessed through the web based AWS management console. You can also access S3 programmatically through the API and S D K S or with third party solutions, which also use the API and S D K S. If your training data is already in S3 and you're planning to run training jobs several times with different algorithms and parameters, you could use Amazon FSX for Luster. It's a file system service that speeds up your training jobs by serving your S3 data to Amazon Sagemaker at high speeds. The first time you run a training job FSX four Luster automatically copies data from S3 and makes it available to sagemaker. You can use the same Amazon FSX file system for subsequent iterations of training jobs which prevents repeated downloads of common S3 objects. Alternatively, your training data might already be in Amazon Elastic file system or Amazon Efs. If so we recommend using EFS as your data source for training data, it can launch your training jobs directly from the service without needing data movement, which results in faster training start times. This is often the case in environments where data scientists have home directories in Amazon EFS, they can quickly iterate on their models by bringing in new data sharing data with colleagues and experimenting with different fields or labels in their data set. For example, a data scientist can use a Jupiter notebook to do an initial cleansing on a training set and launch a training job from Amazon sagemaker. They could then use their Jupiter notebook to drop a column and relaunch the training job and finally compare the resulting models to see which one works better. There are many other AWS services and resources where you might find data. For example, you could use Amazon relational database service or Amazon rds, a manage relational database service. You could also use Amazon Redshift which is a managed data warehouse service. Another option is Amazon time stream, a managed time series database designed specifically to handle large amounts of data from the internet of things or I O T. You could even spin up your own instances on Amazon Elastic compute cloud, which is also known as Amazon EC2 and post your own database on these instances. When you have data sources, you'll need to extract useful data from these sources. When assembling your data for machine learning. We'll look at this next, that's it. For part one of this section, we'll see you again for part two where we'll review how to extract, transform and load data.\n",
      "All the key phrases from the video: ['We', 'a few ways', 'you', 'data', 'this section', 'We', 'some', 'the techniques', 'challenges', 'the data', 'that', 'machine learning', 'the original example', 'credit card fraud', 'you', 'the problem', 'what data', 'you', 'your model', 'you', 'the desired output', 'your intended business outcome', 'you', 'access', 'the data', 'how much data', 'you', 'it', 'What solution', 'you', 'all this data', 'one centralized repository', 'The answers', 'these questions', 'this stage', 'The good news', 'a budding data scientist', 'many places', 'you', 'data', 'private data', 'you', 'your existing customer', 'everything', 'log files', 'private data', 'the problem', 'you', 'many cases', 'private data', 'many different systems', 'We', 'these sources', 'you', 'data', 'that', 'a commercial organization companies', 'Reuters', 'Change Healthcare', 'Dun', 'Bradstreet', 'Foursquare Maintain Databases', 'You', 'They', 'curated news stories', 'health care transactions', 'global business records', 'location data', 'you', 'your own data', 'commercial data', 'you', 'useful insights', 'you', 'many open source data sets', 'wine quality', 'movie reviews', 'These data sets', 'use', 'research', 'purposes', 'I', 'good places', 'open source data sets', 'Government and health organizations', 'other sources', 'data', 'that', 'useful supervised machine learning problems', 'a lot', 'data', 'These', 'observations', 'you', 'the target answer', 'prediction', 'that data', 'This kind', 'data', 'you', 'the target answer', 'prediction', 'labeled data', 'Each observation', 'your data', 'two elements', 'the target', 'the features', 'the target', 'the answer', 'you', 'the credit card transaction example', 'the target', 'any given observation', 'either fraud', 'not fraud', 'A feature', 'an attribute', 'the example', 'you', 'patterns', 'the target answer', 'A feature', 'the credit card example', 'the date', 'the transaction', 'the vendor', 'the amount', 'dollars', 'the transaction', 'You', 'the source', 'the target', 'fraud', 'not fraud', 'this information', 'the transaction', 'the actual card owner', 'a fraudulent transaction', 'their statement', 'This information', 'the transaction', 'exactly the purpose', 'it', 'a future model', 'what', 'you', 'the elements', 'an ML data', 'we', 'the original questions', 'What data', 'you', 'your model', 'the desired output', 'subsequently your intended business outcome', 'This', 'an example', 'a stage', 'the M L pipeline', 'it', 'domain expertise', 'you', 'this question', 'domain knowledge', 'you', 'the features', 'target data', 'Your model', 'accurate predictions', 'Your data', 'the data', 'you', 'you', 'the model', 'a prediction', 'example', 'you', 'credit card fraud', 'you', 'data', 'positive or fraudulent transactions', 'You', 'data', 'negative or non fraudulent transactions', 'You', 'both types', 'data', 'the machine learning algorithm', 'patterns', 'that', 'the two types', 'your average amount', 'fraudulent transactions', '3%', 'your training data', 'a very small fraction', 'fraudulent observations', 'this case', 'it', 'your model', 'patterns', 'fraudulent transactions', 'it', 'production', 'many different services', 'Aws', 'you', 'your data', 'some key services', 'you', \"Amazon's simple storage service\", 'Amazon S3', 'It', 'object level storage', 'S3', 'You', 'as much data', 'you', 'the form', 'objects', 'which', 'you', 'files', 'They', 'CS V files', 'files', 'other formats', 'you', 'S3', 'the web based AWS management console', 'You', 'S3', 'the API', 'S D K S', 'third party solutions', 'which', 'the API', 'S D', 'K S.', 'your training data', 'S3', 'you', 'training jobs', 'different algorithms', 'parameters', 'you', 'Amazon FSX', 'Luster', 'It', 'a file system service', 'that', 'your training jobs', 'your S3 data', 'Amazon Sagemaker', 'high speeds', 'The first time', 'you', 'a training job', 'FSX four Luster', 'data', 'S3', 'it', 'You', 'the same Amazon FSX file system', 'subsequent iterations', 'training jobs', 'which', 'repeated downloads', 'common S3 objects', 'your training data', 'Amazon Elastic file system', 'Amazon Efs', 'we', 'EFS', 'your data source', 'training data', 'it', 'your training jobs', 'the service', 'data movement', 'which', 'faster training start times', 'This', 'the case', 'environments', 'data scientists', 'home directories', 'Amazon EFS', 'they', 'their models', 'new data sharing data', 'colleagues', 'different fields', 'labels', 'their data set', 'example', 'a data scientist', 'a Jupiter notebook', 'an initial cleansing', 'a training job', 'Amazon sagemaker', 'They', 'their Jupiter notebook', 'a column', 'the training job', 'the resulting models', 'which one', 'many other AWS services', 'resources', 'you', 'data', 'example', 'you', 'Amazon relational database service', 'Amazon rds', 'a manage relational database service', 'You', 'Amazon Redshift', 'which', 'a managed data warehouse service', 'Another option', 'Amazon time stream', 'a managed time series database', 'large amounts', 'data', 'the internet', 'things', 'I', 'You', 'your own instances', 'Amazon Elastic compute cloud', 'which', 'Amazon EC2', 'your own database', 'these instances', 'you', 'data sources', 'you', 'useful data', 'these sources', 'your data', 'machine learning', 'We', 'this', 'that', 'it', 'part', 'this section', 'we', 'you', 'part', 'we']\n",
      "Video: demo1-job_name_1.1_12\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring data collection by reviewing how to extract, transform and load data data is typically spread across many different systems and data providers. This presents a challenge. You'll need to bring all these data sources together into something that can be consumed by a machine learning model. You can do this through extract, transform and load, which is also known as E T L. The steps in E T L are defined this way in the extract step, you pull the data from the sources to a single location. During extraction, you might need to modify the data, combine matching records or do other tasks that transform the data. Finally. In the load step, the data is loaded into a repository such as Amazon S3. A typical L framework has several components. As an example, consider the diagram. First, the crawler, a program connects to a data store which can be a source or a target. It progresses through a ranked list of classifiers to determine the schema for your data. Then it creates metadata tables in the aws glue data catalog. A job defines the business logic that's needed to perform E T L work to run the job you'll need to use a schedule or event. As a final note, the services we just discussed exist in the transform partition of the E T L process. Aws Glue is a fully managed E T L service that makes it simple and cost effective to categorize your data, clean it, enrich it and move it reliably between various data stores. Aws Glue consists of a central metadata repository known as the Aws Glue data catalog. This is an E T L engine that automatically generates Python or Scalar code. It also provides a flexible scheduler that handles dependency resolution, job monitoring and retries. Aws GLUE is serverless so you don't need to set up or manage any infrastructure. You can use the Aws GLUE console to discover data, transform it and make it available for search in queries. The console calls the underlying services to orchestrate the work needed to transform your data. You can also use the Aws glue API operations to interface with the Aws glue services. This way you can edit debug and test your Python or Scalea Apache spark E T L code using a familiar development environment. Aws glue is well suited to machine learning because it can receive label data that can be used for training. Here's an example say that you provide Aws glue with training data that teaches the model what duplicate records in the data source look like. Then Aws glue can identify the duplicates and present them for further analysis by a data engineer. Aws Glue enables the orchestration of complex E T L jobs. In the example, Aws glue crawls the data sources and presents the information to clients as a data catalog. Aws glue can run your E T L jobs based on an event such as getting a new data set. For example, you can use an aws LAMBDA function to trigger your E T L jobs to run as soon as new data becomes available in Amazon S3, you can also register this new data set in the AWS glue data catalog as part of your E T L jobs. Although manage tools are available in AWS to manipulate data. A data scientist will also write scripts in their jupiter notebook to handle the data. A very simple extract and load script is shown here. The imports and variables section imports the libraries that are used. Note that Bodo three is the library for AWS. Variables are also set here for the zip files, web location and a local folder for extraction. The download and extract section makes a web request saving the bytes from the URL as a stream. This stream is passed to the zip file function which is then used to extract the data with the extracted files in a folder. The upload to S3 section enumerates the folders files and uploads each file to Amazon S3. If you discover that this script is used, often, it should be migrated to a standalone function that can be imported by other Python applications. That's it. For part two of this section, we'll see you again for part three where we'll review how to secure your data.\n",
      "All the key phrases from the video: ['We', 'data collection', 'transform and load data data', 'many different systems and data providers', 'This', 'a challenge', 'You', 'all these data sources', 'something', 'that', 'a machine learning model', 'You', 'this', 'extract', 'transform', 'load', 'which', 'The steps', 'E T L', 'the extract step', 'you', 'the data', 'the sources', 'a single location', 'extraction', 'you', 'the data', 'records', 'other tasks', 'that', 'the data', 'the load step', 'the data', 'a repository', 'Amazon S3', 'A typical L framework', 'several components', 'an example', 'the diagram', 'the crawler', 'a program', 'a data store', 'which', 'a source', 'a target', 'It', 'a ranked list', 'classifiers', 'the schema', 'your data', 'it', 'metadata tables', 'the aws', 'glue data catalog', 'A job', 'the business logic', 'that', 'E T L work', 'the job', 'you', 'a schedule', 'event', 'a final note', 'we', 'the transform partition', 'the E T L process', 'Aws Glue', 'a fully managed E T L service', 'that', 'it', 'your data', 'it', 'it', 'it', 'various data stores', 'Aws Glue', 'a central metadata repository', 'the Aws Glue data catalog', 'This', 'an E T L engine', 'that', 'Python', 'Scalar code', 'It', 'a flexible scheduler', 'that', 'dependency resolution', 'job monitoring', 'retries', 'Aws GLUE', 'you', 'any infrastructure', 'You', 'the Aws GLUE console', 'data', 'it', 'it', 'search', 'queries', 'The console', 'the underlying services', 'the work', 'your data', 'You', 'the Aws glue API operations', 'the Aws glue services', 'you', 'your Python', 'Scalea Apache spark E T L code', 'a familiar development environment', 'Aws glue', 'machine learning', 'it', 'label data', 'that', 'training', 'an example', 'you', 'Aws glue', 'training data', 'that', 'the model', 'what', 'records', 'the data source', 'Aws glue', 'the duplicates', 'them', 'further analysis', 'a data engineer', 'Aws Glue', 'the orchestration', 'complex E T L jobs', 'the example', 'Aws glue', 'the data sources', 'the information', 'clients', 'a data catalog', 'Aws glue', 'your E T L jobs', 'an event', 'a new data set', 'example', 'you', 'an aws LAMBDA function', 'your E T L jobs', 'new data', 'Amazon S3', 'you', 'this new data', 'the AWS glue data catalog', 'part', 'your E T L jobs', 'manage tools', 'AWS', 'data', 'A data scientist', 'scripts', 'their jupiter notebook', 'the data', 'A very simple extract', 'load script', 'The imports', 'variables', 'section', 'the libraries', 'that', 'Bodo', 'the library', 'AWS', 'Variables', 'the zip files', 'web location', 'a local folder', 'extraction', 'The download', 'section', 'a web request', 'the bytes', 'the URL', 'a stream', 'This stream', 'the zip file function', 'which', 'the data', 'the extracted files', 'a folder', 'The upload', 'S3 section', 'the folders files', 'each file', 'Amazon S3', 'you', 'this script', 'it', 'a standalone function', 'that', 'other Python applications', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'your data']\n",
      "Video: demo1-job_name_1.1_13\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring data collection by reviewing how to secure your data. It's important to consider the security of your data. Though the data sets used in this course are all public real data about customer transactions or health records need to be kept secure. You can use Aws, identity and access management, which is also known as I AM. It's a service that controls access to resources, make sure you're securing your data within AWS correctly so you can avoid data breaches. The diagram shows a simple I AM policy that allows only read access to a specific S3 bucket for the listed role. In addition to controlling access to data, you need to make sure your data is secure. It's a good practice and it might also be legally required for certain data types such as financial data or health care records. Aws provides encryption features for storage services. Typically for data that's at rest or in transit, you can often meet these encryption requirements by enabling encryption on the object or service you want to protect for data in transit. You must use secure transports like secure sockets layer transport layer security or SSL TLS. Another aspect to consider is compliance audits. When dealing with data from regulated industries, you'll often need to audit access to the data. Aws Cloud Trail is a service that enables governance compliance, operational auditing and risk auditing of your Aws account with cloud trail. You can log continuously monitor and retain account activity related to actions across your entire Aws infrastructure. Cloud trail provides an event history of your AWS account activity including actions taken through the Aws management console, Aws S D K S command line tools and other Aws services. This event in history simplifies security analysis, resource change, tracking and troubleshooting. You can also use cloud trail to detect unusual activity in your Aws accounts. All these features can help you simplify operational analysis and troubleshooting. Here are the key takeaways for this section. We looked at the first step in solving machine learning problems, obtaining the data required to train your machine learning model. We also reviewed how E T L can be used to obtain data from multiple sources. Services like AWS glue can make it easy to obtain data from multiple data stores. Finally make sure you understand your security requirements. These are based on both business need and any regulatory requirements. Also make sure your data is secure. Only authorized users should be able to access your data and it should be encrypted where possible. That's it. For section two, we'll see you in the next video\n",
      "All the key phrases from the video: ['We', 'data collection', 'your data', 'It', 'the security', 'your data', 'the data sets', 'this course', 'public real data', 'customer transactions', 'health records', 'You', 'Aws', 'identity', 'access', 'management', 'which', 'I', 'It', 'a service', 'that', 'access', 'resources', 'you', 'your data', 'AWS', 'you', 'data breaches', 'The diagram', 'I', 'that', 'access', 'a specific S3 bucket', 'the listed role', 'addition', 'access', 'data', 'you', 'your data', 'It', 'a good practice', 'it', 'certain data types', 'financial data', 'health care records', 'Aws', 'encryption features', 'storage services', 'data', 'that', 'rest', 'transit', 'you', 'these encryption requirements', 'enabling encryption', 'the object', 'service', 'you', 'data', 'transit', 'You', 'secure transports', 'secure sockets layer transport layer security', 'SSL TLS', 'Another aspect', 'compliance audits', 'data', 'regulated industries', 'you', 'access', 'the data', 'Cloud Trail', 'a service', 'that', 'governance compliance', 'operational auditing', 'risk auditing', 'your Aws', 'cloud trail', 'You', 'account activity', 'actions', 'your entire Aws infrastructure', 'Cloud trail', 'an event history', 'your AWS', 'account activity', 'actions', 'the Aws management console', 'Aws S D K S command line tools', 'other Aws services', 'This event', 'history simplifies security analysis', 'resource change', 'tracking', 'troubleshooting', 'You', 'cloud trail', 'unusual activity', 'your Aws accounts', 'All these features', 'you', 'operational analysis', 'troubleshooting', 'the key takeaways', 'this section', 'We', 'the first step', 'machine learning problems', 'the data', 'your machine learning model', 'We', 'E T L', 'data', 'multiple sources', 'Services', 'AWS glue', 'it', 'data', 'multiple data stores', 'you', 'your security requirements', 'These', 'both business need', 'any regulatory requirements', 'your data', 'Only authorized users', 'your data', 'it', 'That', 'it', 'section', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_14\n",
      "All the text from the video: Hi and welcome back. This is section three and we're going to cover how to evaluate your data in this section. We'll look at different data formats and types. We'll also look at how you can visualize and analyze the data before feature engineering before you can start running statistics on your data to better understand what you're working with. You need to ensure it's in the right format for analysis for Amazon's Sagemaker algorithms support training with data in CS V format. Many of the tools you'll use to explore visualize and analyze the data can also read it in CS V format. Generally speaking, you'll need to have at least some domain knowledge for the problem you're trying to solve with machine learning. For example, if you're developing a model to predict if a set of symptoms indicates a disease, you'd need to know the relationship between the symptoms and the disease data typically needs to be in numeric form. So machine learning algorithms can use the data to make predictions. We'll look at ways you can convert text data in the next section. For now, we'll just explore the data and try to gain some insights into the overall data set. One popular open source Python library is pandas. It can take data in various formats reformat it and load it into a tabular representation of your data, presenting it in rows and columns. Some of the formats that pandas can reformat and load include CS V, Excel, Pickle and javascript object notation or json pandas also has data analysis and manipulation features and we will use them throughout this module. Loading Data can be as simple as the example which pulls in the CS V file from the specified L when you load data into pandas, it's stored as a pandas data frame. In the pandas documentation. A data frame is described as a general two D labeled size mutable tabular structure with potentially heterogeneously typed column. A more helpful way to think of a data frame is to think of it as a spreadsheet or a SQL table like a table or spreadsheet. A data frame will have rows which are also known as instances and it will have columns which are also known as attributes. The shape property of a data frame describes the number of rows and columns. It has each column in a data frame is a series. A series is a one dimensional labeled array. A series can store data of any type to learn more about data structures. In pandas. See the pandas documentation along with data, you can load a data frame with row labels and column labels. The row labels are known as an index and the column labels are known as columns. If you loaded your data from a CS V file with a header row, the columns will be created from the first line of the file. You can change that behavior. However, if you don't have column names in the source file, you can pass them as a parameter. When performing data analysis. It's important to make sure you're using the correct data types. In many cases, pandas will correctly infer the correct data types when it loads data. And you can move on. If you have domain knowledge or access to a domain expert, they can often identify data type issues. You can use either D types or the info function to obtain information on the column types as shown in the example. If you don't have the correct data types, you need to figure out why. This is the case. Often a numeric column could have been missing data or it could be a single text value. For example, in the car data set, the number of doors can be 234 or five more. After you've analyzed the data, you can convert a column to the correct data type using pandas. That's it. For part one of this section, we'll see you again for part two where we will review how to describe your data.\n",
      "All the key phrases from the video: ['This', 'section', 'we', 'your data', 'this section', 'We', 'different data formats', 'types', 'We', 'you', 'the data', 'feature engineering', 'you', 'statistics', 'your data', 'what', 'you', 'You', 'it', 'the right format', 'analysis', \"Amazon's Sagemaker\", 'algorithms support training', 'data', 'CS V format', 'the tools', 'you', 'visualize', 'the data', 'it', 'CS V format', 'you', 'at least some domain knowledge', 'the problem', 'you', 'machine learning', 'example', 'you', 'a model', 'a set', 'symptoms', 'a disease', 'you', 'the relationship', 'the symptoms', 'the disease data', 'numeric form', 'machine learning algorithms', 'the data', 'predictions', 'We', 'ways', 'you', 'text data', 'the next section', 'we', 'the data', 'some insights', 'One popular open source', 'Python library', 'pandas', 'It', 'data', 'various formats', 'it', 'it', 'a tabular representation', 'your data', 'it', 'rows', 'columns', 'Some', 'the formats', 'that', 'CS V', 'Excel', 'Pickle', 'javascript', 'object notation', 'json pandas', 'data analysis', 'manipulation features', 'we', 'them', 'this module', 'Loading Data', 'the example', 'which', 'the CS V file', 'the specified L', 'you', 'data', 'pandas', 'it', 'a pandas data frame', 'the pandas documentation', 'A data frame', 'a general two D', 'size', 'mutable tabular structure', 'potentially heterogeneously typed column', 'A more helpful way', 'a data frame', 'it', 'a spreadsheet', 'a SQL table', 'a table', 'spreadsheet', 'A data frame', 'rows', 'which', 'instances', 'it', 'columns', 'which', 'attributes', 'The shape property', 'a data frame', 'the number', 'rows', 'columns', 'It', 'each column', 'a data frame', 'a series', 'A series', 'a one dimensional labeled array', 'A series', 'data', 'any type', 'data structures', 'pandas', 'the pandas documentation', 'data', 'you', 'a data frame', 'row labels', 'column labels', 'The row labels', 'an index', 'the column labels', 'columns', 'you', 'your data', 'a CS V file', 'a header row', 'the columns', 'the first line', 'the file', 'You', 'that behavior', 'you', 'column names', 'the source file', 'you', 'them', 'a parameter', 'data analysis', 'It', 'you', 'the correct data types', 'many cases', 'pandas', 'the correct data types', 'it', 'data', 'you', 'you', 'domain knowledge', 'access', 'a domain expert', 'they', 'data type issues', 'You', 'either D types', 'the info function', 'information', 'the column types', 'the example', 'you', 'the correct data types', 'you', 'This', 'the case', 'a numeric column', 'data', 'it', 'a single text value', 'example', 'the car data', 'the number', 'doors', 'you', 'the data', 'you', 'a column', 'the correct data type', 'pandas', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'your data']\n",
      "Video: demo1-job_name_1.1_15\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring how to describe your data. Now that your data is in a readable format, you can perform descriptive statistics on the data to better understand it. Descriptive statistics help you gain valuable insights into your data so that you can effectively preproce the data and prepare it for your M L model. We'll look at how you can do that and discuss why it's so important. First, descriptive statistics can be organized into a few different categories. Overall statistics include the number of rows and the number of columns in your data set. This information which relates to the dimensions of your data is very important. For example, it can indicate that you have too many features which can lead to high dimensionality and poor model performance attribute statistics are another type of descriptive statistic. Specifically for numeric attributes. They're used to get a better sense of the shape of your attributes. This includes properties like the mean standard deviation variants and minimum and maximum values. If you need to look at relationships between more than one variable, you can consider Multivariate statistics, they mostly relate to the correlations and relationships between your attributes for cases when you have multiple variables or features, you might want to look at the correlations between them. It's important to identify correlations between attributes because a high correlation between two attributes can sometimes lead to poor model performance. When features are closely correlated and they're all used in the same model to predict the response variable. There could be problems. For example, the model loss might not converge to a minimum state. So be aware of highly correlated features in your data set mean and median are two different measures. Describing the extent that your data is clustered around some value or position mean can be a useful method for understanding your data when the data is symmetrical. However, if your data is skewed or contains outliers, then median tends to provide the better metric for understanding your data as it relates to central tendency. For instance, if you have outliers with large values, the mean can be skewed one way and it wouldn't serve as an accurate representation of where your values are truly centered median isn't affected by outliers. In the same way, we'll talk more about outliers soon statistics are available and they can be viewed on numerical data by using methods such as describe there are also other methods to calculate the mean median and others. You can also view statistics on single or multiple columns. You can even group data by specific values for categorical attributes. You can look at the frequency of attribute values in your data set. That information will give you some idea about what is inside that categorical variable. The diagram here shows the car data set which is made up of several categorical values. Buying mat lug boot safety and class safety can be either low, medium or high from the described function. You can see that there are three unique values with low being the most frequent looking at the class column. It appears that the top value of the four is UN AC C which stands for unaccounted. This accounts for 1210 of the 1728 values or 70%. This might suggest an imbalance for a target variable. That's also of a categorical type. You can look at the class distribution to see whether there's a class imbalance in your data set, imbalanced data can mark a disproportionate ratio for your classes. For instance, your data set is made up of credit card transactions, but only 1/10 of a percent is labeled as fraud. In this case, your algorithm might not learn well enough to predict examples of credit card fraud visualization could help you gain insights into your data that you might not be aware of. Otherwise, a histogram is often a good visualization technique for seeing the overall behavior of a particular feature with a histogram. You can answer questions like is the feature data normally distributed. How many peaks are there in the data. Is there any skewness for that particular feature? When using histograms for your data visualization values are binned, the taller peaks of the histogram indicate the most common values for numerical features. You can use density plots and box plots. In addition to histograms to get an idea of what's inside that particular feature like a histogram. These visualizations will help you answer questions like what's the range of the data, the peak of the data? Are there any outliers? Are there any special features answering these questions? Helps you understand your data better? And can also help you decide if you need to do more specialized data. Preprocessing. A box plot is a method for graphically depicting groups of numerical data through their core tiles. When you have more than two numerical variables in a feature data set, you might want to look at their relationship. A scatter plot is a good way to identify any special relationships among those variables. In this case, the left diagram has sulfates and alcohol. There are two numerical variables. Suppose you want to show the relationship between these variables. You can use a scatter plot to help you visualize that there are plots scattered around and the correlation among them might not be that high because the data is scattered. However, you might find some relatively positive relationships between the two variables scatter plot matrices help you look at the relationship between multiple different features in Pandi, you can easily create scatter plot matrices based on the columns. You want to look at this example, has three columns and it will give the pair wise scatter plot for any two columns with a scatter plot. You might want to identify special regions that a particular subset of data could fit into. In the example, is there a relationship between alcohol sulfates and quality? You could plot those values against good and poor quality wines. Like the example, plotting gives you an idea of how useful particular variables can be. If you're using them for a classification problem. That's it. For part two of this section, we'll see you again for part three where we'll review correlations and the takeaways for this section.\n",
      "All the key phrases from the video: ['We', 'your data', 'your data', 'a readable format', 'you', 'descriptive statistics', 'the data', 'it', 'Descriptive statistics', 'you', 'valuable insights', 'your data', 'you', 'the data', 'it', 'your M L model', 'We', 'you', 'that', 'it', 'descriptive statistics', 'a few different categories', 'Overall statistics', 'the number', 'rows', 'the number', 'columns', 'your data set', 'This information', 'which', 'the dimensions', 'your data', 'example', 'it', 'you', 'too many features', 'which', 'high dimensionality', 'poor model performance attribute statistics', 'another type', 'descriptive statistic', 'numeric attributes', 'They', 'a better sense', 'the shape', 'your attributes', 'This', 'properties', 'the mean standard deviation variants', 'minimum and maximum values', 'you', 'relationships', 'more than one variable', 'you', 'Multivariate statistics', 'they', 'the correlations', 'relationships', 'your attributes', 'cases', 'you', 'multiple variables', 'features', 'you', 'the correlations', 'them', 'It', 'correlations', 'attributes', 'a high correlation', 'two attributes', 'poor model performance', 'features', 'they', 'the same model', 'the response', 'problems', 'example', 'the model loss', 'a minimum state', 'highly correlated features', 'your data', 'two different measures', 'the extent', 'your data', 'some value', 'position', 'mean', 'a useful method', 'your data', 'the data', 'your data', 'outliers', 'median', 'your data', 'it', 'central tendency', 'instance', 'you', 'outliers', 'large values', 'the mean', 'it', 'an accurate representation', 'your values', 'median', 'outliers', 'the same way', 'we', 'outliers', 'statistics', 'they', 'numerical data', 'methods', 'describe', 'other methods', 'the mean median', 'others', 'You', 'statistics', 'single or multiple columns', 'You', 'specific values', 'categorical attributes', 'You', 'the frequency', 'attribute values', 'That information', 'you', 'some idea', 'what', 'that categorical variable', 'The diagram', 'the car data', 'which', 'several categorical values', 'mat lug boot safety and class safety', 'the described function', 'You', 'three unique values', 'the class column', 'It', 'the top value', 'UN AC C', 'which', 'This', 'the 1728 values', '70%', 'This', 'an imbalance', 'a target variable', 'That', 'a categorical type', 'You', 'the class distribution', 'a class imbalance', 'your data set', 'imbalanced data', 'a disproportionate ratio', 'your classes', 'instance', 'your data set', 'credit card transactions', 'a percent', 'fraud', 'this case', 'your algorithm', 'examples', 'credit card fraud visualization', 'you', 'insights', 'your data', 'you', 'a histogram', 'a good visualization technique', 'the overall behavior', 'a particular feature', 'a histogram', 'You', 'questions', 'the feature data', 'How many peaks', 'the data', 'any skewness', 'that particular feature', 'histograms', 'your data visualization values', 'the taller peaks', 'the histogram', 'the most common values', 'numerical features', 'You', 'density plots', 'box plots', 'addition', 'histograms', 'an idea', 'what', 'that particular feature', 'a histogram', 'These visualizations', 'you', 'questions', 'what', 'the range', 'the data', 'the peak', 'the data', 'any outliers', 'any special features', 'these questions', 'you', 'your data', 'you', 'you', 'more specialized data', 'Preprocessing', 'A box plot', 'a method', 'graphically depicting groups', 'numerical data', 'their core tiles', 'you', 'more than two numerical variables', 'a feature data', 'you', 'their relationship', 'A scatter plot', 'a good way', 'any special relationships', 'those variables', 'this case', 'the left diagram', 'sulfates', 'alcohol', 'two numerical variables', 'you', 'the relationship', 'these variables', 'You', 'a scatter plot', 'you', 'plots', 'the correlation', 'them', 'the data', 'you', 'some relatively positive relationships', 'the two variables scatter plot matrices', 'you', 'the relationship', 'multiple different features', 'Pandi', 'you', 'scatter plot matrices', 'the columns', 'You', 'this example', 'three columns', 'it', 'the pair wise scatter plot', 'any two columns', 'a scatter plot', 'You', 'special regions', 'a particular subset', 'data', 'the example', 'a relationship', 'alcohol sulfates', 'quality', 'You', 'those values', 'good and poor quality wines', 'the example', 'you', 'an idea', 'how useful particular variables', 'you', 'them', 'a classification problem', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'correlations', 'the takeaways', 'this section']\n",
      "Video: demo1-job_name_1.1_16\n",
      "All the text from the video: Hi, welcome back. Now we'll review how to find correlations in your data set. How can you quantify the linear relationship among the variables you're seeing in a scanner plot? A correlation matrix is a good tool. In this situation, it conveys both the strong and weak linear relationships among numerical variables. Correlation can go as high as one or as low as minus one when the correlation is one, this means those two numerical features are perfectly correlated with each other. It's like saying Y is proportional to X when the correlation of those two variables is minus one. It's like saying Y is proportional to minus X, any linear relationship in between can be quantified by the correlation. So if the correlation is zero, this means there's no linear relationship but it doesn't mean that there's no relationship. It's just an indication that there's no linear relationship between those two variables. However, looking at a number isn't always straightforward. Often it's easier to view the numbers when they're represented by colors. Now we'll look at the heat map the highest number one in dark green and minus one is in dark brown. The color gives you both the positive and negative directions. And it also shows how strong the correlations are. We can use the Seaborne heat map function to show the correlation matrix. Looking at the chart, there's some correlation between citric acid and fixed acidity that would be expected in wine because citric acid contributes to the acidity of the wine. However, there isn't much correlation between fixed acidity and P H P H is a measurement of the strength of those acids present. But fixed acidity is a measure of the quantity in this particular data set. There doesn't appear to be a correlation here. Some key takeaways from this section of the module include these points. The first step is to get your data into a format that can be used easily. Pandi is a popular Python library for working with data descriptive statistics will help you gain insights into the data. You can use visualizations to examine the data set in more detail. That's it for this section. We'll see you again in the next video.\n",
      "All the key phrases from the video: ['we', 'correlations', 'you', 'the linear relationship', 'the variables', 'you', 'a scanner plot', 'A correlation matrix', 'a good tool', 'this situation', 'it', 'both the strong and weak linear relationships', 'numerical variables', 'Correlation', 'the correlation', 'this', 'those two numerical features', 'It', 'Y', 'X', 'the correlation', 'those two variables', 'It', 'Y', 'minus X', 'any linear relationship', 'the correlation', 'the correlation', 'this', 'no linear relationship', 'it', 'no relationship', 'It', 'an indication', 'no linear relationship', 'those two variables', 'a number', 'it', 'the numbers', 'they', 'colors', 'we', 'the heat map', 'dark green', 'dark brown', 'The color', 'you', 'both the positive and negative directions', 'it', 'the correlations', 'We', 'the Seaborne heat map function', 'the correlation matrix', 'the chart', 'some correlation', 'citric acid', 'fixed acidity', 'that', 'wine', 'citric acid', 'the acidity', 'the wine', 'much correlation', 'fixed acidity', 'P H P H', 'a measurement', 'the strength', 'those acids', 'fixed acidity', 'a measure', 'the quantity', 'this particular data set', 'a correlation', 'Some key takeaways', 'this section', 'the module', 'these points', 'The first step', 'your data', 'a format', 'that', 'Pandi', 'a popular Python library', 'data descriptive statistics', 'you', 'insights', 'the data', 'You', 'visualizations', 'the data', 'more detail', 'That', 'it', 'this section', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_17\n",
      "All the text from the video: Hi and welcome to section four in this section. We're going to look at feature engineering. Feature engineering is one of the most impactful things you can do to improve your machine learning model. We'll now look at what it is. There are two things that can help make your models more successful. The first is feature selection and the second is feature extraction or the process of creating features in feature selection. You select the most relevant features and discard the rest. You can apply feature selection to prevent redundancy or irrelevance in the existing features. You can also use it to limit the number of features to help prevent overfitting. Feature extraction builds valuable information from raw data by reformatting combining and transforming primary features into new ones. This process continues until it yields a new data set that can be consumed by the model to achieve your goals. As the diagram shows feature extraction covers a range of activities from dealing with missing data to converting text data into numerical data. Although the list isn't exhaustive, it should give you some idea of the data handling that's needed to get data into a useful state. Many of the tasks are no different than any other job working with data. You'll want to make sure data is in the correct format that it's consistently represented correctly spelled among other tasks. For example, you might combine data or extract data into multiple columns or you could also remove columns altogether. Specific to machine learning. You'll need to convert text columns to numerical values. You'll also need to decide how to handle outliers and potentially res scale your data. Next, we'll look at some of the more common tasks in this section. Most machine learning algorithms work best with numerical data. You'll need to make sure that all columns in your data set contain numeric data by converting or encoding it. You might need to make several passes through the data sheet before you can encode it. For example, you might have variability in the text values such as rows that contain both medium and M E D as values. If the categorical data has order to it, you'll want to encode the text into numerical values that capture this ordinal relationship. Say you have data showing maintenance costs, you might encode low to one, medium to two, high to three and very high to four. After you've made sure your categorical data is all uniform, you can use tools like sky kit learn and pandas to encode your data. If the categorical data doesn't have any order to it, then you'll need to break the data into multiple columns this will help make sure you don't introduce an ordinal relationship to the data that isn't there. For example, suppose you assigned a value of one to the first color such as red and you then assign two to the next value. Say blue, the model could interpret blue as being more important than red because blue has a higher numeric value encoding non ordinal data into multiple columns or features is a better way. Think of the new features like a check box. Consider the example, there are three features that were generated. The value one indicates that the instance has that feature like its color. That's it for this section, we'll see you again in the next video.\n",
      "All the key phrases from the video: ['Hi and welcome', 'section', 'this section', 'We', 'feature engineering', 'Feature engineering', 'the most impactful things', 'you', 'your machine learning model', 'We', 'what', 'it', 'two things', 'that', 'your models', 'feature selection', 'feature extraction', 'the process', 'features', 'feature selection', 'You', 'the most relevant features', 'the rest', 'You', 'feature selection', 'redundancy', 'irrelevance', 'the existing features', 'You', 'it', 'the number', 'features', 'overfitting', 'Feature extraction', 'valuable information', 'raw data', 'primary features', 'new ones', 'This process', 'it', 'a new data', 'that', 'the model', 'your goals', 'the diagram', 'feature extraction', 'a range', 'activities', 'missing data', 'text data', 'numerical data', 'the list', 'it', 'you', 'some idea', 'the data handling', 'that', 'data', 'a useful state', 'the tasks', 'any other job', 'data', 'You', 'data', 'the correct format', 'it', 'other tasks', 'example', 'you', 'data', 'data', 'multiple columns', 'you', 'columns', 'machine learning', 'You', 'text columns', 'numerical values', 'You', 'outliers', 'scale', 'your data', 'we', 'some', 'the more common tasks', 'this section', 'algorithms', 'numerical data', 'You', 'all columns', 'your data', 'numeric data', 'it', 'You', 'several passes', 'the data sheet', 'you', 'it', 'example', 'you', 'variability', 'the text values', 'rows', 'that', 'both medium', 'M E D', 'values', 'the categorical data', 'order', 'it', 'you', 'the text', 'numerical values', 'that', 'this ordinal relationship', 'you', 'data', 'maintenance costs', 'you', 'you', 'your categorical data', 'you', 'tools', 'your data', 'the categorical data', 'any order', 'it', 'you', 'the data', 'multiple columns', 'this', 'you', 'an ordinal relationship', 'the data', 'that', 'example', 'you', 'a value', 'the first color', 'red', 'you', 'the next value', 'blue', 'the model', 'blue', 'a higher numeric value', 'non ordinal data', 'multiple columns', 'features', 'a better way', 'the new features', 'a check box', 'the example', 'three features', 'that', 'The value', 'one', 'the instance', 'that feature', 'its color', 'That', 'it', 'this section', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_18\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring feature engineering by reviewing how to clean your data set. In addition to converting string data to numerical data, you'll need to clean your data set for several other potential problem areas before encoding the string data, make sure the strings are all consistent. You'll also need to make sure variables use a consistent scale. For example, if one variable describes the number of doors in a car, the scale will probably be between two and eight. But if another variable describes the number of cars of a particular type sold in the state of California, the scale will probably be in the thousands. Some data items might also capture more than one variable in a single value. For instance, suppose the data set includes variables that combine safety and maintenance into a single variable such as safe, high maintenance, you'll need to train your machine learning system for both variables and also split that single variable into two separate variables. You might also encounter data sets that are missing data for some variables and some data sets will include outliers. We'll cover techniques for dealing with these situations. In this section. You might find that data is missing. For example, some columns in your data set could be missing data because of a data collection error or maybe data wasn't collected on a particular feature until the data collection process was underway. Missing data can make it difficult to accurately interpret the relationship between the related feature and the target variable. So regardless of how the data ended up being missed, it's important for you to deal with this issue. Unfortunately, most machine learning algorithms can't handle missing values automatically. You'll need to use human intelligence to update missing values with data that's meaningful and relevant to the problem. Most Python libraries for data manipulation include functions for finding missing data. So how do you decide if you should drop or impute missing values? This question is answered in part by better understanding how those values came to be missing in the first place and how much data the missing values represent within your larger data set. For instance, say the missing values are randomly spread throughout your data set and don't represent a larger portion of its respective row or column. In this case, imputation is most likely the better option. In contrast, say that you have a column or row that has a large percentage of missing values. In this case, dropping the entire row or column would be preferred over imputation. If you decide to drop rows with missing data, you can use built in functions to do this for example, pandas drop N A function can drop all rows with missing data or you can drop specific data values by using a subset as an alternative to dropping missing values. You can impute values for those missing values. There are different ways to impute a missing value for categorical values. The missing value is usually replaced with the mean, the median or the most frequent values for numerical or continuous variables. The missing value is usually replaced with the mean or the median. You can impute a single row of missing data which is known as una variate. You can also do this for multiple rows which is known as Multivariate. We'll now look at a Univ variate example here, the ci pit learn computer function is being used to impute some missing values. It's a fairly small data set, but there are two missing values. The missing value was imputed by the strategy of the mean to do this. You first calculate the mean here, it's the mean of three and two, which is 2.5. Then you'll impute the mean value for the missing value. Some data libraries include an impute package that provides more complex ways to impute data. Examples include K nearest neighbor soft impute multiple imputation by chain equations and others. That's it. For part two of this section, we'll see you again for part three where we'll review how to work with outliers in your data.\n",
      "All the key phrases from the video: ['We', 'feature engineering', 'addition', 'string data', 'numerical data', 'you', 'your data', 'several other potential problem areas', 'the string data', 'the strings', 'You', 'variables', 'a consistent scale', 'example', 'one variable', 'the number', 'doors', 'a car', 'the scale', 'another variable', 'the number', 'cars', 'a particular type', 'the state', 'California', 'the scale', 'the thousands', 'Some data items', 'more than one variable', 'a single value', 'instance', 'the data set', 'variables', 'that', 'safety', 'maintenance', 'a single variable', 'safe, high maintenance', 'you', 'your machine learning system', 'both variables', 'that single variable', 'two separate variables', 'You', 'data sets', 'that', 'data', 'some variables', 'some data sets', 'outliers', 'We', 'techniques', 'these situations', 'this section', 'You', 'data', 'example', 'some columns', 'your data', 'set', 'data', 'a data collection error', 'data', 'a particular feature', 'the data collection process', 'Missing data', 'it', 'the relationship', 'the related feature', 'the target variable', 'the data', 'it', 'you', 'this issue', 'most machine learning algorithms', 'missing values', 'You', 'human intelligence', 'missing values', 'data', 'that', 'the problem', 'Most Python', 'data manipulation', 'functions', 'missing data', 'you', 'you', 'missing values', 'This question', 'part', 'those values', 'the first place', 'how much data', 'the missing values', 'instance', 'the missing values', 'your data', 'a larger portion', 'its respective row', 'column', 'this case', 'imputation', 'the better option', 'contrast', 'you', 'a column', 'row', 'that', 'a large percentage', 'missing values', 'this case', 'the entire row', 'column', 'imputation', 'you', 'rows', 'missing data', 'you', 'functions', 'this', 'example', 'A function', 'all rows', 'missing data', 'you', 'specific data values', 'a subset', 'an alternative', 'missing values', 'You', 'values', 'those missing values', 'different ways', 'a missing value', 'categorical values', 'The missing value', 'the mean', 'the median', 'the most frequent values', 'numerical or continuous variables', 'The missing value', 'the mean', 'the median', 'You', 'a single row', 'missing data', 'which', 'una variate', 'You', 'this', 'multiple rows', 'which', 'Multivariate', 'We', 'a Univ variate example', 'the ci pit', 'computer function', 'some missing values', 'It', 'a fairly small data set', 'two missing values', 'The missing value', 'the strategy', 'the mean', 'this', 'You', 'the mean', 'it', 'the mean', 'which', 'you', 'the mean value', 'the missing value', 'Some data libraries', 'an impute package', 'that', 'more complex ways', 'data', 'Examples', 'soft impute', 'chain equations', 'others', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'outliers', 'your data']\n",
      "Video: demo1-job_name_1.1_19\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring feature engineering by describing how to work with outliers. You might also need to clean your data based on any outliers that exist. Outliers are points in your data set that lie at an abnormal distance from other values. They're not always something you want to clean up because they can add richness to your data set. But they can also make it harder to make accurate predictions because they skew values away from the other more normal values related to that feature. An outlier might also indicate that the data point actually belongs to another column. You can think of outliers as falling into two broad categories. The first is a single variation for just a single variable or a uni variate outlier. The second is a variation of two or more variables or a Multivariate outlier. One of the more common ways to find unvaried outliers is with a box plot. A box plot shows how far a data point is to the mean for that variable. The box in the plot shows the data values within two core tiles of the mean values outside that range are represented by the lines extending from the box which are sometimes called whiskers. A scatter plot can be an effective way to see Multivariate outliers. For example, this diagram shows the amount of sulfates and alcohol in a collection of wines with the scatter plot. You can quickly visualize whether there are Multivariate outliers for the two variables. The origin of your outlier will most likely inform how you deal with it during this pre processing phase of the pipeline or possibly later. During feature engineering, there are several different approaches to dealing with outliers. You could delete the outlier if your outlier is based on an artificial error. This means the outlier isn't natural. It was introduced because of some failure, like incorrectly entered data. You could also transform the outlier by taking the natural log of a value. This in turn reduces the variation caused by the extreme outlier value which would then reduce the outlier influence on the overall data set. Finally, you could use the mean of the feature and impute that value to replace the outlier value. Again, this would be a good approach. If the outlier was caused by artificial error, this isn't an exhaustive list, but it describes the most common options. After you've extracted features, you'll need to select the most appropriate features for training your model. There are three main feature selection methods. Filter methods use statistical methods to measure the relevance of features by their correlation with the target variable wrapper methods measure how useful. A subset of a feature is they do this by training a model on the feature and then measuring how successful the model is. Filters are faster and cheaper than wrapper methods because they don't involve training the models repeatedly. Wrappers typically find the best subset of features. But there's a risk of overfitting compared to using subsets of features from filter methods. Embedded methods are algorithm specific and they might use a combination of both filters and wrappers. Filter methods use a proxy measure instead of the actual model's performance, they're fast to compute but they can still capture how useful the feature set is. Here are some common measures. The first is Pearson's correlation coefficient which measures the statistical relationship or association between two continuous variables. The second is linear discriminant analysis or LDA. This is used to find a linear combination of features that separates two or more classes. The third is analysis of variance or a Nova. This is used to analyze the differences among group means in a sample. And finally, chi square is a single number that tells you how much difference exists between your observed counts and the counts you'd expect if there were absolutely no relationships in the population filters are usually less computationally intensive than wrappers. But they produce a feature set that isn't tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than one from a wrapper, the filter also usually has a lower prediction performance than a wrapper. However, the filter's feature set doesn't contain the assumptions of a prediction model. So it's more useful for exposing relationships between features. Many filters provide feature ranking instead of an explicit best feature subset. And the cut off point in the ranking is chosen through cross validation filters have also been used as a preprocessing step for wrappers which enables a wrapper to be used on larger problems. Wrapper methods use a predictive model to score feature subsets. Each new subset is used to train a model which is then tested on a holdout set. The score for that subset is calculated by counting the number of mistakes made on that holdout set or the error rate of the model. Because rappers train a new model for each subset they're computationally intensive. However, they usually provide the best performing feature set for that particular type of model or problem forward selection starts with no features and adds them until the best model is found backward selection starts with all features, drops them one at a time and then selects the best model. Embedded methods combine the qualities of filter and wrapper methods. They're implemented by algorithms that have their own built in feature selection methods. Some of the most popular examples of these methods are lasso and ridge regression. They have built in penalization functions to reduce overfitting. Here are some key takeaways from this section of the module. First feature engineering involves selecting the best features for machine learning preprocessing, gives you better data to work with and better data typically provides better results. Two categories for preprocessing are converting data to numerical values and cleaning up dirty data by removing missing data and cleaning outliers. Finally, how you handle dirty data impacts your model? That's it. For section four, we'll see you in the next video.\n",
      "All the key phrases from the video: ['We', 'feature engineering', 'outliers', 'You', 'your data', 'any outliers', 'that', 'Outliers', 'points', 'your data', 'that lie', 'an abnormal distance', 'other values', 'They', 'something', 'you', 'they', 'they', 'it', 'accurate predictions', 'they', 'values', 'the other more normal values', 'that feature', 'An outlier', 'the data point', 'another column', 'You', 'outliers', 'two broad categories', 'a single variation', 'just a single variable', 'a uni variate outlier', 'a variation', 'two or more variables', 'a Multivariate outlier', 'the more common ways', 'unvaried outliers', 'a box plot', 'A box plot', 'a data point', 'the mean', 'that variable', 'The box', 'the plot', 'the data values', 'two core tiles', 'the mean values', 'that range', 'the lines', 'the box', 'which', 'whiskers', 'A scatter plot', 'an effective way', 'Multivariate outliers', 'example', 'this diagram', 'the amount', 'sulfates', 'alcohol', 'a collection', 'wines', 'the scatter plot', 'You', 'Multivariate outliers', 'the two variables', 'The origin', 'your outlier', 'you', 'it', 'this pre processing phase', 'the pipeline', 'feature engineering', 'several different approaches', 'outliers', 'You', 'the outlier', 'your outlier', 'an artificial error', 'This', 'the outlier', 'It', 'some failure', 'incorrectly entered data', 'You', 'the outlier', 'the natural log', 'a value', 'This', 'turn', 'the variation', 'the extreme outlier value', 'which', 'the outlier influence', 'you', 'the mean', 'the feature', 'that value', 'the outlier value', 'this', 'a good approach', 'the outlier', 'artificial error', 'this', 'an exhaustive list', 'it', 'the most common options', 'you', 'features', 'you', 'the most appropriate features', 'your model', 'three main feature selection methods', 'Filter methods', 'statistical methods', 'the relevance', 'features', 'their correlation', 'the target variable wrapper methods', 'A subset', 'a feature', 'they', 'this', 'a model', 'the feature', 'the model', 'Filters', 'wrapper methods', 'they', 'the models', 'Wrappers', 'the best subset', 'features', 'a risk', 'subsets', 'features', 'filter methods', 'Embedded methods', 'they', 'a combination', 'both filters', 'wrappers', 'Filter methods', 'a proxy measure', \"the actual model's performance\", 'they', 'they', 'the feature', 'some common measures', \"Pearson's correlation coefficient\", 'which', 'the statistical relationship', 'association', 'two continuous variables', 'linear discriminant analysis', 'LDA', 'This', 'a linear combination', 'features', 'that', 'two or more classes', 'analysis', 'variance', 'a Nova', 'This', 'the differences', 'a sample', 'chi square', 'a single number', 'that', 'you', 'how much difference', 'your observed counts', 'the counts', 'you', 'no relationships', 'the population filters', 'wrappers', 'they', 'that', 'a specific type', 'predictive model', 'This lack', 'tuning', 'a feature', 'a filter', 'a wrapper', 'the filter', 'a lower prediction performance', 'a wrapper', 'the assumptions', 'a prediction model', 'it', 'relationships', 'features', 'Many filters', 'feature', 'an explicit best feature subset', 'the cut off point', 'the ranking', 'cross validation filters', 'a preprocessing step', 'wrappers', 'which', 'a wrapper', 'larger problems', 'Wrapper methods', 'a predictive model', 'feature subsets', 'Each new subset', 'a model', 'which', 'a holdout set', 'The score', 'that subset', 'the number', 'mistakes', 'the model', 'rappers', 'a new model', 'each subset', 'they', 'they', 'the best performing feature', 'that particular type', 'model', 'problem', 'forward selection', 'no features', 'them', 'the best model', 'backward selection', 'all features', 'them', 'a time', 'the best model', 'Embedded methods', 'the qualities', 'filter and wrapper methods', 'They', 'algorithms', 'that', 'feature selection methods', 'Some', 'the most popular examples', 'these methods', 'lasso', 'ridge regression', 'They', 'penalization functions', 'some key takeaways', 'this section', 'the module', 'First feature engineering', 'the best features', 'machine learning preprocessing', 'you', 'better data', 'better data', 'better results', 'Two categories', 'preprocessing', 'data', 'numerical values', 'dirty data', 'missing data and cleaning outliers', 'you', 'your model', 'That', 'it', 'section', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_20\n",
      "All the text from the video: Hi, welcome back to module three. This is section five on training in this section. We're going to look at how to select a model and train it with the data we have pre processed at this point. You've done a lot to clean and prepare your data, but that doesn't mean your data is completely ready to train the algorithm. Some algorithms may not be able to work with training data in a data frame format. Some file formats like CS V are commonly used by various algorithms but they do not make use of that optimization that some of the file formats like record IO proto buff can use many Amazon Sagemaker algorithms support training with data in a CS V format. Amazon's sagemaker requires that a CS V file doesn't have a header record and that the target variable is in the first column. Most Amazon sagemaker algorithms work best when you use the optimized proto buff record I O format for the training data using this format allows you to take advantage of pipe mode when training the algorithms that support it in pipe mode. Your training job streams data directly from Amazon S3. When using the CS V format, the target variable in your training data set should be the first column on the left and your features should be to the right of the target variable column. Evaluating a model with the same data that it trained on will lead to overfitting. Recall. Overfitting is where your model learns the particulars of a data set too. Well, it's essentially memorizing the training data rather than learning the relationships between features and labels. This means the model isn't learning from those relationships and patterns to apply them to new data in the future. Hold out is when you split your data into multiple sets, commonly sets for training data validation data and testing data, training data, which includes both features and labels feeds into the algorithm you've selected to produce your model. You then use the model to make predictions over the validation data set, which is where you'll likely notice things you'll want to tweak and tune and change. Then when you're ready, you run the test data set, which only includes features. Since you want the labels to actually be predicted the performance you get here with the test data set is what you can reasonably expect to see in production. A common split when using the holdout method is using 80% of the data for a training set, 10% for validation and 10% for test. Or if you have a lot of data, you can split it into 70% training 15% validation and 15% test. So for a small data set, we can use K fold cross validation to utilize as much of the data as possible while still having relatively good metrics. In order to choose which model is better K fold cross validation randomly partitions the data into k different segments. For each segment, we'll use the rest of the data outside of it for training in order to do a validation on that particular segment. Let's look at an example here we have a five fold cross validation. The available training data is separated into five different chunks. For the training of the first model. We're using all those chunks as the training data. And then we're going to calculate the metrics on this test piece for the second bottle. We're going to use these pieces as training. After the model is trained, you apply it to this test piece. We do the same thing five times. We use all the training data and we test it on five different models on different chunks of the test data, eventually testing it on all data points. One other thing to note about splitting your data data in a specific order can lead to biases on your model. This is especially true if you're working with structured data. For example, the wine data is ordered by the quality column. When you run your model against your test data, this ordered pattern will be applied, biasing the model. It might also mean that some targets are missing from the training data, typically randomizing your dataset prior to splitting is sufficient. And many libraries will provide functions for this with smaller sets. It is sometimes useful to use stratified sampling, stratified sampling ensures that the training and test sets have approximately the same percentage of samples of each target class as the complete set. An internet search will give you many ways to shuffle and split the data. One of the easiest is to use the train test split function from S K Learn. Amazon Sagemaker provides four different ways. You can train models. The built in algorithms available can be easily deployed from the AWS console CL or a Jupiter notebook containers are used behind the scenes when you use one of the Amazon Sage maker built in algorithms, but you do not have to deal with them directly. Amazon Sagemaker supported frameworks provide prebuilt containers to support deep learning frameworks such as a patchy M X net tensor flow pie Toch and Chainer. It also supports machine learning libraries such as Sky Kit Learn and Spark M L by providing prebuilt Docker images. If you use the Amazon Sagemaker, Python S D K, they're deployed using their respective Amazon Sagemaker S D K estimator class. If there is no prebuilt Amazon Sagemaker container image that you can use or modify for an advanced scenario, you can package your own script or algorithm to use with Amazon Sagemaker, you can use any programming language or framework to develop your container. For an example. If your team works and builds M L models in R, you can build your own containers to train and host an algorithm in R as well. Someone else may have already developed and tuned a model. It is worth looking in the AWS marketplace to find available models. Amazon's Sagemaker provides high performance scalable machine learning algorithms optimized for speed scale and accuracy for supervised learning. Amazon's sagemaker includes X G boost and linear learner algorithms for classification and quantitative or regression problems. There is also a factorization machine to address recommendation and time series prediction problems. Amazon's sagemaker includes support for unsupervised learning such as with K means clustering and principal component analysis PC A to solve problems like identifying customer groupings based on purchasing behavior. Finally, there are a selection of specialized algorithms for processing images and other deep learning tasks. Let's look a little closer at three of the most commonly used built in algorithms and their use cases X G boost or extreme gradient boosting is a popular and efficient open source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler weaker models X G boost has done remarkably well in machine learning competitions because it robustly handles a variety of data types relationships and distributions. The large number of hyper parameters can be tweaked and tuned for improved fit. This flexibility makes X G boost a solid choice for problems in regression classification, binary and multi class and ranking. The Amazon Sagemaker linear learner algorithm provides a solution for both classification and regression problems. With the Amazon Sagemaker algorithm. You can simultaneously explore different training objectives and choose the best solution from your validation set. You can also explore a large number of models and choose the best one for your needs compared with methods that provide a solution for only continuous objectives. The Amazon Sagemaker linear learner algorithm provides a significant increase in speed over naive hyper parameter optimization techniques. K means is an unsupervised learning algorithm. It attempts to find discrete groupings within data where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity to train a model. An Amazon Sagemaker, you create a training job. The training job includes the URL of the Amazon S3 bucket where you've stored the training data, the URL of the S3 bucket where you want to store the output of the job. The Amazon elastic container registry path where the training code is stored. The compute resources that you want. Amazon Sagemaker to use for model training compute resources are M L compute instances managed by Amazon Sagemaker, Amazon. Sagemaker provides a selection of instance types optimized to fit different machine learning. Use cases instance types comprise varying combinations of C P U GP U memory and networking capacity and give you the flexibility to choose the appropriate mix of resources for building training and deploying your M L models. Each instance type includes one or more instance sizes allowing you to scale your resources to the requirements of your target workload. Some key takeaways from this section of the module include split data into training and testing sets helps you validate the model's accuracy. K fold cross validation can help with smaller data sets. Two key algorithms for supervised learning are X G boost and linear learner. Use K means for unsupervised learning and use Amazon Sagemaker to train models. That's it for section five. I hope to see you in the next video.\n",
      "All the key phrases from the video: ['module', 'This', 'section', 'training', 'this section', 'We', 'a model', 'it', 'the data', 'we', 'this point', 'You', 'a lot', 'your data', 'that', 'your data', 'the algorithm', 'Some algorithms', 'training data', 'a data frame format', 'Some file formats', 'CS V', 'various algorithms', 'they', 'use', 'that optimization', 'some', 'the file formats', 'record IO proto buff', 'many Amazon Sagemaker', 'support training', 'data', 'a CS V format', \"Amazon's sagemaker\", 'a CS V file', 'a header record', 'the target variable', 'the first column', 'work', 'you', 'the optimized proto buff record I O format', 'the training data', 'this format', 'you', 'advantage', 'pipe mode', 'the algorithms', 'that', 'it', 'pipe mode', 'Your training job', 'Amazon S3', 'the CS V format', 'the target variable', 'your training data', 'the first column', 'your features', 'the right', 'the target variable column', 'a model', 'the same data', 'that', 'it', 'overfitting', 'your model', 'the particulars', 'a data', 'it', 'the training data', 'the relationships', 'features', 'labels', 'This', 'the model', 'those relationships', 'patterns', 'them', 'new data', 'the future', 'you', 'your data', 'multiple sets', 'commonly sets', 'data validation data and testing data', 'training data', 'which', 'both features', 'labels', 'the algorithm', 'you', 'your model', 'You', 'the model', 'predictions', 'the validation data set', 'which', 'you', 'things', 'you', 'you', 'you', 'the test data set', 'which', 'features', 'you', 'the labels', 'the performance', 'you', 'the test data', 'what', 'you', 'production', 'A common split', 'the holdout method', '80%', 'the data', 'a training set', '10%', 'validation', '10%', 'test', 'you', 'a lot', 'data', 'you', 'it', '70% training', '15% validation', '15% test', 'a small data set', 'we', 'K fold cross validation', 'the data', 'relatively good metrics', 'order', 'which model', 'K', 'cross validation', 'the data', 'k different segments', 'each segment', 'we', 'the rest', 'the data', 'it', 'training', 'order', 'a validation', 'that particular segment', \"'s\", 'an example', 'we', 'a five fold cross validation', 'The available training data', 'five different chunks', 'the training', 'the first model', 'We', 'all those chunks', 'the training data', 'we', 'the metrics', 'this test piece', 'the second bottle', 'We', 'these pieces', 'training', 'the model', 'you', 'it', 'this test piece', 'We', 'the same thing', 'We', 'all the training data', 'we', 'it', 'five different models', 'different chunks', 'the test data', 'it', 'all data points', 'One other thing', 'your data data', 'a specific order', 'biases', 'your model', 'This', 'you', 'structured data', 'example', 'the wine data', 'the quality column', 'you', 'your model', 'your test data', 'this ordered pattern', 'the model', 'It', 'some targets', 'the training data', 'your dataset', 'splitting', 'many libraries', 'functions', 'this', 'smaller sets', 'It', 'stratified sampling', 'stratified sampling', 'the training', 'test sets', 'approximately the same percentage', 'samples', 'each target class', 'the complete set', 'An internet search', 'you', 'many ways', 'the data', 'the train test split function', 'S K Learn', 'Amazon Sagemaker', 'four different ways', 'You', 'models', 'algorithms', 'the AWS console CL', 'a Jupiter notebook containers', 'the scenes', 'you', 'the Amazon Sage maker', 'algorithms', 'you', 'them', 'Amazon Sagemaker', 'frameworks', 'prebuilt containers', 'deep learning frameworks', 'a patchy M X net tensor flow pie Toch', 'Chainer', 'It', 'machine learning libraries', 'Sky Kit Learn', 'Spark M L', 'prebuilt Docker images', 'you', 'the Amazon Sagemaker', 'Python S D K', 'they', 'their respective Amazon Sagemaker S D K estimator class', 'no prebuilt Amazon Sagemaker container image', 'that', 'you', 'an advanced scenario', 'you', 'your own script', 'algorithm', 'Amazon Sagemaker', 'you', 'any programming language', 'framework', 'your container', 'an example', 'your team', 'M L models', 'R', 'you', 'your own containers', 'an algorithm', 'R', 'Someone', 'a model', 'It', 'the AWS marketplace', 'available models', \"Amazon's Sagemaker\", 'high performance scalable machine learning algorithms', 'speed scale', 'accuracy', 'supervised learning', \"Amazon's sagemaker\", 'classification and quantitative or regression problems', 'a factorization machine', 'recommendation and time series prediction problems', \"Amazon's sagemaker\", 'support', 'unsupervised learning', 'K', 'clustering and principal component analysis PC A', 'problems', 'customer groupings', 'purchasing behavior', 'a selection', 'specialized algorithms', 'processing images', 'other deep learning tasks', \"'s\", 'algorithms', 'their use cases', 'X G', 'a popular and efficient open source implementation', 'the gradient', 'trees', 'a supervised learning', 'algorithm', 'that', 'a target variable', 'an ensemble', 'estimates', 'a set', 'simpler weaker models', 'X G boost', 'machine learning competitions', 'it', 'a variety', 'data types relationships', 'distributions', 'The large number', 'hyper parameters', 'This flexibility', 'X G', 'a solid choice', 'problems', 'regression classification', 'binary and multi class', 'ranking', 'The Amazon Sagemaker linear learner algorithm', 'a solution', 'both classification and regression problems', 'the Amazon Sagemaker', 'You', 'different training objectives', 'the best solution', 'your validation set', 'You', 'a large number', 'models', 'your needs', 'methods', 'that', 'a solution', 'only continuous objectives', 'The Amazon Sagemaker linear learner algorithm', 'a significant increase', 'speed', 'naive hyper parameter optimization techniques', 'K', 'an unsupervised learning algorithm', 'It', 'discrete groupings', 'data', 'members', 'a group', 'members', 'other groups', 'You', 'the attributes', 'that', 'you', 'the algorithm', 'similarity', 'a model', 'An Amazon Sagemaker', 'you', 'a training job', 'The training job', 'the URL', 'the Amazon S3 bucket', 'you', 'the training data', 'the URL', 'the S3 bucket', 'you', 'the output', 'the job', 'The Amazon elastic container registry path', 'the training code', 'The compute resources', 'that', 'you', 'Amazon Sagemaker', 'model training compute resources', 'M L compute instances', 'Amazon Sagemaker', 'Amazon', 'Sagemaker', 'a selection', 'instance types', 'different machine learning', 'Use cases', 'instance types', 'varying combinations', 'C P U GP U memory and networking capacity', 'you', 'the flexibility', 'the appropriate mix', 'resources', 'training', 'your M L models', 'Each instance type', 'one or more instance sizes', 'you', 'your resources', 'the requirements', 'your target workload', 'Some key takeaways', 'this section', 'the module', 'split data', 'training and testing sets', 'you', \"the model's accuracy\", 'K', 'cross validation', 'smaller data sets', 'Two key algorithms', 'supervised learning', 'X G', 'linear learner', 'Use K', 'unsupervised learning', 'Amazon Sagemaker', 'train models', 'That', 'it', 'section', 'I', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_21\n",
      "All the text from the video: Hi and welcome back. This is section six and we're going to look at hosting and using the model in this section. We'll look at how you can deploy your trained model so it can be consumed by applications. After you've trained, tuned and tested your model, you'll learn more about testing in the next section. You're now ready to deploy your model. If you're thinking that we're looking at the phases out of order, here's why we're discussing deployment. Now, if you want to test your model and get performance metrics from it, you first need to make an inference or prediction from the model and this typically requires deployment. Deployment for testing is different from production. Although the mechanics are the same. Amazon's Sagemaker provides everything you need to host your model for simple testing and evaluation from a few requests to deployments, handling tens of thousands of requests. There are two ways you can deploy your model for single predictions. You can deploy your model with Amazon Sagemaker hosting services. Sagemaker will deploy multiple compute instances which run your model behind a load balanced endpoint applications can call the API at the end point to make predictions with this model, you can scale the number of instances up or down based on demand to get predictions for an entire data set. Use Amazon's Sagemaker batch transform instead of deploying and maintaining a permanent endpoint sagemaker will spin up your model and perform the predictions for the entire data set. You provide, it will then store the results in Amazon S3 before it shuts down and terminates the compute instances. It's useful for performing batch predictions. When you test the model, you can quickly run your entire validation set against the model without writing any code to process and collate the individual results. The goal of the deployment phase is to provide a managed environment to host models for providing inference securely and with low latency. After your model is deployed into production, you should monitor your production data and retrain your model. If necessary newly deployed models need to reflect the current production data, new data has accumulated over time and it could potentially identify alternative or new outcomes. And so deploying a model is not a one time exercise. Instead it's a continuous process. With one click, you can deploy your model on Amazon ML instances that can automatically scale across multiple availability zones. For higher redundancy, just specify the type of instance and the maximum and minimum number of instances desired sagemaker will take care of the rest. It will launch the instances, deploy your model and set up the secure http S endpoint for your application. Your application only needs to include an API call to this end point to achieve inference with low latency and high throughput. With this architecture, you can integrate your new models into your application in minutes because changes to the model no longer need changes to the application code. Sagemaker manages your production compute infrastructure on your behalf. It can perform health checks, apply security patches and conduct other routine maintenance all with built in Amazon cloudwatch monitoring and logging. After you've trained the model, you can create the endpoint either in code or by using the Sagemaker console. If you're planning to host only a single model, you can create an endpoint for that model. But if you're planning to host multiple models, you need to create a multi model endpoint, multi model. End points provide a scalable and cost effective solution for deploying large numbers of models. They use a shared serving container that's enabled to host multiple models. This reduces hosting costs by improving endpoint utilization compared to using single model endpoints. It also reduces deployment overhead because Sagemaker manages loading models in memory and scaling the models based on the traffic patterns to them. When you deploy machine learning models into production to make predictions on new data, you need to make sure you apply the same data processing steps that were used in training to each inference request. Otherwise you can get incorrect prediction results. By using inference pipelines, you can reuse the data processing steps from model training during inference without maintaining two separate copies of the same code. This helps ensure the accuracy of your predictions and reduces development overhead. Because Sagemaker is a managed service inference pipelines are completely managed. When you deploy the pipeline model, the service installs and runs the sequence of containers on each EC2 instance in the end point or each batch transform job. Additionally, the sequence of feature processing and inference runs with low latency because the containers are collated on the same E two instances. Some key takeaways from this section of the module include these points. You can deploy your train model by using sagemaker to handle API calls from applications or to perform predictions using a batch transformation. The goal of your model is to generate predictions to answer the business problem. Be sure that your model can generate good results before you deploy to production. Finally use multi model endpoint support to save resources when you have multiple models to deploy. That's it. For this section, we'll see you in the next video.\n",
      "All the key phrases from the video: ['This', 'section', 'we', 'the model', 'this section', 'We', 'you', 'your trained model', 'it', 'applications', 'you', 'your model', 'you', 'testing', 'the next section', 'You', 'your model', 'you', 'we', 'the phases', 'order', 'we', 'deployment', 'you', 'your model', 'performance metrics', 'it', 'you', 'an inference', 'prediction', 'the model', 'this', 'deployment', 'Deployment', 'testing', 'production', 'the mechanics', \"Amazon's Sagemaker\", 'everything', 'you', 'your model', 'simple testing', 'evaluation', 'a few requests', 'deployments', 'tens of thousands', 'requests', 'two ways', 'you', 'your model', 'single predictions', 'You', 'your model', 'Amazon Sagemaker hosting services', 'Sagemaker', 'multiple compute instances', 'which', 'your model', 'a load balanced endpoint applications', 'the API', 'the end point', 'predictions', 'this model', 'you', 'the number', 'instances', 'demand', 'predictions', 'an entire data set', \"Amazon's Sagemaker batch transform\", 'a permanent endpoint sagemaker', 'your model', 'the predictions', 'the entire data set', 'You', 'it', 'the results', 'Amazon S3', 'it', 'the compute instances', 'It', 'batch predictions', 'you', 'the model', 'you', 'your entire validation', 'the model', 'any code', 'the individual results', 'The goal', 'the deployment phase', 'a managed environment', 'to host models', 'inference', 'low latency', 'your model', 'production', 'you', 'your production data', 'your model', 'necessary newly deployed models', 'the current production data', 'new data', 'time', 'it', 'alternative or new outcomes', 'a model', 'a one time exercise', 'it', 'a continuous process', 'one click', 'you', 'your model', 'Amazon ML instances', 'that', 'multiple availability zones', 'higher redundancy', 'the type', 'instance', 'the maximum and minimum number', 'instances', 'sagemaker', 'care', 'the rest', 'It', 'the instances', 'your model', 'the secure http S endpoint', 'your application', 'Your application', 'an API call', 'this end point', 'inference', 'low latency', 'high throughput', 'this architecture', 'you', 'your new models', 'your application', 'minutes', 'changes', 'the model', 'changes', 'the application code', 'Sagemaker', 'your production compute infrastructure', 'your behalf', 'It', 'health checks', 'security patches', 'other routine maintenance', 'Amazon cloudwatch monitoring', 'you', 'the model', 'you', 'the endpoint', 'code', 'the Sagemaker console', 'you', 'only a single model', 'you', 'an endpoint', 'that model', 'you', 'multiple models', 'you', 'a multi model endpoint', 'multi model', 'End points', 'a scalable and cost effective solution', 'large numbers', 'models', 'They', 'a shared serving container', 'that', 'multiple models', 'This', 'costs', 'endpoint utilization', 'single model endpoints', 'It', 'deployment overhead', 'Sagemaker', 'loading models', 'memory', 'the models', 'the traffic patterns', 'them', 'you', 'machine learning models', 'production', 'predictions', 'new data', 'you', 'you', 'the same data processing steps', 'that', 'training', 'each inference request', 'you', 'incorrect prediction results', 'inference pipelines', 'you', 'the data processing steps', 'model training', 'inference', 'two separate copies', 'the same code', 'This', 'the accuracy', 'your predictions', 'development', 'Sagemaker', 'a managed service inference pipelines', 'you', 'the pipeline model', 'the service', 'the sequence', 'containers', 'each EC2 instance', 'the end point', 'each batch transform job', 'Additionally, the sequence', 'feature processing and inference runs', 'low latency', 'the containers', 'the same E two instances', 'Some key takeaways', 'this section', 'the module', 'these points', 'You', 'your train model', 'sagemaker', 'API calls', 'applications', 'predictions', 'a batch transformation', 'The goal', 'your model', 'predictions', 'the business problem', 'your model', 'good results', 'you', 'production', 'multi model endpoint support', 'resources', 'you', 'multiple models', 'That', 'it', 'this section', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_22\n",
      "All the text from the video: Hi, welcome back to module three in this section. We'll look at how you can evaluate your model success in predicting results. At this point. You've trained your models. It's now time to evaluate that model to determine if it will do a good job predicting the target on new and future data. Because future instances have unknown target values. You need to assess how the model will perform on data where you already know the target answer. You'll then use this assessment as a proxy for performance on future data. This is the reason why you hold out a sample of your data for evaluating or testing. An important part of this phase involves choosing the most appropriate metric for your business situation. Think back to the earlier section on problem formulation. During that phase, you define your business problem and outcome and then you craft a business metric to evaluate success. The model metric you choose at this phase should be linked to that business metric as much as possible. There's often a high correlation between the two metrics. In addition to considering your business problem and success metric, the type of M L problem you're working with will influence the model metric. You choose throughout the rest of this module. We'll look at examples of common metrics used in classification problems. We'll also look at common metrics used in regression problems. We're going to start by considering a simple binary classification problem. Here's a specific example, imagine that you have a simple image recognition model that's labeling data as either cat or not cat. After the model has been trained, you can use the test data set, you held back to perform predictions to help examine the performance of the model. You can compare the predicted values with the actual values. If you plot the values into a table, like the example, you can start getting some insights into how well the model performed in a confusion matrix. You can get a high level comparison of how the predicted class is matched up against the actual classes. If the actual label or class is cat, which is identified as P for positive and the predicted label or class is also cat, then you have a true positive. This is a good outcome for your model. Similarly, if you have an actual label of not cat, which is identified as N for negative and the predicted label or class is also not cat, then you have a true negative. This is also a good outcome for your model. In both these cases, your model predicted the correct outcome. When it used the testing data. There are two other possible outcomes and both aren't considered good outcomes. The first one is when the actual class is negative. So you got not cat but the predicted class is positive or cat. This is called a false positive because the prediction is positive but incorrect. Finally, there are false negatives. These happen when the actual class is positive. So you got cat but the predicted class is negative or not cat. That's it. For part one of this section, we'll see you again for part two where we'll review calculating classification metrics.\n",
      "All the key phrases from the video: ['module', 'this section', 'We', 'you', 'your model success', 'results', 'this point', 'You', 'your models', 'It', 'time', 'that model', 'it', 'a good job', 'the target', 'new and future data', 'future instances', 'unknown target values', 'You', 'the model', 'data', 'you', 'the target answer', 'You', 'this assessment', 'a proxy', 'performance', 'future data', 'This', 'the reason', 'you', 'a sample', 'your data', 'testing', 'An important part', 'this phase', 'the most appropriate metric', 'your business situation', 'the earlier section', 'problem formulation', 'that phase', 'you', 'your business problem', 'you', 'a business metric', 'success', 'The model metric', 'you', 'this phase', 'that business metric', 'a high correlation', 'the two metrics', 'addition', 'your business problem', 'success metric', 'the type', 'M L problem', 'you', 'You', 'the rest', 'this module', 'We', 'examples', 'common metrics', 'classification problems', 'We', 'common metrics', 'regression problems', 'We', 'a simple binary classification problem', 'a specific example', 'you', 'a simple image recognition model', 'that', 'labeling data', 'either cat', 'the model', 'you', 'the test data', 'you', 'predictions', 'the performance', 'the model', 'You', 'the predicted values', 'the actual values', 'you', 'the values', 'a table', 'the example', 'you', 'some insights', 'the model', 'a confusion matrix', 'You', 'a high level comparison', 'the predicted class', 'the actual classes', 'the actual label', 'class', 'cat', 'which', 'P', 'the predicted label', 'class', 'you', 'This', 'a good outcome', 'your model', 'you', 'an actual label', 'not cat', 'which', 'N', 'the predicted label', 'class', 'you', 'a true negative', 'This', 'a good outcome', 'your model', 'both these cases', 'your model', 'the correct outcome', 'it', 'the testing data', 'two other possible outcomes', 'both', 'good outcomes', 'The first one', 'the actual class', 'you', 'the predicted class', 'This', 'the prediction', 'false negatives', 'These', 'the actual class', 'you', 'cat', 'the predicted class', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'classification metrics']\n",
      "Video: demo1-job_name_1.1_23\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring how to evaluate your model. The diagram shows the confusion matrix of how two different models performed on the same data. Can you tell which one's better? Which is better isn't a good question to ask? What do you mean by better? Does better mean making sure you find all the cats even if it means you'll get many false positives or does better mean making sure the model is the most accurate. It's difficult to see just by looking at the two charts. What if you're trying several models using multiple folds and have hundreds of data points to compare to do that? You'll need to calculate more metrics. The first metric is sensitivity. This is sometimes referred to as recall hit rate or true positive rate. Sensitivity is the percentage of positive identifications. In the cat example, it represents what percentage of cats were correctly identified to calculate sensitivity, take the number of true positives or the number of positive identifications of cats and divide that by the total number of actual cats. In this example, 60% of cats that were cats were correctly identified as cats specificity is sometimes referred to as selectivity or true negative rate specificity is the percentage of negatives correctly identified. In the cat example, this is the number of images that were not cats that were correctly identified as not cats. To calculate specificity, take the number of true negatives and divide that by the total number of actual negatives. So for the example, that's the number of knot cats that were correctly identified divided by the total number of actual knot cats. This means that in the example, 64% of not cats were identified as not cats. Now that you have these metrics for each model, knowing what your business goal is, makes it easier to decide which model to use. Which model would you choose if you wanted to make sure you'll identify as many cats as possible? Model B would be a good answer if you're not concerned about having many false positives, that is if you're not concerned about having incorrectly identified not cats, which model would you choose if you wanted to make sure you identified animals that were not cats model A might work for this scenario. Again, it would depend on how many false negatives you can tolerate. If this was a classification of patients who had heart disease or not, which model would be best. This is where it gets interesting. A fun website might get a bad reputation if it can't identify cats correctly. But if you're trying to diagnose patients, your focus will probably be very different. It's important to understand the tradeoffs you're making. When you decide which model to use, there are also other metrics that can help you make your decisions. That's it. For part two of this section, we'll see you again for part three where we'll start looking at thresholds.\n",
      "All the key phrases from the video: ['We', 'your model', 'The diagram', 'the confusion matrix', 'how two different models', 'the same data', 'you', 'which one', 'Which', 'a good question', 'What', 'you', 'you', 'all the cats', 'it', 'you', 'many false positives', 'the model', 'It', 'the two charts', 'What', 'you', 'several models', 'multiple folds', 'hundreds', 'data points', 'that', 'You', 'more metrics', 'The first metric', 'sensitivity', 'This', 'recall hit rate', 'true positive rate', 'Sensitivity', 'the percentage', 'positive identifications', 'the cat example', 'it', 'what percentage', 'cats', 'sensitivity', 'the number', 'true positives', 'the number', 'positive identifications', 'cats', 'the total number', 'actual cats', 'this example', '60%', 'cats', 'that', 'cats', 'cats specificity', 'selectivity', 'true negative rate specificity', 'the percentage', 'negatives', 'the cat example', 'this', 'the number', 'images', 'that', 'cats', 'that', 'not cats', 'specificity', 'the number', 'true negatives', 'the total number', 'actual negatives', 'the example', 'that', 'the number', 'knot cats', 'that', 'the total number', 'actual knot cats', 'This', 'the example', '64%', 'not cats', 'not cats', 'you', 'these metrics', 'each model', 'what', 'your business goal', 'it', 'which model', 'Which model', 'you', 'you', 'you', 'as many cats', 'Model B', 'a good answer', 'you', 'many false positives', 'you', 'not cats', 'which model', 'you', 'you', 'you', 'animals', 'that', 'cats model A', 'this scenario', 'it', 'how many false negatives', 'you', 'this', 'a classification', 'patients', 'who', 'heart disease', 'which model', 'This', 'it', 'A fun website', 'a bad reputation', 'it', 'cats', 'you', 'patients', 'your focus', 'It', 'the tradeoffs', 'you', 'you', 'which model', 'other metrics', 'that', 'you', 'your decisions', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'thresholds']\n",
      "Video: demo1-job_name_1.1_24\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring how to evaluate your model. Classification models are going to return a probability for the target. This is a value of the input belonging to the target class and it will be between zero and one. To convert the value to a class. You need to determine the threshold to use. You might think it's 50% but you could change it to be lower or higher to improve your results. As you've seen with sensitivity and specificity, there's a trade off between correctly and incorrectly identifying classes, changing the threshold can impact that outcome. We're going to take a look at how you can visualize this a receiver operating characteristic graph is also known as an R C graph. It summarizes all the confusion matrices that each threshold produced to build one you calculate and plot the sensitivity or true positive rate against the false positive rate. On a graph. For each threshold value, you can calculate the false positive rate by subtracting the specificity from one. After you plot those points, you can draw a line between them. The dotted black line from 00 to 11 means that the sensitivity or true positive rate is equal to the false positive rate. The point at 11 means that you've correctly identified all the cats, but you've also incorrectly identified all the not cats. This is bad. Any point on this line means that the proportion of correctly classified samples is the same as the proportion of incorrectly classified samples. The point at 00 represents that there are zero true positives and zero false positives. A model that has high sensitivity and low false positive rate is usually the goal. So it's considered to be better when the line between the threshold recordings is closer towards the top left corner. If you had the data from two models, you could plot out the R O C curve for each model and compare them. However, that can be tedious. There's another graph you can use for this which we'll look at next. Another evaluation metric you can use is the area under the curve receiver operator curve which is also known as an A R C. The A U C part is the area under the plotted line. When the A U C is higher, it means the model will be better at predicting cats as cats and not cats as not cats. You can use the A U C to quickly compare models with each other with the four numbers from our confusion matrix. You can calculate the model's accuracy. This is also known as its score. You can do this by adding up the correct predictions and then dividing that number by the total number of predictions. Though accuracy is widely used metric for classification problems, it has limitations. This metric isn't as effective when there are a lot of true negative cases in your data set. Think about the cat, not cat example, if most of your accuracy is based on true negatives, it says that your model is good at predicting what isn't a cat. In this case, you might not feel confident in your models ability to predict cats after you roll it out into production. This leads to an example of why it's important to make sure that the metric you choose for model evaluation aligns to your business goal. Think about the credit card fraud example, in this case, using accuracy as your main metric probably isn't a good idea because you have a lot of true negatives. Your high true negative number might hide the fact that your model's ability to identify cases of fraud that is to identify true positives isn't ideal as a credit card company. It's probably unacceptable to have less than almost perfect performance, identifying fraud cases that would drive customers away, which would be the opposite of what you'd want to achieve from a business standpoint. This is why two other metrics are often used in these situations. The first one is precision, which essentially removes the negative predictions. Precision is the proportion of positive predictions that are actually correct. You can calculate it by taking the true positive and dividing it by true positive plus false positive. When the cost of false positives is high in your particular business situation, precision might be a good metric. Think about a classification model that identifies email messages as spam or not. In this case, you don't want your model to label an email message as spam and thus prevent your users from seeing that message when it's actually legitimate or consider an example of a model that needs to predict whether a patient has a terminal illness. In this case, using precision as your evaluation metric doesn't account for false negatives in your model here. For the model to be successful. It's crucial that it doesn't falsely predict the absence of illness in a patient who actually has that illness sensitivity would be a better metric to use for this situation. But it doesn't always need to be one or the other. The F one score combines precision and sensitivity together. It gives you one number that quantifies the overall performance of a particular M L algorithm. You should consider using an F one score when you have a class imbalance but want to preserve the equality between precision and sensitivity. But what do you do if you're dealing with a regression problem? In that case, there are other common metrics you can use to evaluate your model including the main squared error. The mean squared error is frequently used. Its general purpose is the same as what you saw with classification metrics, you determine the prediction from the model and you compare the difference between the prediction and the actual outcome. More specifically, you take the difference between the prediction and actual value square that difference and then sum up all the squared differences for all the observations in sky kit learn. You can use the mean squared error function directly from the metrics library. There are other metrics you can use for linear models such as R squared. So you've trained your model performed a batch transformation on your test data and calculated your metrics. Now, what will you do? You'll use these metrics to help you tune the model. You could select a different set of features and train the model again. After you retrain the model ask yourself which was the better model? The metrics will help inform you you could also use different data and retrain the model with the same features. Remember K fold cross validation from earlier in this module. Finally, you could tune the parameters of the model itself, which is the subject of the next section. Here are key takeaways from this section of the module to evaluate the model. You need to have data that the model hasn't seen. This could be either a holdout set or you could use K fold cross validation. Different machine learning models use different metrics classification can use the confusion matrix and the A U C R O C that you can generate from it regression can use means squared. That's it for section seven. See you in the next video.\n",
      "All the key phrases from the video: ['We', 'your model', 'Classification models', 'a probability', 'the target', 'This', 'a value', 'the input', 'the target class', 'it', 'the value', 'a class', 'You', 'the threshold', 'You', 'it', '50%', 'you', 'it', 'your results', 'you', 'sensitivity', 'specificity', 'a trade', 'correctly and incorrectly identifying classes', 'the threshold', 'that outcome', 'We', 'a look', 'you', 'this', 'a receiver operating characteristic graph', 'an R C graph', 'It', 'all the confusion matrices', 'that', 'each threshold', 'you', 'the sensitivity or true positive rate', 'the false positive rate', 'a graph', 'each threshold value', 'you', 'the false positive rate', 'the specificity', 'you', 'those points', 'you', 'a line', 'them', 'The dotted black line', 'the sensitivity or true positive rate', 'the false positive rate', 'The point', '11 means', 'you', 'all the cats', 'you', 'all the not cats', 'This', 'Any point', 'this line', 'the proportion', 'correctly classified samples', 'the proportion', 'incorrectly classified samples', 'The point', 'zero true positives', 'zero false positives', 'A model', 'that', 'high sensitivity', 'low false positive rate', 'the goal', 'it', 'the line', 'the threshold recordings', 'the top left corner', 'you', 'the data', 'two models', 'you', 'the R O C curve', 'each model', 'them', 'that', 'another graph', 'you', 'this', 'which', 'we', 'Another evaluation metric', 'you', 'the area', 'the curve receiver operator curve', 'which', 'an A R C.', 'The A U C part', 'the area', 'the plotted line', 'the A U C', 'it', 'the model', 'cats', 'cats', 'not cats', 'not cats', 'You', 'the A U C', 'models', 'the four numbers', 'our confusion matrix', 'You', \"the model's accuracy\", 'This', 'its score', 'You', 'this', 'the correct predictions', 'that number', 'the total number', 'predictions', 'accuracy', 'classification problems', 'it', 'limitations', 'This metric', 'a lot', 'true negative cases', 'the cat, not cat example', 'your accuracy', 'true negatives', 'it', 'your model', 'what', 'a cat', 'this case', 'you', 'your models ability', 'cats', 'you', 'it', 'production', 'This', 'an example', 'it', 'you', 'model evaluation aligns', 'your business goal', 'the credit card fraud example', 'this case', 'accuracy', 'your main metric', 'a good idea', 'you', 'a lot', 'true negatives', 'Your high true negative number', 'the fact', \"your model's ability\", 'cases', 'fraud', 'that', 'true positives', 'a credit card company', 'It', 'less than almost perfect performance', 'fraud cases', 'that', 'customers', 'which', 'the opposite', 'what', 'you', 'a business standpoint', 'This', 'two other metrics', 'these situations', 'The first one', 'precision', 'which', 'the negative predictions', 'Precision', 'the proportion', 'positive predictions', 'that', 'You', 'it', 'it', 'the cost', 'false positives', 'your particular business situation', 'precision', 'a classification model', 'that', 'email messages', 'spam', 'this case', 'you', 'your model', 'an email message', 'spam', 'your users', 'that message', 'it', 'an example', 'a model', 'that', 'a patient', 'a terminal illness', 'this case', 'precision', 'your evaluation metric', 'false negatives', 'your model', 'the model', 'It', 'it', 'the absence', 'illness', 'a patient', 'who', 'that illness sensitivity', 'this situation', 'it', 'The F one score', 'precision', 'sensitivity', 'It', 'you', 'one number', 'that', 'the overall performance', 'a particular M L algorithm', 'You', 'an F one score', 'you', 'a class imbalance', 'the equality', 'precision', 'sensitivity', 'what', 'you', 'you', 'a regression problem', 'that case', 'other common metrics', 'you', 'your model', 'the main squared error', 'The mean squared error', 'Its general purpose', 'what', 'you', 'classification metrics', 'you', 'the prediction', 'the model', 'you', 'the difference', 'the prediction', 'the actual outcome', 'you', 'the difference', 'the prediction', 'actual value', 'that difference', 'all the squared differences', 'all the observations', 'You', 'the mean squared error function', 'the metrics library', 'other metrics', 'you', 'linear models', 'R', 'you', 'your model', 'a batch transformation', 'your test data', 'your metrics', 'what', 'you', 'You', 'these metrics', 'you', 'the model', 'You', 'a different set', 'features', 'the model', 'you', 'the model', 'yourself', 'which', 'the better model', 'The metrics', 'you', 'you', 'different data', 'the model', 'the same features', 'K', 'cross validation', 'this module', 'you', 'the parameters', 'the model', 'itself', 'which', 'the subject', 'the next section', 'key takeaways', 'this section', 'the module', 'the model', 'You', 'data', 'that', 'the model', 'This', 'you', 'K fold cross validation', 'Different machine learning models', 'different metrics classification', 'the confusion matrix', 'the A U C R O C', 'you', 'it', 'regression', 'means', 'That', 'it', 'section', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_25\n",
      "All the text from the video: Hi and welcome back to module three. This is section eight in this section, we're going to take a look at how you can tune the model's hyper parameters to improve model performance. Recall from an earlier module that hyper parameters can be thought of as the knobs that tune the machine learning algorithm to improve its performance. Now that we're looking more explicitly at tuning models. It's time to look more specifically at the different types of hyper parameters and how to perform hyper parameter optimization. There are a couple of different categories of hyper parameters. The first kind are model hyper parameters. They help define the model itself as an example, consider a neural network for a computer vision problem for this case. Additional attributes of the architecture need to be defined like filter size pooling and the stride or padding. The second kind are optimizer hyper parameters. They relate to how the model learns patterns based on data and they're used for a neural network model. These types of hyper parameters include optimizes like gradient descent and stochastic gradient descent. They can also include optimizes that use momentum like atom or that initialize the parameter weights with methods like xavier initialization or he initialization. The third kind are data hyper parameters. They relate to the attributes of the data itself. These include attributes that define different data augmentation techniques like cropping or resizing for image related problems. They're often used when you don't have enough data or enough variation in your data tuning hyper parameters can be very labor intensive. Traditionally, this was done manually by someone who had domain experience related to the hyper parameter. And the use case, this person would manually select the hyper parameters based on their intuition and experience. Then they would train the model and score it on the validation data. This process would be repeated over and over again until they achieved satisfactory results. This manual process isn't always the most thorough and efficient way of tuning hyper parameters with Sagemaker, you can perform automated hyper parameter tuning with Amazon's Sagemaker, automatic model tuning. It finds the best version of a model by running multiple training jobs on your data set. By using the algorithm and hyper parameter ranges. You specify it then chooses the hyper parameter values that results in a model that performs the best as measured by a metric. You choose it uses Gaussian process regression to predict which hyper parameter values might be most effective at improving fit. It also uses Bayesian optimization to balance exploring the hyper parameter space and exploiting specific hyper parameter values when appropriate and importantly, automatic model tuning can be used with built in algorithms from sagemaker pre built deep learning frameworks and bring your own algorithm containers suppose that you want to solve a binary classification problem on a fraud data set. Your goal is to maximize the area under the A U C curve metric of the algorithm. By training a linear learner algorithm model. You don't know which values of the learning rate beta one beta two and epoch you should use to train the best model to find the best values for these hyper parameters. You can specify ranges of values that sagemaker hyper parameter tuning will then search it will find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. In the example. Sagemaker hyper parameter tuning launches training jobs that use hyper parameter values in the ranges you specified and then returns the training job with the highest A U C hyper parameter tuning might not necessarily improve your model. It's an advanced tool for building machine solutions as such. It should be considered part of the scientific method process. When you build complex machine learning systems like deep learning neural networks exploring all possible combinations is impractical to improve optimization. Use the following guidelines when you create hyper parameters first, instead of using all hyper parameters, limit the number of hyper parameters to the ones you think would give you good results. The range of values for the hyper parameters. You choose to search can significantly affect the success of hyper parameter optimization. Although you might want to specify a large range that covers every possible value for a hyper parameter, you'll get better results by limiting your search to a small range of values. If you get the best metric values within a part of a range, consider limiting the range to only that part during hyper parameter tuning. Sagemaker attempts to figure out if your hyper parameters are log scaled or linear scaled. Initially, it assumes the hyper parameters are linear scaled. If they should be log scaled, it might take time for Sagemaker to discover that on its own. If you know that a hyper parameter should be long scale and you can convert it yourself doing so can improve hyper parameter optimization. Running more hyper parameter tuning jobs concurrently gets more work done quickly. But a tuning job improves only through successive rounds of experiments. Typically running one training job at a time, achieves the best results with the least amount of compute time. See that you have a distributed training job that runs on multiple instances. In this case, hyper parameter tuning uses the last reported objective metric from all instances of that training job as the value of the objective metric for that training job design, distributed training jobs so that they report the objective metric you want now that you've gone through the end to end process of training and tuning a machine learning model. It's worth talking about Amazon's sagemaker autopilot. This service can help you find a good model with little effort or input on your part with autopilot. You create a job that supplies the test training and target autopilot will analyze the data, select appropriate features and then train and tune the models. It will document the metrics and find the best model based on the provided data. The results include the winning model and metrics and a Jupiter notebook. You can use to investigate the results. Although using autopilot doesn't remove your need to prep process the data. It can save you time during feature selection and model tuning. Some key takeaways from this section of the module include these points. First model tuning is important for finding the best solution to your business problem. Hyper parameters can be tuned for the model optimizer and data sagemaker can perform automatic hyper parameter tuning. And finally, overall model development can be accelerated by using autopilot. That's it for this video. See you in the next one.\n",
      "All the key phrases from the video: ['module', 'This', 'section', 'this section', 'we', 'a look', 'you', \"the model's hyper parameters\", 'model performance', 'Recall', 'an earlier module', 'hyper parameters', 'the knobs', 'that', 'the machine', 'algorithm', 'its performance', 'we', 'tuning models', 'It', 'time', 'the different types', 'hyper parameters', 'hyper parameter optimization', 'a couple', 'different categories', 'hyper parameters', 'The first kind', 'model hyper parameters', 'They', 'the model', 'itself', 'an example', 'a neural network', 'a computer vision problem', 'this case', 'Additional attributes', 'the architecture', 'filter size pooling', 'the stride', 'padding', 'The second kind', 'optimizer', 'hyper parameters', 'They', 'the model', 'patterns', 'data', 'they', 'a neural network model', 'These types', 'hyper parameters', 'optimizes', 'gradient descent', 'stochastic gradient descent', 'They', 'optimizes', 'that', 'momentum', 'atom', 'that', 'the parameter weights', 'methods', 'xavier initialization', 'he initialization', 'The third kind', 'data hyper parameters', 'They', 'the attributes', 'the data', 'itself', 'These', 'attributes', 'that', 'different data augmentation techniques', 'cropping', 'resizing', 'image related problems', 'They', 'you', 'enough data', 'enough variation', 'your data', 'hyper parameters', 'this', 'someone', 'who', 'domain experience', 'the hyper parameter', 'this person', 'the hyper parameters', 'their intuition', 'experience', 'they', 'the model', 'it', 'the validation data', 'This process', 'they', 'satisfactory results', 'This manual process', 'the most thorough and efficient way', 'hyper parameters', 'Sagemaker', 'you', \"Amazon's Sagemaker\", 'It', 'the best version', 'a model', 'multiple training jobs', 'your data set', 'the algorithm', 'hyper parameter', 'You', 'it', 'the hyper parameter values', 'that', 'a model', 'that', 'a metric', 'You', 'it', 'Gaussian process regression', 'which', 'hyper parameter values', 'fit', 'It', 'Bayesian optimization', 'the hyper parameter space', 'specific hyper parameter values', 'automatic model tuning', 'algorithms', 'sagemaker', 'pre', 'deep learning frameworks', 'your own algorithm containers', 'you', 'a binary classification problem', 'a fraud data set', 'Your goal', 'the area', 'the A U C curve metric', 'the algorithm', 'a linear learner algorithm model', 'You', 'which values', 'the learning rate', 'one beta', 'epoch', 'you', 'the best model', 'the best values', 'these hyper parameters', 'You', 'ranges', 'values', 'that', 'hyper parameter tuning', 'it', 'the combination', 'values', 'that', 'the training job', 'that', 'the objective metric', 'that', 'you', 'the example', 'Sagemaker hyper parameter', 'launches', 'jobs', 'that', 'hyper parameter values', 'the ranges', 'you', 'the training job', 'the highest A U C hyper parameter tuning', 'your model', 'It', 'an advanced tool', 'machine solutions', 'It', 'part', 'the scientific method process', 'you', 'complex machine learning systems', 'neural networks', 'all possible combinations', 'optimization', 'the following guidelines', 'you', 'hyper parameters', 'all hyper parameters', 'the number', 'hyper parameters', 'the ones', 'you', 'you', 'good results', 'The range', 'values', 'the hyper parameters', 'You', 'the success', 'hyper parameter optimization', 'you', 'a large range', 'that', 'every possible value', 'a hyper parameter', 'you', 'better results', 'your search', 'a small range', 'values', 'you', 'the best metric values', 'a part', 'a range', 'the range', 'only that part', 'hyper parameter tuning', 'Sagemaker', 'your hyper parameters', 'it', 'the hyper parameters', 'they', 'it', 'time', 'Sagemaker', 'you', 'a hyper parameter', 'long scale', 'you', 'it', 'hyper parameter optimization', 'more hyper parameter', 'jobs', 'more work', 'a tuning job', 'successive rounds', 'experiments', 'one training job', 'a time', 'the best results', 'the least amount', 'compute time', 'you', 'a distributed training job', 'that', 'multiple instances', 'this case', 'hyper parameter tuning', 'the last reported objective metric', 'all instances', 'that training job', 'the value', 'the objective metric', 'that training job design', 'training jobs', 'they', 'the objective metric', 'you', 'you', 'the end', 'end process', 'training', 'a machine learning model', 'It', \"Amazon's sagemaker autopilot\", 'This service', 'you', 'a good model', 'little effort', 'input', 'your part', 'autopilot', 'You', 'a job', 'that', 'the test training', 'the data', 'select appropriate features', 'the models', 'It', 'the metrics', 'the best model', 'the provided data', 'The results', 'the winning model', 'metrics', 'a Jupiter notebook', 'You', 'the results', 'autopilot', 'your need', 'prep', 'the data', 'It', 'you', 'time', 'feature selection', 'model tuning', 'Some key takeaways', 'this section', 'the module', 'these points', 'First model tuning', 'the best solution', 'your business problem', 'Hyper parameters', 'the model optimizer', 'data sagemaker', 'automatic hyper parameter tuning', 'overall model development', 'autopilot', 'That', 'it', 'this video', 'you']\n",
      "Video: demo1-job_name_1.1_26\n",
      "All the text from the video: It's now time to review the module and wrap up with a knowledge check in this module. You learned how to formulate a problem from a business request, obtain and secure data for machine learning build a Jupiter notebook by using Amazon Sagemaker outline the process for evaluating data. Explain why data needs to be pre processed. Use open source tools to examine and prep process data. Use Amazon Sagemaker to train and host a machine learning model. Use cross validation to test the performance of an ML model. Use a hosted model for inference and create an Amazon Sagemaker hyper parameter tuning job to optimize a model's effectiveness that concludes this module. Thanks for watching. We'll see you again in the next video.\n",
      "All the key phrases from the video: ['It', 'time', 'the module', 'a knowledge check', 'this module', 'You', 'a problem', 'a business request', 'data', 'machine learning', 'a Jupiter notebook', 'Amazon Sagemaker', 'the process', 'data', 'data', 'open source tools', 'prep process data', 'Amazon Sagemaker', 'a machine learning model', 'validation', 'the performance', 'an ML model', 'a hosted model', 'inference', 'an Amazon Sagemaker hyper parameter tuning', 'job', \"a model's effectiveness\", 'that', 'this module', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_27\n",
      "All the text from the video: Hi and welcome to module four of Aws Academy Machine learning in this module, we're going to look at forecasting. We'll start with an introduction to forecasting and look at how time series data is different from other kinds of data. Then we're going to look at Amazon forecast a service that helps you simplify building forecasts. At the end of this module, you'll be able to describe the business problem solved with Amazon forecast. Describe the challenges of working with time series data list the steps required to create a forecast by using Amazon forecast and use Amazon forecast to make a prediction. See you in the next video.\n",
      "All the key phrases from the video: ['Aws Academy Machine learning', 'this module', 'we', 'forecasting', 'We', 'an introduction', 'forecasting', 'time series data', 'other kinds', 'data', 'we', 'Amazon', 'a service', 'that', 'you', 'forecasts', 'the end', 'this module', 'you', 'the business problem', 'Amazon forecast', 'the challenges', 'time series data', 'the steps', 'a forecast', 'Amazon forecast', 'Amazon forecast', 'a prediction', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_28\n",
      "All the text from the video: Hi and welcome to section one. We'll get started by reviewing what forecasting is and some use cases for it. Forecasting is an important area of machine learning. It's important because there are so many opportunities for predicting future outcomes based on historical data. Many of these opportunities involve a time component. However, while the time component adds additional information, it also makes time series problems more difficult to handle compared to other types of predictions. You can think of time series data as falling into two broad categories. The first type is Univ variate data which means there's just one variable. The second one is Multivariate data which means there's more than one variable. There are several common patterns in time series data. The first pattern is a trend with a trend. You get a pattern with the values increasing, decreasing or staying the same over time. There are seasonal patterns, these reflect times of the year, month, day or other patterns. Cyclical patterns are similar to seasonal patterns. These are patterns that repeat like a large retail sale event that happens the same time each year. Finally, there are changes in the data over time that appear to be random or that have no discernible pattern. There are many uses for forecasting. You can use forecasting in marketing applications such as for sales forecasting or demand projections. It could also be used in inventory management systems that anticipate required inventory levels. Forecasting. Energy consumption can help predict when and where energy is needed. And weather forecasting systems can be used for governments and commercial applications such as agriculture. That's it for this section. See you in the next video.\n",
      "All the key phrases from the video: ['Hi and welcome', 'section', 'We', 'what forecasting', 'some', 'cases', 'it', 'Forecasting', 'an important area', 'machine learning', 'It', 'so many opportunities', 'future outcomes', 'historical data', 'these opportunities', 'a time component', 'the time component', 'additional information', 'it', 'time series problems', 'other types', 'predictions', 'You', 'time series data', 'two broad categories', 'The first type', 'Univ variate data', 'which', 'just one variable', 'The second one', 'Multivariate data', 'which', 'more than one variable', 'several common patterns', 'time series data', 'The first pattern', 'a trend', 'a trend', 'You', 'a pattern', 'the values', 'time', 'seasonal patterns', 'these reflect times', 'the year', 'month', 'day', 'other patterns', 'Cyclical patterns', 'seasonal patterns', 'These', 'patterns', 'that', 'a large retail sale event', 'that', 'changes', 'the data', 'time', 'that', 'that', 'no discernible pattern', 'many uses', 'forecasting', 'You', 'forecasting', 'marketing applications', 'sales forecasting or demand projections', 'It', 'inventory management systems', 'that', 'required inventory levels', 'Forecasting', 'Energy consumption', 'energy', 'weather forecasting systems', 'governments', 'commercial applications', 'agriculture', 'That', 'it', 'this section', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_29\n",
      "All the text from the video: Hi and welcome back. This is section two and we're going to focus on processing time series data because it can be different from other types of data you've been using so far. Time series data is data that is captured in chronological sequence over a defined period of time. Introducing time into a machine learning model has a positive impact because the model can derive meaning from changes in the data points over time time series data tends to be correlated. This means that there's a dependency between data points. This has mixed results for forecasting. This is because you're dealing with the regression problem and regression assumes that data points are independent. You need to develop a method for dealing with data dependence. So you can increase the validity of the predictions. In addition to the time series data, you can add related data to augment a forecasting model. For example, suppose you want to make a prediction about retail sales. You could include information about the product being sold such as item identification or sales price along with the number of units sold per time period. The third type of data is metadata about the data set for instance, say that you have a retail data set, you might want to include metadata like a brand name or a genre for music or videos. So you can group results. It's better to have more data. When you work with multiple data sources, you'll face the challenge of handling the time stamp of the data. You'll observe differences in the time stamp format and other challenges such as incomplete data. However, you might be able to infer missing data in some cases. For example, say you have some data that contains both the month and the day, but no year observe whether the data seems to sequence through the month numbers in the database repeating after 12 if it does. So you could add the year. If you knew when the data started, you could then infer future years based on the order of the data. Much time stamp data is stored in U T C format but not all data is, you should check if the time stamp is in local or universal time. Sometimes the time stamp doesn't represent the time you think it does. For example, suppose you have a database of cars that were serviced at a garage. Does the time stamp indicate the time the car arrived was completed or picked up? Or does it indicate when the final entry was entered into the system? Say you're trying to model the hourly caloric intake of patients. However, you only have daily data then you'll need to adjust your target time scale. Also, your data might not have any time stamps. There could be other ways to extrapolate a time series depending on the data and domain. For example, you might have wavelength measurements or vectors within an image. As a final note, remember that daylight savings is different around the world. Also because of daylight savings time might even occur twice a year in their time zones. A common occurrence in real world forecasting problems is missing values in the raw data. Missing values makes it harder for a model to generate a forecast. The primary example in retail is an out of stock situation in demand forecasting. If an item goes out of stock, the sales for the day will zero. If the forecast is generated based on those zero sales values, the forecast will be incorrect. There are many reasons why values can be marked as missing, missing values can occur because of no transaction. They can also occur because of possible measurement errors. For example, a service that monitored certain data wasn't working correctly. Or as another example, the measurement couldn't happen correctly in retail. The primary example for an inability to take correct measurements is an out of stock situation in demand forecasting. This means that demand doesn't equal sales on that day. There are several ways you can calculate the missing data. The first method is forward fill. This uses the last known value for the missing value. Building on that idea, moving average uses the average of the last known values to calculate the missing value backward fill uses the next known value after the missing value. The danger here is that you're using the future to calculate the past, which is bad in forecasting. This method is also known as look ahead and should be avoided. Interpolation uses an equation to calculate the missing value. You can also use a zero fill. This is often used in retail because missing sales data shouldn't be calculated. The missing data represents that there were no orders on that day. It would be wise to investigate why this happened. But in this case, you don't want to fill in the missing value. You might get data at different frequencies. For example, you might have sales data that includes the exact time stamp, the sale was recorded but have inventory data that only contains the year, month and day of the inventory level. When you have data that's at a different frequency than other data sets or data that's not compatible with your question. You might need to down sample. Down sampling is moving from a more finely grained time to a less finely grained time. As the example shows this could be converting an hourly data set to a daily data set. When down sampling, you need to decide how to combine the values. In the previous case of sales data. Summing the quantity makes the most sense. If the data is temperature. You might want to find the average understanding your data helps you decide what's the best course of action. The opposite of down sampling is up sampling. When you move from a less finely grained time to a more finely grained time. The problem with up sampling is that it's extremely difficult to achieve. In most cases, suppose you want to up sample your sales data from daily sales to hourly sales. Unless you have some other data source to reference, you wouldn't be able to do this. There are cases when you need to do something perhaps to match the frequency of another time series or you might have an irregular time series or specific domain knowledge that would help in those cases, you need to be careful how you make the conversion for the retail example. The best you could do is create a single order for the day at a specified hour. For temperature. You could copy the daily temperature into each hourly slot or use a formula to calculate a curve in data science. Outliers have a mix of positive and negative attributes. The same is true of time series data. Suppose you were examining sales data and you had an order that has an unusually high number of items. You might not want to include that in your forecast calculations because the order size might never be repeated, removing these outliers and anomalies is known as smoothing, smoothing your data can help you deal with outliers and other anomalies. There are a few reasons why you might consider smoothing first during data preparation, you remove error values and could also remove outliers. You might also want to smooth your data to generate features. For visualization. You could smooth your data to reduce the noise in a plot. It's important to understand why you are smoothing the data and the impact that it might have. The outcome might be to reduce noise and create a better model. But an equally important question is, could your smoothing compromise the model is the model expecting noisy data? Will you also be able to smooth the data in production? That's it. For part one of this section, we'll see you again for part two where we'll review more time series specific challenges and the tools and algorithms that can help us wrangle your data.\n",
      "All the key phrases from the video: ['This', 'section', 'we', 'processing time series data', 'it', 'other types', 'data', 'you', 'Time series data', 'data', 'that', 'chronological sequence', 'a defined period', 'time', 'time', 'a machine learning model', 'a positive impact', 'the model', 'meaning', 'changes', 'the data points', 'time time series data', 'This', 'a dependency', 'data points', 'This', 'mixed results', 'forecasting', 'This', 'you', 'the regression problem', 'regression', 'data points', 'You', 'a method', 'data dependence', 'you', 'the validity', 'the predictions', 'addition', 'the time series data', 'you', 'related data', 'a forecasting model', 'example', 'you', 'a prediction', 'retail sales', 'You', 'information', 'the product', 'item identification', 'sales price', 'the number', 'units', 'time period', 'The third type', 'data', 'the data', 'instance', 'you', 'a retail data', 'you', 'metadata', 'a brand name', 'a genre', 'music', 'videos', 'you', 'results', 'It', 'more data', 'you', 'multiple data sources', 'you', 'the challenge', 'the time stamp', 'the data', 'You', 'differences', 'the time stamp format', 'other challenges', 'incomplete data', 'you', 'missing data', 'some cases', 'example', 'you', 'some data', 'that', 'both the month', 'the day', 'no year', 'the data', 'the month numbers', 'the database', 'it', 'you', 'the year', 'you', 'the data', 'you', 'future years', 'the order', 'the data', 'stamp data', 'U T C format', 'not all data', 'you', 'the time stamp', 'local or universal time', 'stamp', 'the time', 'you', 'it', 'example', 'you', 'a database', 'cars', 'that', 'a garage', 'the time stamp', 'the time', 'the car', 'it', 'the final entry', 'the system', 'you', 'the hourly caloric intake', 'patients', 'you', 'daily data', 'you', 'your target time scale', 'your data', 'any time stamps', 'other ways', 'a time series', 'the data', 'domain', 'example', 'you', 'measurements', 'vectors', 'an image', 'a final note', 'daylight savings', 'the world', 'daylight savings time', 'their time zones', 'A common occurrence', 'real world forecasting problems', 'values', 'the raw data', 'Missing values', 'it', 'a model', 'a forecast', 'The primary example', 'retail', 'an out', 'stock situation', 'demand forecasting', 'an item', 'stock', 'the sales', 'the day', 'the forecast', 'those zero sales values', 'the forecast', 'many reasons', 'values', 'missing values', 'no transaction', 'They', 'possible measurement errors', 'example', 'a service', 'that', 'certain data', 'another example', 'the measurement', 'retail', 'The primary example', 'an inability', 'correct measurements', 'an out', 'stock situation', 'demand forecasting', 'This', 'demand', 'sales', 'that day', 'several ways', 'you', 'the missing data', 'The first method', 'fill', 'This', 'the last known value', 'the missing value', 'that idea', 'the average', 'the last known values', 'the missing value backward fill', 'the next known value', 'the missing value', 'The danger', 'you', 'the future', 'the past', 'which', 'forecasting', 'This method', 'Interpolation', 'an equation', 'the missing value', 'You', 'a zero fill', 'This', 'retail', 'missing sales data', 'The missing data', 'no orders', 'that day', 'It', 'this', 'this case', 'you', 'the missing value', 'You', 'data', 'different frequencies', 'example', 'you', 'sales data', 'that', 'the exact time stamp', 'the sale', 'inventory data', 'that', 'the year', 'month', 'day', 'the inventory level', 'you', 'data', 'that', 'a different frequency', 'other data sets', 'data', 'that', 'your question', 'You', 'sampling', 'a more finely grained time', 'a less finely grained time', 'the example', 'this', 'an hourly data', 'a daily data set', 'you', 'the values', 'the previous case', 'sales data', 'the quantity', 'the most sense', 'the data', 'temperature', 'You', 'your data', 'you', 'what', 'the best course', 'action', 'The opposite', 'down sampling', 'you', 'a less finely grained time', 'a more finely grained time', 'The problem', 'it', 'most cases', 'you', 'your sales data', 'daily sales', 'hourly sales', 'you', 'some other data source', 'you', 'this', 'cases', 'you', 'something', 'the frequency', 'another time series', 'you', 'an irregular time series', 'specific domain knowledge', 'that', 'those cases', 'you', 'you', 'the conversion', 'the retail example', 'you', 'a single order', 'the day', 'a specified hour', 'temperature', 'You', 'the daily temperature', 'each hourly slot', 'a formula', 'a curve', 'data science', 'Outliers', 'a mix', 'positive and negative attributes', 'time series data', 'you', 'sales data', 'you', 'an order', 'that', 'an unusually high number', 'items', 'You', 'that', 'your forecast calculations', 'the order size', 'these outliers', 'anomalies', 'smoothing', 'your data', 'you', 'outliers', 'other anomalies', 'a few reasons', 'you', 'data preparation', 'you', 'error values', 'outliers', 'You', 'your data', 'features', 'visualization', 'You', 'your data', 'the noise', 'a plot', 'It', 'you', 'the data', 'the impact', 'that', 'it', 'The outcome', 'noise', 'a better model', 'an equally important question', 'your smoothing compromise', 'the model', 'the model', 'noisy data', 'you', 'the data', 'production', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'more time series specific challenges', 'the tools', 'algorithms', 'that', 'us', 'your data']\n",
      "Video: demo1-job_name_1.1_30\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring wrangling time series data seasonality in data is any kind of repeating observation where the frequency of the observation is stable. For example, in sales, you typically see higher sales at the end of a quarter and into the fourth quarter, consumer retail sees even higher sales in the fourth quarter. Be aware that data can have multiple types of seasonality in the same data set. There are many times when you should incorporate seasonality information into your forecast. For instance, localized holidays are a good example for sales. The chart shows that the total revenue generated by arcades has a strong correlation with the number of computer science doctorates awarded in the US. But correlations do not mean causation. If you disagree, see the source for the chart, there are many other correlations plotted on the site and none of them make any sense with your own data. Be careful that you're not seeing and acting on correlations that don't have meaning in the real world. Here's an experiment. If you generate two random time series data sets of numbers between zero and one, you'll find that they have a very low correlation But if you introduce the same slope to both datasets, you'll see a very strong correlation. You need to know how stable a system is. The level of stability or stationery can inform how much you should expect the system's past behavior. To inform future behavior. A system with low stability won't be successful at predicting the future. You'll often want to determine the trend for a time series. But if you adjust the series for the trend, it can be difficult to compare it with another series that was also adjusted for the trend. This is because the trends might dominate the values in the series. This could then lead to overestimates and correlation between the two series like we discussed previously auto correlation is one of the special problems you face with time series data as you've seen in other machine learning problems. The goal of building an ML model is to make sure you're separating the signal from the noise. Auto correlation is a form of noise because separate observations aren't independent of each other. A time series with auto correlation might overstate the accuracy of the model that's produced some of the algorithms you'll look at in this module can help correct for auto correlation. These factors along with seasonality will influence the model. You'll select to produce your forecast. Some algorithms handle seasonality and auto correlation but others do not. Penda was developed with financial data analysis in mind as such, it's good at handling time series data. First, you can set the index for your penda data frame to be a date time. You can then use date and time to select your data. You can use ranges that contain partial dates. You can also extract date parts such as year, month, weekday name and more for grouping and re sampling tasks. Pentas has built in functions to do both. Finally, pandas can give you insights into auto correlation for more information about pandas in the time series. Refer to the pandas documentation. One of the tasks in building a forecasting application is to choose an appropriate algorithm. Your choice of algorithm should be determined by the type of data set you're using. And the features of that data set. Amazon forecast supports these five algorithms but there are others. Each algorithm can handle data with slightly different characteristics. For example, take autoregressive integrated moving average, which is also known as Arima. It removes auto correlations that could influence the pattern of observations or take exponential smoothing, which is also known as ETS. This algorithm is useful for data sets with seasonality. You can find out more about these algorithms in the Amazon forecast documentation. Some key takeaways from this section of the module include time series data is sequence data that includes a time element which makes it different from regular data sets. Some of the time challenges include dealing with different time formats, handling missing data through down sampling up sampling and smoothing, dealing with seasonality such as weekdays and yearly cycles, avoiding bad correlations. PEDIS has excellent time series support with functions for dealing with time. There are five algorithms used by Amazon forecast arema deep A R plus ETS N PTS and profit. That's it for this section. We'll see you in the next video.\n",
      "All the key phrases from the video: ['We', 'wrangling time series data seasonality', 'data', 'any kind', 'observation', 'the frequency', 'the observation', 'example', 'sales', 'you', 'higher sales', 'the end', 'a quarter', 'the fourth quarter', 'consumer retail', 'even higher sales', 'the fourth quarter', 'data', 'multiple types', 'seasonality', 'the same data set', 'many times', 'you', 'seasonality information', 'your forecast', 'instance', 'localized holidays', 'a good example', 'sales', 'The chart', 'the total revenue', 'arcades', 'a strong correlation', 'the number', 'computer science doctorates', 'the US', 'correlations', 'causation', 'you', 'the source', 'the chart', 'many other correlations', 'the site', 'none', 'them', 'any sense', 'your own data', 'you', 'correlations', 'that', 'meaning', 'the real world', 'an experiment', 'you', 'two random time series data sets', 'numbers', 'you', 'they', 'a very low correlation', 'you', 'the same slope', 'both datasets', 'you', 'a very strong correlation', 'You', 'a system', 'The level', 'stability', 'stationery', 'you', \"the system's past behavior\", 'future behavior', 'A system', 'low stability', 'the future', 'You', 'the trend', 'a time series', 'you', 'the series', 'the trend', 'it', 'it', 'another series', 'that', 'the trend', 'This', 'the trends', 'the values', 'the series', 'This', 'overestimates', 'correlation', 'the two series', 'we', 'auto correlation', 'the special problems', 'you', 'time series data', 'you', 'other machine learning problems', 'The goal', 'an ML model', 'you', 'the signal', 'the noise', 'Auto correlation', 'a form', 'noise', 'separate observations', 'A time series', 'auto correlation', 'the accuracy', 'the model', 'that', 'some', 'the algorithms', 'you', 'this module', 'auto correlation', 'These factors', 'seasonality', 'the model', 'You', 'your forecast', 'Some algorithms', 'seasonality and auto correlation', 'others', 'Penda', 'financial data analysis', 'mind', 'it', 'time series data', 'you', 'the index', 'your penda data frame', 'a date time', 'You', 'date', 'time', 'your data', 'You', 'ranges', 'that', 'partial dates', 'You', 'date parts', 'year', 'month', 'weekday name', 'sampling tasks', 'Pentas', 'functions', 'both', 'pandas', 'you', 'insights', 'auto correlation', 'more information', 'pandas', 'the time series', 'the pandas documentation', 'the tasks', 'a forecasting application', 'an appropriate algorithm', 'Your choice', 'algorithm', 'the type', 'data', 'you', 'And the features', 'Amazon forecast', 'these five algorithms', 'others', 'Each algorithm', 'data', 'slightly different characteristics', 'example', 'autoregressive integrated moving average', 'which', 'Arima', 'It', 'auto correlations', 'that', 'the pattern', 'observations', 'exponential smoothing', 'which', 'ETS', 'This algorithm', 'data sets', 'seasonality', 'You', 'these algorithms', 'the Amazon forecast documentation', 'Some key takeaways', 'this section', 'the module', 'time series data', 'sequence data', 'that', 'a time element', 'which', 'it', 'regular data sets', 'Some', 'the time challenges', 'different time formats', 'missing data', 'smoothing', 'seasonality', 'weekdays', 'yearly cycles', 'bad correlations', 'PEDIS', 'excellent time series support', 'functions', 'time', 'five algorithms', 'Amazon', 'deep A R plus ETS N PTS', 'profit', 'That', 'it', 'this section', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_31\n",
      "All the text from the video: Hi and welcome back in this section. We'll look at how you can use Amazon forecast to create a predictor and generate forecasts. When you generate forecasts, you can apply the machine learning development pipeline you've seen throughout this course, but you still need data. You need to import as much data as you have both historical data and related data. You'll want to do some basic evaluation and feature engineering before you use the data to train a model. So you can meet the requirements of Amazon forecast to train a predictor. You need to choose an algorithm if you're not sure which algorithm is the best for your data. Amazon forecast can choose for you to do this select auto M L as your algorithm. You also need to select a domain for your data. If you're not sure what the best fit is, you can also select a custom domain domains have specific types of data they require. When you have a trained model, you can then use the model to make a forecast, using an input data set group. After you've generated a forecast, you can query the forecast. You can also export it to a bucket in Amazon S3. And finally, you can encrypt the data in the forecast before exporting it. The overall process for working with Amazon forecast is to import historical and related data. Amazon forecast inspects the data identifies key data and selects an appropriate algorithm. It uses the algorithm to train and optimize a custom model and produce a predictor. You create forecasts by applying the predictor to your data set. You can then retrieve these forecasts in the Aws management console or you can export the forecasts as comma delimited files. You can also use an API and AWS C commands to create and retrieve forecasts. When you work with Amazon forecast, you can select the domain you're working in. There are domains ranging from retail to web traffic and there's also a custom option for everything else. By selecting a domain, you improve the efficiency of the predictor. Each domain has specific types of data that you'll supply. When you build the predictor. For example, the retail domain expects data for the item identifiers. A time stamp for the observation, the number of sales for that item and the specified time stamp. Here's an example of the data you'd need to provide for a retail demand forecast for the time series. You need the time when the transaction took place. Ideally in U T C format, the item id of the item and how many items were sold. The metadata for the item might include the category, the item color and other attributes. The link back to the time series data will be only the item id because item metadata typically doesn't change related data for creating a more useful forecast could include the sales price or other promotion data. To link this back to the item. You must include the time stamp and the item id. Here's an example of the data you'd need to provide for a web traffic forecast for the time series. You need the web page id, the number of page views per month and the time stamp related data for creating a more useful forecast could include the page category such as navigation or content category. You'll also need the geographic identifier for the web client for metadata. You might also need to provide the region and the sales promotion information. Amazon forecast predictors use an algorithm to train a model. They then use the model to make a forecast using an input data set group to help you get started. Amazon forecast provides predefined algorithms arema deep A R plus E T SNP T S and profit. You can also use the auto ML feature. It will try all the algorithms to see which ones at the best at predicting data. When you prepare data for training and machine learning, you typically hold back data to use when you validate and score the model. The data that you hold back is usually a random sample of your available data with time series data. You must process your data differently because of a correlation between time when you import your data. Amazon forecast breaks it into training and test data sets which the diagram shows the training data is used to train the model which is then tested against the data that was held back. You can specify multiple back test windows which will split the data multiple times, train the model and use metrics to determine which model gives the best results. The default back test window is one you can change how Amazon forecasts splits the data by setting the back test window offset parameter. When you create the predictor, if you don't set this value, the algorithms use default values after you've trained a model, you will need to measure its accuracy which you'll learn about next. The first Amazon forecast evaluation metric is the weighted quantile loss or w quantile loss. When Amazon forecast creates a forecast, it provides probabilistic predictions at three distinct quantiles, 10% 50% and 90%. These prediction quantiles show you how much uncertainty is associated with each forecast. A P 10 quantile predicts that 10% of the time the true value will be less than the predicted value. For example, suppose that you are a retailer, you want to forecast product demand for winter gloves that sell well only during the fall and winter say that you don't have sufficient storage space and the cost of invested capital is high or that the price of being overstocked on winter gloves concerns. You, then you might use the P 10 quantile to order a relatively low number of winter gloves. You know that the P 10 forecast overestimates the demand for your winter gloves only 10% of the time. So you'll be sold out of your winter gloves for 90% of the time. A P 50 quantile predicts that 50% of the time the true value will be less than the predicted value. Continuing the winter gloves example, say, you know that there will be a moderate amount of demand for the gloves and you aren't concerned about being overstocked. Then you might choose to use the P 50 quantile to order gloves. A P 90 quantile predicts that 90% of the time the true value will be less than the predicted value. Suppose you determine that being under stocked on gloves will result in large amounts of lost revenue. For example, the cost of not selling gloves is extremely high or the cost of invested capital is low. In this case, you might choose to use the P 90 quantile to order gloves. Amazon forecast also calculates the associated loss or error at each quantile weighted quantile loss. Calculates how far off the forecast. A certain quantile is from actual demand in either direction. Lower W quantile loss metrics mean that the model's forecasts are more reliable. The root mean square error or R MS E is another method for evaluating the reliability of your forecasts like W quantile loss RMS. E calculates how far off the forecasted values were from the actual test data. The R MS E finds the difference between the actual target value in the data set and the forecasted value for that time period. And it then squares the differences. The example shows how to calculate R M E. The R MS E value represents the standard deviation of the prediction errors. This test is good for forecast validity. When the errors are mostly of the same size that is there, there aren't many outliers lower R MS E metrics indicate that the model's forecasts are more reliable. Here's an example of how a web retailer might use the accuracy metrics to evaluate a forecast. The retailer wants to predict the demand for sales of a particular brand of shoes. They input the sales records for this brand into Amazon forecast to create a predictor. The predictor provides a forecasted demand of 1000 pairs with the P 10 P 50 and P 90 values shown. The weighted quantile loss values indicate that 10% of the time there will be fewer than 880 pairs sold, 50% of the time, fewer than 1050 pairs will be sold and 90% of the time fewer than 1200 pairs will be sold. The retailer can then use these values to determine which level of inventory to hold. They can base their decision on their assessment of the risk that they won't be able to fulfill orders or that they'll have excess inventory. Some key takeaways from this section of the module include, you can use Amazon forecast to train and use a model for time series data. There are specific schemas defined for domains such as retail and EC2 capacity planning or you can use a custom schema. You need to supply at least the time series data but can also provide metadata and related data to add more information to the model. As with most supervised machine learning problems, your data is split into training and testing data but takes into account the time element use R MS E and W quantile loss metrics to evaluate the efficiency of the model. That's it for this video. We'll see you in the next one.\n",
      "All the key phrases from the video: ['this section', 'We', 'you', 'Amazon forecast', 'a predictor', 'forecasts', 'you', 'forecasts', 'you', 'the machine', 'development pipeline', 'you', 'this course', 'you', 'data', 'You', 'as much data', 'you', 'both historical data', 'related data', 'You', 'some basic evaluation', 'feature engineering', 'you', 'the data', 'a model', 'you', 'the requirements', 'Amazon', 'a predictor', 'You', 'an algorithm', 'you', 'algorithm', 'your data', 'Amazon forecast', 'you', 'this select auto M L', 'your algorithm', 'You', 'a domain', 'your data', 'you', 'what', 'the best fit', 'you', 'a custom domain domains', 'specific types', 'data', 'they', 'you', 'a trained model', 'you', 'the model', 'a forecast', 'an input data set group', 'you', 'a forecast', 'you', 'the forecast', 'You', 'it', 'a bucket', 'Amazon S3', 'you', 'the data', 'the forecast', 'it', 'The overall process', 'Amazon forecast', 'historical and related data', 'Amazon forecast', 'the data', 'key data', 'an appropriate algorithm', 'It', 'the algorithm', 'a custom model', 'a predictor', 'You', 'forecasts', 'the predictor', 'You', 'these forecasts', 'the Aws management console', 'you', 'the forecasts', 'comma delimited files', 'You', 'an API and AWS C commands', 'forecasts', 'you', 'Amazon forecast', 'you', 'the domain', 'you', 'domains', 'retail', 'web traffic', 'a custom option', 'everything', 'a domain', 'you', 'the efficiency', 'the predictor', 'Each domain', 'specific types', 'data', 'that', 'you', 'you', 'the predictor', 'example', 'the retail domain', 'data', 'the item identifiers', 'A time', 'the observation', 'the number', 'sales', 'that item', 'the specified time stamp', 'an example', 'the data', 'you', 'a retail demand forecast', 'the time series', 'You', 'the time', 'the transaction', 'place', 'U T C format', 'id', 'the item', 'how many items', 'The metadata', 'the item', 'the category', 'the item color', 'other attributes', 'The link', 'the time series data', 'only the item', 'i', 'item metadata', 'related data', 'a more useful forecast', 'the sales price', 'other promotion data', 'this', 'the item', 'You', 'the time stamp', 'the item', 'i', 'an example', 'the data', 'you', 'a web traffic forecast', 'the time series', 'You', 'the web page', 'id', 'the number', 'page views', 'month', 'the time', 'stamp related data', 'a more useful forecast', 'the page category', 'navigation or content category', 'You', 'the geographic identifier', 'the web client', 'metadata', 'You', 'the region', 'the sales promotion information', 'Amazon', 'an algorithm', 'a model', 'They', 'the model', 'a forecast', 'an input data set group', 'you', 'Amazon forecast', 'predefined algorithms', 'deep A R plus E T SNP T S', 'profit', 'You', 'the auto ML feature', 'It', 'all the algorithms', 'data', 'you', 'data', 'training', 'machine learning', 'you', 'data', 'you', 'the model', 'The data', 'that', 'you', 'a random sample', 'your available data', 'time series data', 'You', 'your data', 'a correlation', 'time', 'you', 'your data', 'Amazon forecast', 'it', 'training and test data sets', 'which', 'the diagram', 'the training data', 'the model', 'which', 'the data', 'that', 'You', 'multiple back test windows', 'which', 'the data', 'the model', 'metrics', 'which model', 'the best results', 'The default back test window', 'you', 'the data', 'the back test window offset parameter', 'you', 'the predictor', 'you', 'this value', 'the algorithms', 'default values', 'you', 'a model', 'you', 'its accuracy', 'which', 'you', 'The first Amazon forecast evaluation metric', 'the weighted quantile loss', 'w quantile loss', 'Amazon forecast', 'a forecast', 'it', 'probabilistic predictions', 'three distinct quantiles', '10%', '50%', '90%', 'These prediction quantiles', 'you', 'how much uncertainty', 'each forecast', 'A P 10 quantile predicts', '10%', 'the time', 'the true value', 'the predicted value', 'example', 'you', 'a retailer', 'you', 'product demand', 'winter gloves', 'that', 'the fall', 'winter', 'you', 'sufficient storage space', 'the cost', 'invested capital', 'winter gloves concerns', 'You', 'you', 'the P', 'a relatively low number', 'winter gloves', 'You', 'the P 10 forecast', 'the demand', 'your winter gloves', 'only 10%', 'the time', 'you', 'your winter gloves', '90%', 'the time', 'A P 50 quantile predicts', '50%', 'the time', 'the true value', 'the predicted value', 'you', 'a moderate amount', 'demand', 'the gloves', 'you', 'you', 'the P', '50 quantile', 'gloves', 'A P 90 quantile predicts', '90%', 'the time', 'the true value', 'the predicted value', 'you', 'gloves', 'large amounts', 'lost revenue', 'example', 'the cost', 'gloves', 'the cost', 'invested capital', 'this case', 'you', 'the P', '90 quantile', 'gloves', 'Amazon forecast', 'the associated loss', 'error', 'weighted quantile loss', 'the forecast', 'A certain quantile', 'actual demand', 'either direction', 'Lower W quantile loss metrics', \"the model's forecasts\", 'The root mean square error', 'R MS E', 'another method', 'the reliability', 'your forecasts', 'W quantile loss RMS', 'E', 'the forecasted values', 'the actual test data', 'The R MS E', 'the difference', 'the actual target value', 'the data', 'the forecasted value', 'that time period', 'it', 'the differences', 'The example', 'R M E.', 'The R MS E value', 'the standard deviation', 'the prediction errors', 'This test', 'forecast validity', 'the errors', 'the same size', 'that', 'many outliers', 'R MS E metrics', \"the model's forecasts\", 'an example', 'a web retailer', 'the accuracy metrics', 'a forecast', 'The retailer', 'the demand', 'sales', 'a particular brand', 'shoes', 'They', 'the sales records', 'this brand', 'Amazon forecast', 'a predictor', 'The predictor', 'a forecasted demand', '1000 pairs', 'the P 10 P 50 and P 90 values', 'The weighted quantile loss values', '10%', 'the time', 'fewer than 880 pairs', '50%', 'the time', 'fewer than 1050 pairs', '90%', 'the time', 'fewer than 1200 pairs', 'The retailer', 'these values', 'which level', 'inventory', 'They', 'their decision', 'their assessment', 'the risk', 'they', 'orders', 'they', 'excess inventory', 'Some key takeaways', 'this section', 'the module', 'you', 'Amazon forecast', 'a model', 'time series data', 'specific schemas', 'domains', 'retail', 'EC2 capacity planning', 'you', 'a custom schema', 'You', 'at least the time series data', 'metadata', 'related data', 'more information', 'the model', 'most supervised machine learning problems', 'your data', 'training and testing data', 'account', 'the time element use R MS E and W quantile loss metrics', 'the efficiency', 'the model', 'That', 'it', 'this video', 'We', 'you']\n",
      "Video: demo1-job_name_1.1_32\n",
      "All the text from the video: Hi, welcome back. It's now time to review the module and wrap it up in this module. You learned how to describe the business problem solved by Amazon forecast. Describe the challenges of working with time series data list. The steps required to create forecast by using Amazon forecast and use Amazon forecast to make a prediction. Thanks for participating. See you in the next module.\n",
      "All the key phrases from the video: ['It', 'time', 'the module', 'it', 'this module', 'You', 'the business problem', 'Amazon forecast', 'the challenges', 'time series data list', 'The steps', 'forecast', 'Amazon forecast', 'Amazon forecast', 'a prediction', 'Thanks', 'you', 'the next module']\n",
      "Video: demo1-job_name_1.1_33\n",
      "All the text from the video: Welcome back to Aws Academy Machine Learning. This is module five and we have a great topic for you today. Computer vision in this module, we'll start with an overview of the computer vision space and you'll learn about some of the use cases and terminology. Next, we'll explore details about analyzing image and video with managed services from Amazon web services or Aws. Finally, we'll look at how you can use your own customized data sets for performing object detection. At the end of this module, you'll be able to describe the use cases for computer vision. Describe the Amazon managed machine learning services available for image and video analysis list the steps required to prepare a custom data set for object detection. Describe how Amazon sagemaker ground truth can be used to prepare a custom data set and finally use Amazon recognition to perform facial detection. Thanks for watching. We'll see you in the next video.\n",
      "All the key phrases from the video: ['Aws Academy Machine Learning', 'This', 'module', 'we', 'a great topic', 'you', 'this module', 'we', 'an overview', 'the computer vision space', 'you', 'some', 'the use cases', 'terminology', 'we', 'details', 'image', 'video', 'managed services', 'Amazon web services', 'Aws', 'we', 'you', 'your own customized data sets', 'object detection', 'the end', 'this module', 'you', 'the use cases', 'computer vision', 'the Amazon', 'image and video analysis list', 'the steps', 'a custom data', 'object detection', 'Amazon sagemaker ground truth', 'a custom data', 'Amazon recognition', 'facial detection', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_34\n",
      "All the text from the video: Hi, welcome back. This is Section one and we're going to introduce computer vision. Computer vision is an exciting space in machine learning. You can think of computer vision as the automated extraction of information from digital images. Using computer vision machines can identify people places and things in images with an accuracy that's at or above human levels and with greater speed and efficiency. Computer vision is often built with deep learning models. It automates the extraction analysis, classification and understanding of useful information from a single image or a sequence of images. The image data can take many forms such as single images, video sequences, views from multiple cameras or three dimensional data. Computing power and algorithms have advanced over the last 10 years. This has led to an increase in capabilities and easier access to computer vision technologies. So how is computer vision being used? Here are some of the primary use cases for computer vision. You can use image and facial recognition to improve public safety and home security or as a way to authenticate access to personal devices, you can also use it to automatically classify images for content management and analysis. Autonomous driving is partly enabled by computer vision technologies. And so are safety features of cars such as lane detection or collision avoidance. Medical image analysis with computer vision can improve the accuracy and speed of a patient's medical diagnosis. This can result in better treatment outcomes and life expectancy for the patient. And finally, in manufacturing, well trained computer vision is incorporated into robotics. This can improve quality assurance and operational efficiencies. These are just a few examples and you can probably think of more computer vision problems can be broken down into a few areas. Content recognition is about identifying things in images. It's a classification problem but it's a complex one with several layers in the picture here. What's represented is it breakfast, lunch or dinner, would the classification only be food? The answer depends on what model you use to perform. The classification models must be trained. And the training data provides the algorithm with data for it to learn from. Say that you have a model that was trained with pictures of different types of food. You might expect the image to output categories such as milk, peaches, mashed potato, chicken, nuggets, and salad. If you trained the model with different images, it could classify objects as tray, cutlery and napkin. Instead, when you work with images, you might want to know what kinds of objects are in the image and the location of those objects. Object detection provides the image categories and where the objects are located in the image, there's a set of coordinates defining the location of a box surrounding the image, which is known as the bounding box. Bounding boxes. For detection are typically top left width and height coordinates surrounding the images. You can use these coordinates in your applications. When objects are detected in an image, there's a confidence number usually associated with that object. This percentage indicates the probability that the object belongs to a specific class. This confidence level is important when you want to determine an action that's based on object detection, especially in facial detection applications or cases where the action has significance. Object segmentation is also known as semantic segmentation. It's like object detection, but you go into more detail to get fine boundaries for each detected object. Basically, it's a fine grained inference for predicting each pixel in the image. Some applications that require object segmentation include autonomous vehicles and advanced computer human interactions. Though object segmentation is a key problem in the field of computer vision, we won't be covering it in this course. Video adds another dimension to computer vision with video you get more data to work with. So you can capture the movement of people or objects which are referred to as instances. For example, you can detect people who enter and leave frames and also deal with moving cameras. Here's a use case for computer vision building on detection and tracking. You can analyze shopper behavior in your retail store by studying the path each person follows. If you use face analysis, you can understand other details about shoppers such as average age ranges, gender distribution and expressed emotions without identifying them. Here's another computer vision use case, you can also analyze images to identify actions using the motion in the video, for example, activities such as delivering a package or dancing, looking at this image of a baseball player. Some examples from the image could include capturing the batter's accuracy. The pitcher's pitching style, the type of pitch, slow ball slider and others, the inning and the batter's performance versus the specific pitcher managers could use all that data to coach players on how to improve their performance. And they do coaches can also use the data during the game to make game time decisions. Say you want to initiate various actions based on the speed of the baseball leaving the bat and its trajectory a hit that's calculated by an M L model could lead to an audio or visual warning about a possible foul ball into the crowd or it could result in a preemptive alarm that a hit has a high probability of being a home run. This means that events following a home run could be both well timed and automated such as playing music or setting off fireworks when the home run is hit by the home team to wrap up. This section, here are some key takeaways for this section. First, we covered how computer vision is the automated extraction of information from images. You can divide computer vision into two distinct areas. Image analysis and video analysis. Image analysis includes object classification detection and segmentation. Video analysis includes instance, tracking action recognition and motion estimation. Thanks for watching. We'll see you in the next video.\n",
      "All the key phrases from the video: ['This', 'Section', 'we', 'computer vision', 'Computer vision', 'an exciting space', 'machine learning', 'You', 'computer vision', 'the automated extraction', 'information', 'digital images', 'computer vision machines', 'people places', 'things', 'images', 'an accuracy', 'that', 'human levels', 'greater speed', 'efficiency', 'Computer vision', 'deep learning models', 'It', 'the extraction analysis', 'classification', 'understanding', 'useful information', 'a single image', 'a sequence', 'images', 'The image data', 'many forms', 'single images', 'video sequences', 'views', 'multiple cameras', 'three dimensional data', 'Computing power', 'algorithms', 'the last 10 years', 'This', 'an increase', 'capabilities', 'easier access', 'computer vision technologies', 'computer vision', 'some', 'the primary use cases', 'computer vision', 'You', 'image', 'facial recognition', 'public safety', 'home security', 'a way', 'access', 'personal devices', 'you', 'it', 'images', 'content management', 'analysis', 'Autonomous driving', 'computer vision technologies', 'safety features', 'cars', 'lane detection', 'collision avoidance', 'Medical image analysis', 'computer vision', 'the accuracy', 'speed', \"a patient's medical diagnosis\", 'This', 'better treatment outcomes', 'life expectancy', 'the patient', 'manufacturing', 'well trained computer vision', 'robotics', 'This', 'quality assurance', 'operational efficiencies', 'These', 'just a few examples', 'you', 'more computer vision problems', 'a few areas', 'Content recognition', 'things', 'images', 'It', 'a classification problem', 'it', 'a complex one', 'several layers', 'the picture', 'What', 'it', 'breakfast', 'lunch', 'dinner', 'the classification', 'food', 'The answer', 'what model', 'you', 'The classification models', 'the training data', 'the algorithm', 'data', 'it', 'you', 'a model', 'that', 'pictures', 'different types', 'food', 'You', 'the image', 'output categories', 'milk', 'peaches', 'mashed potato', 'chicken', 'nuggets', 'salad', 'you', 'the model', 'different images', 'it', 'objects', 'cutlery', 'napkin', 'you', 'images', 'you', 'what kinds', 'objects', 'the image', 'those objects', 'Object detection', 'the image categories', 'the objects', 'the image', 'a set', 'coordinates', 'the location', 'a box', 'the image', 'which', 'the bounding box', 'Bounding boxes', 'detection', 'top left width and height coordinates', 'the images', 'You', 'these coordinates', 'your applications', 'objects', 'an image', 'a confidence number', 'that object', 'This percentage', 'the probability', 'the object', 'a specific class', 'This confidence level', 'you', 'an action', 'that', 'object detection', 'facial detection applications', 'cases', 'the action', 'significance', 'Object segmentation', 'semantic segmentation', 'It', 'object detection', 'you', 'more detail', 'fine boundaries', 'each detected object', 'it', 'a fine grained inference', 'each pixel', 'the image', 'Some applications', 'that', 'object segmentation', 'autonomous vehicles', 'advanced computer human interactions', 'object segmentation', 'a key problem', 'the field', 'computer vision', 'we', 'it', 'this course', 'Video', 'another dimension', 'computer vision', 'video', 'you', 'more data', 'you', 'the movement', 'people', 'objects', 'which', 'instances', 'example', 'you', 'people', 'who', 'frames', 'moving cameras', 'a use case', 'detection', 'tracking', 'You', 'shopper behavior', 'your retail store', 'the path', 'each person', 'you', 'face analysis', 'you', 'other details', 'shoppers', 'average age ranges', 'gender distribution', 'emotions', 'them', 'another computer vision use case', 'you', 'images', 'actions', 'the motion', 'the video', 'example', 'activities', 'a package', 'dancing', 'this image', 'a baseball player', 'Some examples', 'the image', \"the batter's accuracy\", \"The pitcher's pitching style\", 'the type', 'pitch', 'slow ball slider', 'others', 'the inning', \"the batter's performance\", 'the specific pitcher managers', 'all that data', 'coach players', 'their performance', 'they', 'coaches', 'the data', 'the game', 'game time decisions', 'you', 'various actions', 'the speed', 'the baseball', 'the bat', 'its trajectory', 'a hit', 'that', 'an M L model', 'an audio or visual warning', 'a possible foul ball', 'the crowd', 'it', 'a preemptive alarm', 'a hit', 'a high probability', 'a home run', 'This', 'events', 'a home run', 'music', 'fireworks', 'the home run', 'the home team', 'This section', 'some key takeaways', 'this section', 'we', 'computer vision', 'the automated extraction', 'information', 'images', 'You', 'computer vision', 'two distinct areas', 'Image analysis', 'video analysis', 'Image analysis', 'object classification detection', 'segmentation', 'Video analysis', 'instance', 'action recognition', 'motion estimation', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_35\n",
      "All the text from the video: Welcome back in this section, we'll explore image analysis in more detail. And in part two, we'll take a closer look into video analysis to start. We'll introduce the main Amazon service. We'll be using Amazon recognition. Amazon recognition is a computer vision service that's based on deep learning. You can use it to add image and video analysis to your applications. There are many uses for Amazon recognition including creating searchable image and video libraries. Amazon recognition makes both images and stored videos searchable so you can discover the objects and scenes that appear in them. You can use Amazon recognition to build a face based user verification system. So your applications can confirm user identities by comparing their live image with a reference image. Amazon recognition interprets emotional expressions such as happy, sad or surprise. It can also interpret demographic information from facial images such as gender. Amazon recognition can also detect inappropriate content in both images and stored videos. And finally, Amazon recognition can recognize and extract text content from images before we go further. Here's a quick note on security. You need to check if the applications you build using Amazon recognition fall under any regulatory restrictions as defined in your field or country security and compliance. For Amazon Recognition is a shared responsibility between Aws and the customer. For more information about this topic, see the Aws compliance page, Amazon Recognition is an Aws managed service with a managed service. Amazon hosts the machine learning models, maintains an API and scales out to meet demand for you. You can benefit from a set of models that constantly learn and improve. Also, you can focus on building applications that use the API and optionally training the service to understand your unique business needs. There are various resources you can use to access and interact with Amazon recognition such as API S S D K S and commands for the AWS command line interface, which is also known as the AWS cli. The languages supported by the S D K s include javascript, Python P H P dot net, Ruby, Java go no J S and C++. Finally, Amazon recognition integrates with other Aws services. For example, if you need storage, you can use Amazon's simple storage service or S3 for authentication and authorization. You can use Aws identity and access management, which is also known as I AM. This diagram illustrates an image search feature where users can take pictures and get information about the real estate properties they're viewing first. The user takes a picture with their mobile device. The user then initiates a search which causes the application to upload to Amazon S3 S3 is configured to call other services. When a write event occurs. In this case, the bucket passes the S3 path of the new object to Aws LAMBDA. When the LAMBDA function is called, it uses the Amazon recognition SDK to call the service. Amazon recognition analyzes the image detects aspects of the property creates labels and passes the information back to LAMBDA as an object formatted in javascript object notation or json LAMBDA, then stores the labels and confidence score in Amazon elastic search service, which is also known as Amazon E S application. Users can now identify aspects of a property using the objects that were detected in the image. In this example architecture, the system checks uploaded images for inappropriate content. Like the previous example, processing begins when the user uploads content. First, the user uploads an image to Amazon S3. Second, the S3 bucket is configured to call a Lambda function. When an object is written to the bucket. Third, lambda calls Amazon recognition via the S D K Amazon recognition, then analyzes the images for inappropriate content and sends the response back to LAMBDA. Fourth. If the content is appropriate, the content is approved. Fifth. If the content isn't appropriate, the content can be sent for manual inspection. And finally, if the content isn't approved, a notification is sent to the user. In this final use case, the system analyzes a video feed for sentiment analysis. First, an in store camera captures video that's then sent to a back office or a cloud based application. Typically an application like this uses Amazon kinesis to stream the video. Second, the application uses the S D K to send the video to Amazon recognition. For further analysis, visual sentiment is extracted along with other attributes such as age. Third, the discovered attributes are sent to Amazon kinesis. Fourth, a lambda function extracts the data from the stream. Fifth, the data is then written to S3. Next, the data is loaded into Amazon redshift on a regular basis. And finally, tools like Amazon Quicks site can be used to generate reports from the data. Amazon recognition is designed to integrate into your applications through the API and S D K S API operations are provided for detecting labels, faces, recognizing celebrities and detecting unsafe images to perform a prediction. Provide the service with an image object in Amazon S3 or upload a byte stream of an image images can be in JPEG or P and G formats. Amazon recognition processes. The image performs the prediction and returns a JSON object with the results. When Amazon recognition performs predictions, it often returns multiple labels. Each label has a confidence level. This confidence level indicates how likely the label was found in the image. Like this example shows labels can also have hierarchies. When you find instances of objects you need to understand where the detected object is in the image. For each instance, the results from Amazon recognition include a bounding box that contains the starting coordinate of top left and box dimensions of width height. Like the example, you can use this information to determine the location of the detected object in the image. It's important to note that all findings contain a confidence score. You can use the confidence score in your applications to tune your response to predictions with a higher score. It's more likely that the object was correctly labeled. That's it. For part one of this section, we'll see you again for part two where we'll explore facial detection.\n",
      "All the key phrases from the video: ['this section', 'we', 'image analysis', 'more detail', 'part', 'we', 'a closer look', 'video analysis', 'We', 'the main Amazon service', 'We', 'Amazon recognition', 'Amazon recognition', 'a computer vision service', 'that', 'deep learning', 'You', 'it', 'image', 'video analysis', 'your applications', 'many uses', 'Amazon recognition', 'searchable image', 'video libraries', 'Amazon recognition', 'both images', 'stored videos', 'you', 'the objects', 'scenes', 'that', 'them', 'You', 'Amazon recognition', 'a face based user verification system', 'your applications', 'user identities', 'their live image', 'a reference image', 'Amazon recognition', 'emotional expressions', 'It', 'demographic information', 'facial images', 'gender', 'Amazon recognition', 'inappropriate content', 'both images', 'stored videos', 'Amazon recognition', 'text content', 'images', 'we', 'a quick note', 'security', 'You', 'the applications', 'you', 'Amazon recognition', 'any regulatory restrictions', 'your field or country security', 'compliance', 'Amazon Recognition', 'a shared responsibility', 'Aws', 'the customer', 'more information', 'this topic', 'the Aws compliance page', 'Amazon Recognition', 'an Aws managed service', 'a managed service', 'Amazon', 'the machine learning models', 'an API', 'demand', 'you', 'You', 'a set', 'models', 'that', 'you', 'applications', 'that', 'the API', 'the service', 'your unique business needs', 'various resources', 'you', 'Amazon recognition', 'API S S D K S', 'commands', 'the AWS command line interface', 'which', 'the AWS cli', 'The languages', 'the S D K s', 'javascript', 'Python P H P dot net', 'Ruby', 'Java', 'no J S', 'C++', 'Amazon recognition', 'other Aws services', 'example', 'you', 'storage', 'you', \"Amazon's simple storage service\", 'S3', 'authentication', 'authorization', 'You', 'Aws identity and access management', 'which', 'I', 'This diagram', 'an image search feature', 'users', 'pictures', 'information', 'the real estate properties', 'they', 'The user', 'a picture', 'their mobile device', 'The user', 'a search', 'which', 'the application', 'Amazon', 'S3 S3', 'other services', 'a write event', 'this case', 'the bucket', 'the S3 path', 'the new object', 'Aws LAMBDA', 'the LAMBDA function', 'it', 'the Amazon recognition', 'SDK', 'the service', 'Amazon recognition', 'the image', 'aspects', 'the property', 'labels', 'the information', 'LAMBDA', 'an object', 'javascript object notation', 'json LAMBDA', 'the labels', 'confidence score', 'Amazon elastic search service', 'which', 'Amazon E S application', 'Users', 'aspects', 'a property', 'the objects', 'that', 'the image', 'this example architecture', 'the system', 'uploaded images', 'inappropriate content', 'the previous example', 'processing', 'the user', 'content', 'the user', 'an image', 'Amazon S3', 'the S3 bucket', 'a Lambda function', 'an object', 'the bucket', 'lambda', 'Amazon recognition', 'the S D K Amazon recognition', 'the images', 'inappropriate content', 'the response', 'LAMBDA', 'the content', 'the content', 'the content', 'the content', 'manual inspection', 'the content', 'a notification', 'the user', 'this final use case', 'the system', 'a video feed', 'sentiment analysis', 'store', 'that', 'a back office', 'a cloud based application', 'an application', 'this', 'Amazon kinesis', 'the video', 'the application', 'the S D K', 'the video', 'Amazon recognition', 'further analysis', 'visual sentiment', 'other attributes', 'age', 'the discovered attributes', 'Amazon kinesis', 'a lambda function', 'the data', 'the stream', 'the data', 'S3', 'the data', 'Amazon redshift', 'a regular basis', 'tools', 'Amazon Quicks site', 'reports', 'the data', 'Amazon recognition', 'your applications', 'the API and S D K S API operations', 'labels', 'celebrities', 'unsafe images', 'a prediction', 'the service', 'an image object', 'Amazon S3', 'a byte stream', 'an image images', 'JPEG', 'P', 'G', 'formats', 'Amazon recognition processes', 'The image', 'the prediction', 'a JSON object', 'the results', 'Amazon recognition', 'predictions', 'it', 'multiple labels', 'Each label', 'a confidence level', 'This confidence level', 'the label', 'the image', 'this example', 'labels', 'hierarchies', 'you', 'instances', 'objects', 'you', 'the detected object', 'the image', 'each instance', 'the results', 'Amazon recognition', 'a bounding box', 'that', 'the starting coordinate', 'top left and box dimensions', 'width height', 'the example', 'you', 'this information', 'the location', 'the detected object', 'the image', 'It', 'all findings', 'a confidence score', 'You', 'the confidence score', 'your applications', 'your response', 'predictions', 'a higher score', 'It', 'the object', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'facial detection']\n",
      "Video: demo1-job_name_1.1_36\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring image analysis with a closer look at facial detection. Facial detection uses a model that was tuned to perform predictions specifically for detecting faces and facial features. Facial detection has many of the same features as standard object detection such as a bounding box or the coordinates of the box surrounding the face that was detected. This will include a value representing the confidence that the bounding box contains a face. There will be a list of attributes if found such as if the face has a beard or if it appears to be male or female, there will also be a confidence score for these attributes. It can also detect physical emotions like whether the person is smiling or frowning. It's important to understand this classification is based only on visual clues. And so it might not represent the actual emotion of the person. Facial landmarks are components of the face such as eyes and mouth. Typical landmarks also include X and Y coordinates quality describes the brightness and the sharpness of the face and pose describes the rotation of the face inside the image. Again, confidence is a feature here and it's provided for each detected feature. And remember the feature prediction is based only on visual observations with Amazon recognition. You can compare two images to determine if they contain the same person. Comparisons require both a source and a target image. The results will include all the faces that were found and they include information about matching and non matching faces. Again, confidence scores indicate how likely each prediction is. Amazon recognition can also search for known faces to use this feature. You need to train the model by providing a collection of images to use. After you train the model, you can then detect those people and images you provide to find known faces. First create a collection and add faces to the collection. Amazon recognition will perform facial recognition on the images you provide, it will then return typical information like the bounding box coordinates or the confidence score to associate faces with an image specify an image ID in the external image ID request parameter. This could be the file name of the image or another ID that you create. After you create your collection, you can then use the search faces by image operation to search for faces from the collection. The return data contains an array of all faces that matched the information includes bounding boxes, confidence scores and the external image ID value. You can then use this ID value to link back to the source image. Now that you've learned about the facial detection features of Amazon recognition. Here's a summary of the guidelines we've discussed so far. When Amazon recognition detects a human face, it captures a bounding box that shows where the face was found in the video. It can also detect attributes such as the position of the eyes, nose and mouth. It can detect emotion, the quality of the detection and any landmarks that might appear. All these items will have an associated confidence score. A higher score means that the model has greater confidence about the detection gender is inferred from the image, not inferred from identity. Similarly, emotion is also determined from the image and it might not reflect the subject's actual emotional state. How should I apply facial recognition responsibly? Facial recognition should never be used in a way that violates an individual's rights including the right to privacy. It should also never be used to make autonomous decisions for scenarios that require a human being to analyze them. For example, suppose that a bank uses tools like Amazon recognition in a financial application to verify their customers identities. The bank should always clearly disclose the use of the technology and ask the customer to approve their terms and conditions. For more information about this topic, see the Aws web page about the facts on facial recognition with artificial intelligence will now explain how you can use Amazon recognition to process videos. You can perform video processing on both stored videos and video streams, stored videos should be uploaded and stored in an S3 bucket. Each type of detection has its own start operation. You can search for people, faces, labels, celebrities, text and inappropriate content. Amazon recognition publishes a completion status to a topic in Amazon's simple notification service which is also known as Amazon S N S. Then S N S can route these messages to subscribers for durability. It's a best practice to route messages to a message queue. In Amazon's simple QE service or Amazon S Q S, your application should monitor the SQS Q for completion. Each start operation has a corresponding get operation for retrieving the results. If you call get detection results, it returns an array of labels that contain information about any labels found in the video. The label information includes the same labels as image detection, but it also includes a time stamp of where the label was detected in milliseconds from the start of the video. In addition to stored videos, you can also use Amazon recognition video to detect and recognize faces in streaming video. A typical use case for this is detecting a known face in a video stream. Amazon recognition video uses Amazon kinesis video streams to receive and process a video stream. The analysis results are output from Amazon recognition video to a kinesis data stream. They are then read by your client application. Amazon recognition video provides a stream processor that's called create stream processor and you can use it to start and manage the analysis of the streaming video to use Amazon recognition video with your streaming video, your application must implement these resources. First, you need a kinesis video stream to send streaming video to Amazon recognition video. Next, you need an Amazon recognition video stream processor to manage the streaming video analysis. And finally, you need a kinesis data stream consumer to read the analysis results that Amazon recognition video sends to the data stream. If you want to find a face in a video, you need to create a collection. This process is the same as creating a collection for still images. Amazon recognition video places a JSON frame record for each analyzed frame into the kinesis output stream. Amazon recognition video doesn't analyze every frame that's passed to it through the kinesis video stream. A frame record that's sent to a kinesis data stream contains information about which video stream fragment the frame is in where the frame is in the fragment and faces that are recognized in the frame. It also includes status information for the stream processor before we wrap up. Here's a quick summary. Amazon recognition is a computer vision service that's based on deep learning. You can easily add image and video analysis to your applications. Amazon recognition can detect faces sentiment, text unsafe content and library search in both images and video. Amazon recognition is integrated with other Aws services. Thanks for watching. We'll see you in the next video\n",
      "All the key phrases from the video: ['We', 'image analysis', 'a closer look', 'facial detection', 'Facial detection', 'a model', 'that', 'predictions', 'faces', 'facial features', 'Facial detection', 'the same features', 'standard object detection', 'a bounding box', 'the coordinates', 'the box', 'the face', 'that', 'This', 'a value', 'the confidence', 'the bounding box', 'a face', 'a list', 'attributes', 'the face', 'a beard', 'it', 'a confidence score', 'these attributes', 'It', 'physical emotions', 'the person', 'It', 'this classification', 'visual clues', 'it', 'the actual emotion', 'the person', 'Facial landmarks', 'components', 'the face', 'eyes', 'mouth', 'Typical landmarks', 'X', 'Y', 'quality', 'the brightness', 'the sharpness', 'the face', 'the rotation', 'the face', 'the image', 'confidence', 'a feature', 'it', 'each detected feature', 'the feature prediction', 'visual observations', 'Amazon recognition', 'You', 'two images', 'they', 'the same person', 'Comparisons', 'both a source', 'a target image', 'The results', 'all the faces', 'that', 'they', 'information', 'matching', 'matching', 'faces', 'confidence scores', 'each prediction', 'Amazon recognition', 'known faces', 'this feature', 'You', 'the model', 'a collection', 'images', 'you', 'the model', 'you', 'those people', 'images', 'you', 'known faces', 'a collection', 'faces', 'the collection', 'Amazon recognition', 'facial recognition', 'the images', 'you', 'it', 'typical information', 'the bounding box coordinates', 'the confidence score', 'faces', 'an image ID', 'the external image ID request parameter', 'This', 'the file name', 'the image', 'another ID', 'that', 'you', 'you', 'your collection', 'you', 'the search faces', 'image operation', 'faces', 'the collection', 'The return data', 'an array', 'all faces', 'that', 'the information', 'bounding boxes', 'confidence scores', 'the external image ID value', 'You', 'this ID value', 'the source image', 'you', 'the facial detection features', 'Amazon recognition', 'a summary', 'the guidelines', 'we', 'Amazon recognition', 'a human face', 'it', 'a bounding box', 'that', 'the face', 'the video', 'It', 'attributes', 'the position', 'the eyes', 'nose', 'mouth', 'It', 'emotion', 'the quality', 'the detection', 'any landmarks', 'that', 'All these items', 'an associated confidence score', 'A higher score', 'the model', 'greater confidence', 'the detection gender', 'the image', 'identity', 'emotion', 'the image', 'it', \"the subject's actual emotional state\", 'I', 'facial recognition', 'Facial recognition', 'a way', 'that', \"an individual's rights\", 'the right', 'privacy', 'It', 'autonomous decisions', 'scenarios', 'that', 'a human being', 'them', 'example', 'a bank', 'tools', 'Amazon recognition', 'a financial application', 'their customers identities', 'The bank', 'the use', 'the technology', 'the customer', 'their terms', 'conditions', 'more information', 'this topic', 'the Aws web page', 'the facts', 'facial recognition', 'artificial intelligence', 'you', 'Amazon recognition', 'videos', 'You', 'video processing', 'both stored videos', 'video streams', 'stored videos', 'an S3 bucket', 'Each type', 'detection', 'its own start operation', 'You', 'people', 'labels', 'celebrities', 'text', 'inappropriate content', 'Amazon recognition', 'a completion status', 'a topic', \"Amazon's simple notification service\", 'which', 'S N S', 'these messages', 'subscribers', 'durability', 'It', 'a best practice', 'messages', 'a message queue', \"Amazon's simple QE service\", 'Amazon S Q S', 'your application', 'the SQS Q', 'completion', 'Each start operation', 'a corresponding get operation', 'the results', 'you', 'detection results', 'it', 'an array', 'labels', 'that', 'information', 'any labels', 'the video', 'The label information', 'the same labels', 'image detection', 'it', 'a time stamp', 'the label', 'milliseconds', 'the start', 'the video', 'addition', 'stored videos', 'you', 'Amazon recognition video', 'faces', 'streaming video', 'A typical use case', 'this', 'a known face', 'a video stream', 'Amazon recognition video', 'Amazon kinesis video streams', 'a video stream', 'The analysis results', 'output', 'Amazon recognition video', 'They', 'your client application', 'Amazon recognition video', 'a stream processor', 'that', 'create stream processor', 'you', 'it', 'the analysis', 'the streaming video', 'Amazon recognition video', 'your streaming video', 'your application', 'these resources', 'you', 'a kinesis video stream', 'streaming video', 'Amazon recognition video', 'you', 'an Amazon recognition video stream processor', 'the streaming video analysis', 'you', 'a kinesis data stream consumer', 'Amazon recognition video', 'the data stream', 'you', 'a face', 'a video', 'you', 'a collection', 'This process', 'a collection', 'still images', 'Amazon recognition video', 'a JSON frame record', 'each analyzed frame', 'the kinesis output stream', 'Amazon recognition video', 'every frame', 'that', 'it', 'the kinesis video stream', 'A frame record', 'that', 'a kinesis data stream', 'information', 'which video stream fragment', 'the frame', 'the frame', 'the fragment', 'faces', 'that', 'the frame', 'It', 'status information', 'the stream processor', 'we', 'a quick summary', 'Amazon recognition', 'a computer vision service', 'that', 'deep learning', 'You', 'image', 'video analysis', 'your applications', 'Amazon recognition', 'faces sentiment', 'unsafe content', 'library search', 'both images', 'video', 'Amazon recognition', 'other Aws services', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_37\n",
      "All the text from the video: In this section, we'll look at preparing custom data sets for computer vision so you can detect custom objects. One challenge of using a prebuilt model is that it will only find images it was trained to find. Though Amazon recognition was trained with tens of millions of images, it can't detect objects that it wasn't trained on. For example, consider the eight of hearts playing card. If you run this card through Amazon recognition, the results show various attributes. However, none of the labels are playing card or eight of hearts. If you want Amazon recognition to detect images in your problem domain, you must train the model with your images. So in this section, you'll learn how to train Amazon recognition with images from your problem domain. Though you'll focus only on using Amazon recognition here, you'll encounter a similar process. If you use other pre trained models, training a computer vision algorithm to recognize images requires a large input data set which isn't practical for most organizations. Many machine learning problems today can be solved by training existing models. Or you can use a managed service like Amazon recognition, custom labels like other machine learning processes. You need to train Amazon recognition. So it recognizes scenes and objects that are in a specific domain. You'll need both a training data set and a test data set that contain labeled images. If you have images that need labels, you can use Amazon recognition custom labels to simplify your labeling tasks. For example, it provides a U I for labeling images which includes a feature you can use to draw bounding boxes around images. It can also help find objects and scenes that are unique to your business needs. You can use it to classify images or detect objects within an image. Say you want to identify specific machine parts and images such as turbo chargers or torque converters. You could collect pictures of each kind of machine part and use them to train your model. Amazon recognition. Custom labels also includes automated machine learning capabilities that handle the machine learning process for you. When you provide training images, the service can automatically load and inspect the data, select the correct machine learning algorithms, train a model and provide model performance metrics. When you finish training your model, you can then evaluate your custom models performance on your test set. Each image in the test set has a side by side comparison of the model's prediction versus the label it assigned. There are also detailed performance metrics for you to review. You can start using your model immediately for image analysis or you can iterate and retrain new versions with more images to refine the model. After you start using your model, you can track your predictions, correct any mistakes and use the feedback data to retrain new model versions and improve their performance. So how do you label images? The diagram shows a typical process for training a computer vision model which includes the Amazon recognition custom labels feature. We'll step through this in some detail. The process of developing a custom model to analyze images requires time, expertise and resources. It often takes months to complete. It can also require thousands or tens of thousands of hand labeled images. So the model has enough data to make accurate decisions. It can take months to generate and gather this data and it can require large teams of labelers to prepare it for use in machine learning. Amazon recognition custom labels builds on the existing capabilities of Amazon recognition which is already trained on tens of millions of images across many categories. Instead of thousands of images, you can upload a small set of training images that are specific to your use case. Typically, you'd use a few 100 images for this. You can use the AWS management console to upload training images. If your images are already labeled Amazon recognition, custom labels can begin training your model. If they're not, you can label the images directly in the labeling interface or you can use Amazon Sagemaker ground Truth to label them for you. There'll be more on that shortly. Amazon recognition custom labels works best when you use different models for different domains. For example, if you need to detect both machine parts and plant health, you'd use two different models. Images you select for training should be similar to the images that will be used for inference use images that use various lighting conditions, backgrounds and resolutions. Ideally, your training images will mirror images. You'd want to perform detection on if you can use the same source like you'd use in production that works best. The documentation includes additional guidelines on image type. So whether they are J pegs or png's and other properties like image size and resolution. That's it. For part one of this section, we'll see you again for part two where we'll review how to create the training data set.\n",
      "All the key phrases from the video: ['this section', 'we', 'custom data sets', 'computer vision', 'you', 'custom objects', 'One challenge', 'a prebuilt model', 'it', 'images', 'it', 'Amazon recognition', 'tens of millions', 'images', 'it', 'objects', 'it', 'example', 'hearts', 'card', 'you', 'this card', 'Amazon recognition', 'the results', 'various attributes', 'none', 'the labels', 'card', 'hearts', 'you', 'Amazon recognition', 'images', 'your problem domain', 'you', 'the model', 'your images', 'this section', 'you', 'Amazon recognition', 'images', 'your problem domain', 'you', 'Amazon recognition', 'you', 'a similar process', 'you', 'other pre trained models', 'a computer vision', 'images', 'a large input data', 'which', 'most organizations', 'Many machine learning problems', 'existing models', 'you', 'a managed service', 'Amazon recognition', 'custom labels', 'other machine learning processes', 'You', 'Amazon recognition', 'it', 'scenes', 'objects', 'that', 'a specific domain', 'You', 'both a training data', 'a test data', 'that', 'labeled images', 'you', 'images', 'that', 'labels', 'you', 'Amazon recognition custom labels', 'your labeling tasks', 'example', 'it', 'a U I', 'labeling images', 'which', 'a feature', 'you', 'bounding boxes', 'images', 'It', 'objects', 'scenes', 'that', 'your business needs', 'You', 'it', 'images', 'objects', 'an image', 'you', 'specific machine parts', 'images', 'turbo chargers', 'torque converters', 'You', 'pictures', 'each kind', 'machine part', 'them', 'your model', 'Amazon recognition', 'Custom labels', 'automated machine learning capabilities', 'that', 'the machine learning process', 'you', 'you', 'training images', 'the service', 'the data', 'the correct machine learning algorithms', 'a model', 'model performance metrics', 'you', 'your model', 'you', 'your custom models performance', 'Each image', 'a side', 'side comparison', \"the model's prediction\", 'the label', 'it', 'detailed performance metrics', 'you', 'You', 'your model', 'image analysis', 'you', 'new versions', 'more images', 'the model', 'you', 'your model', 'you', 'your predictions', 'any mistakes', 'the feedback data', 'new model versions', 'their performance', 'you', 'images', 'The diagram', 'a typical process', 'a computer vision model', 'which', 'the Amazon recognition custom labels', 'We', 'this', 'some detail', 'The process', 'a custom model', 'images', 'time', 'expertise', 'resources', 'It', 'months', 'It', 'thousands', 'tens of thousands', 'hand', 'labeled images', 'the model', 'enough data', 'accurate decisions', 'It', 'months', 'this data', 'it', 'large teams', 'labelers', 'it', 'use', 'machine learning', 'Amazon recognition custom labels', 'the existing capabilities', 'Amazon recognition', 'which', 'tens of millions', 'images', 'many categories', 'images', 'you', 'a small set', 'training images', 'that', 'your use case', 'you', 'a few 100 images', 'this', 'You', 'the AWS management console', 'training images', 'your images', 'Amazon recognition', 'custom labels', 'your model', 'they', 'you', 'the images', 'the labeling interface', 'you', 'Amazon Sagemaker', 'Truth', 'them', 'you', 'that', 'Amazon recognition custom labels', 'you', 'different models', 'different domains', 'example', 'you', 'both machine parts and plant health', 'you', 'two different models', 'Images', 'you', 'training', 'the images', 'that', 'inference use images', 'that', 'various lighting conditions', 'backgrounds', 'resolutions', 'your training images', 'images', 'You', 'detection', 'you', 'the same source', 'you', 'production', 'that', 'The documentation', 'additional guidelines', 'image type', 'they', 'J pegs', 'png', 'other properties', 'image size', 'resolution', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we']\n",
      "Video: demo1-job_name_1.1_38\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the training data set. Data sets contain information that's needed to train and test an Amazon recognition custom labels model such as images, labels and bounding boxes. You can use images from Amazon S3 or you can upload them from your computer to S3 as part of the process to train a model. Your data set should have at least two labels with at least 10 images per label. Each image in your data set must be labeled as we mentioned earlier. You can use the Amazon recognition custom labels console or Amazon Sagemaker Ground Truth to label your images again to train an Amazon recognition custom labels model. Your images must be labeled a label indicates that an image contains an object scene or concept. As we mentioned earlier, a data set needs at least two defined labels. Also, each image must have at least one assigned label that identifies the object scene or concept in the image. When you apply labels to an image as a whole, these labels are known as image level labels. They're useful for identifying scenes or concepts that you want to detect. For example, one of the images shows a beach scene from Ko Olena. It's on the island of Oahu in the US state of Hawaii to train a model to detect beaches, you'd add a beach label that applies to the entire image. You can also apply labels to specific areas of an image that contain an object you want to detect. For example, if you want your model to detect Amazon echo devices, it must identify the different types of echo devices in an image. The model needs information about where the devices are located in the image and it needs a corresponding label that identifies the type of the device. This information is known as localization information. The location of the device is expressed as a bounding box. The example objects with bounding boxes, image shows a bounding box that surrounds an Amazon echo dot The image also contains an Amazon echo without a bounding box. The output of the labeling process will be a manifest file. The manifest file for an image level label typically contains the label or class name along with some metadata about how the image was labeled for object detection. The manifest contains information about each labeled image. The bounding box identifies where the object is in the image along with the label that the bounding box belongs to. We've mentioned Amazon Sagemaker ground truth a few times we now look at what it is and how it might help you with Sagemaker Ground Truth, you can build high quality training data sets for your machine learning models to use it, create a data set that needs labeling. You then provide detailed instructions on what needs to be labeled and submit the job. You can decide who processes the images to create a label data set. You can use workers from the Amazon Mechanical Turk service, a vendor company or an internal workforce with machine learning. You can use the label dataset output from Sagemaker Ground Truth to train your own models or you can also use it with Amazon recognition custom labels. Sagemaker Ground Truth can use active learning to automate the labeling of your input data. Active learning is a machine learning technique that identifies data that should be labeled by your workers in Sagemaker Ground Truth. This functionality is called automated data labeling. Automated data labeling can reduce the time and cost. It takes to label your data set compared to using only human workers. When you use automated labeling, you incur Amazon sagemaker training and inference costs. Yes, we just said that you can use machine learning to label the images that you'll then use for machine learning. We'll talk through how this works. When Sagemaker Ground Truth starts an automated data labeling job, it selects a random sample of input data or objects and sends it to human workers when the label data is returned. Sagemaker ground Truth uses this data which is the validation data to validate the models that were trained for automated data labeling. Sagemaker. Ground truth runs a batch transform job using the validated model for inference on the validation data. Batch inference produces a confidence score and quality metric for each object. In the validation data, automated labeling determines if the confidence score for each object which was produced in step five meets the required threshold, which was determined in step four. If the confidence score meets the threshold, the expected quality of automatic labeling exceeds the requested level of accuracy. The object is then considered to be automatically labeled. Step six produces a data set of unlabeled data with confidence scores. Sagemaker ground truth selects data points with low confidence scores from this data set and sends them to human workers for additional labeling. Sagemaker ground truth then uses the existing human label data and the additional human label data to train a new model. The process is repeated until the data set is fully labeled or until another stopping condition is met. For example, automatic labeling can stop when you meet your budget for human annotation. We recommend using automated data labeling on large data sets. The minimum number of objects allowed for automated data labeling is 1250. However, we strongly suggest providing a minimum of 5000 objects. That's it. For part two of this section, we'll see you again for part three where we'll review how to evaluate and improve your model.\n",
      "All the key phrases from the video: ['We', 'video analysis', 'the training data set', 'Data', 'information', 'that', 'an Amazon recognition custom labels model', 'images', 'labels', 'bounding boxes', 'You', 'images', 'Amazon S3', 'you', 'them', 'your computer', 'S3', 'part', 'the process', 'a model', 'Your data', 'at least two labels', 'at least 10 images', 'label', 'Each image', 'your data', 'set', 'we', 'You', 'the Amazon recognition custom labels console', 'Amazon Sagemaker Ground Truth', 'your images', 'an Amazon recognition custom labels model', 'Your images', 'a label', 'an image', 'an object scene', 'we', 'a data', 'at least two defined labels', 'each image', 'at least one assigned label', 'that', 'the object scene', 'the image', 'you', 'labels', 'an image', 'a whole', 'these labels', 'image level labels', 'They', 'scenes', 'concepts', 'that', 'you', 'example', 'the images', 'a beach scene', 'Ko Olena', 'It', 'the island', 'Oahu', 'the US state', 'Hawaii', 'a model', 'beaches', 'you', 'a beach label', 'that', 'the entire image', 'You', 'labels', 'specific areas', 'an image', 'that', 'an object', 'you', 'example', 'you', 'your model', 'Amazon echo devices', 'it', 'the different types', 'echo devices', 'an image', 'The model', 'information', 'the devices', 'the image', 'it', 'a corresponding label', 'that', 'the type', 'the device', 'This information', 'localization information', 'The location', 'the device', 'a bounding box', 'The example', 'bounding boxes', 'image', 'a bounding box', 'that', 'an Amazon echo dot', 'The image', 'an Amazon echo', 'a bounding box', 'The output', 'the labeling process', 'a manifest file', 'The manifest file', 'an image level label', 'the label or class name', 'some metadata', 'the image', 'object detection', 'The manifest', 'information', 'each labeled image', 'The bounding box identifies', 'the object', 'the image', 'the label', 'that', 'the bounding box', 'We', 'Amazon Sagemaker ground truth', 'we', 'what', 'it', 'it', 'you', 'Sagemaker Ground Truth', 'you', 'high quality training data sets', 'your machine learning models', 'it', 'a data', 'that', 'labeling', 'You', 'detailed instructions', 'what', 'the job', 'You', 'who', 'the images', 'a label data set', 'You', 'workers', 'the Amazon Mechanical Turk service', 'a vendor company', 'an internal workforce', 'machine learning', 'You', 'the label dataset output', 'Sagemaker Ground Truth', 'your own models', 'you', 'it', 'Amazon recognition custom labels', 'Sagemaker Ground Truth', 'active learning', 'the labeling', 'your input data', 'Active learning', 'a machine learning technique', 'that', 'data', 'that', 'your workers', 'Sagemaker Ground Truth', 'This functionality', 'automated data labeling', 'Automated data labeling', 'the time', 'cost', 'It', 'your data', 'only human workers', 'you', 'automated labeling', 'you', 'sagemaker', 'we', 'you', 'machine learning', 'the images', 'you', 'machine learning', 'We', 'this', 'Sagemaker Ground Truth', 'an automated data labeling job', 'it', 'a random sample', 'input data', 'objects', 'it', 'human workers', 'the label data', 'Sagemaker', 'this data', 'which', 'the validation data', 'the models', 'that', 'automated data labeling', 'Sagemaker', 'Ground truth', 'a batch transform job', 'the validated model', 'inference', 'the validation data', 'Batch inference', 'a confidence score', 'quality metric', 'each object', 'the validation data', 'automated labeling', 'if the confidence score', 'each object', 'which', 'step', 'the required threshold', 'which', 'step', 'the confidence score', 'the threshold', 'the expected quality', 'automatic labeling', 'the requested level', 'accuracy', 'The object', 'Step', 'a data', 'unlabeled data', 'confidence scores', 'Sagemaker', 'data points', 'low confidence scores', 'this data', 'them', 'human workers', 'additional labeling', 'Sagemaker ground truth', 'the existing human label data', 'the additional human label data', 'a new model', 'The process', 'the data set', 'another stopping condition', 'example', 'automatic labeling', 'you', 'your budget', 'human annotation', 'We', 'automated data labeling', 'large data sets', 'The minimum number', 'objects', 'automated data labeling', 'we', 'a minimum', '5000 objects', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'your model']\n",
      "Video: demo1-job_name_1.1_39\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the test data set. The final step before you train your model is to identify a test data set. You will use this test data set to validate and evaluate the model's performance. You'll do this by performing an inference on the images in the test data set. You'll then compare the results with the labeling information that's in the training data set. You can create your own test data set. Alternatively, you can use Amazon recognition custom labels to split your training data set into two data sets. By using an 80 20 split. This split means that 80% of the data is used for training and 20% is used for testing. After you define the training and test data sets. Amazon recognition custom labels can automatically train the model for you. The service automatically loads and inspects the data, selects the correct machine learning algorithms trains a model and provides model performance metrics you're charged for the amount of time a model takes to train a data set that contains more images and labels will take longer to train when training is complete. You evaluate the performance of the model. During testing, Amazon recognition, custom labels predicts if a test image contains a custom label. The confidence score is a value that quantifies the certainty of the model's prediction. Because this is a classification problem, the results can be mapped to a confusion matrix with a true positive. The model correctly predicts the presence of the custom label in the test image. That is the predicted label is also a ground truth label for that image. For example, Amazon recognition custom labels correctly returns a cat label. When a cat is present in an image for a false positive, the model incorrectly predicts the presence of a custom label in a test image. That is the predicted label isn't a ground truth label for the image. For example, Amazon recognition custom labels returns a cat label but there's no cat label in the ground truth for that image for a false negative. The model doesn't predict that a custom label is present in the image. But the ground truth for that image includes this label. For example, Amazon recognition custom labels doesn't return a cat custom label for an image that contains a cat with a true negative. The model correctly predicts that a custom label isn't present in the test image. For example, Amazon recognition custom labels doesn't return a cat label for an image that doesn't contain a cat. The console provides access to true positive, false positive and false negative values for each image in your test data set. These prediction results are used to calculate the various metrics for each label and an aggregate of metrics for your entire test set. The same definitions apply to predictions that the model makes at the bounding box level. With bounding boxes, all metrics are calculated over each bounding box in each test image regardless of whether the boxes are prediction or ground truth. To help you, Amazon recognition custom labels provides various metrics. For example, you can view summary metrics and evaluation metrics for each label. It also provides precision metrics for each label and an average precision metric for the entire test data set precision is the proportion of positive results that were correctly classified. Amazon recognition. Custom labels provides average recall metrics for each label and an average recall metric for the entire test data set recall is the fraction of your test set labels that were correctly classified using the previous example of cats. That would be how many cats were correctly classified. The service also provides an average model performance score for each label and an average model performance score for the entire test data set. The F one score combines precision and recall together to give you just one number that quantifies the overall performance of a particular machine learning algorithm. You might use the F one score when you have a class imbalance, but you also want to preserve the equality between precision and sensitivity a higher value means better model performance for both recall and precision. If you're satisfied with the accuracy of your model, you can start using it. That's it. For part three of this section, we'll see you again for part four where we'll review how to evaluate and improve your model.\n",
      "All the key phrases from the video: ['We', 'video analysis', 'the test data', 'The final step', 'you', 'your model', 'a test data', 'You', 'this test data', \"the model's performance\", 'You', 'this', 'an inference', 'the images', 'the test data', 'You', 'the results', 'the labeling information', 'that', 'the training data', 'You', 'you', 'Amazon recognition custom labels', 'your training data', 'two data sets', 'an 80 20 split', 'This split', '80%', 'the data', 'training', '20%', 'testing', 'you', 'the training and test data sets', 'Amazon recognition custom labels', 'the model', 'you', 'The service', 'the data', 'the correct machine learning algorithms', 'a model', 'model performance metrics', 'you', 'the amount', 'time', 'a model', 'a data', 'that', 'more images', 'labels', 'training', 'You', 'the performance', 'the model', 'testing', 'Amazon recognition', 'custom labels', 'a test image', 'a custom label', 'The confidence score', 'a value', 'that', 'the certainty', \"the model's prediction\", 'this', 'a classification problem', 'the results', 'a confusion matrix', 'The model', 'the presence', 'the custom label', 'the test image', 'That', 'the predicted label', 'a ground truth label', 'that image', 'example', 'Amazon recognition custom labels', 'a cat label', 'a cat', 'an image', 'a false positive', 'the model', 'the presence', 'a custom label', 'a test image', 'That', 'the predicted label', 'a ground truth label', 'the image', 'example', 'Amazon recognition custom labels', 'a cat label', 'no cat label', 'the ground truth', 'that image', 'a false negative', 'The model', 'a custom label', 'the image', 'the ground truth', 'that image', 'this label', 'example', 'Amazon recognition custom labels', 'a cat custom label', 'an image', 'that', 'a cat', 'a true negative', 'The model', 'a custom label', 'the test image', 'example', 'Amazon recognition custom labels', 'a cat label', 'an image', 'that', 'a cat', 'The console', 'access', 'true positive, false positive and false negative values', 'each image', 'your test data', 'These prediction results', 'the various metrics', 'each label', 'an aggregate', 'metrics', 'your entire test set', 'The same definitions', 'predictions', 'the model', 'the bounding box level', 'bounding boxes', 'all metrics', 'each bounding box', 'each test image', 'the boxes', 'prediction', 'ground truth', 'you', 'Amazon recognition custom labels', 'various metrics', 'example', 'you', 'summary metrics', 'evaluation metrics', 'each label', 'It', 'precision metrics', 'each label', 'an average precision metric', 'the entire test data', 'set precision', 'the proportion', 'positive results', 'that', 'Amazon recognition', 'Custom labels', 'average recall metrics', 'each label', 'an average recall metric', 'the entire test data', 'recall', 'the fraction', 'your test', 'labels', 'that', 'the previous example', 'cats', 'That', 'how many cats', 'The service', 'an average model performance score', 'each label', 'an average model performance score', 'the entire test data', 'The F one score', 'precision', 'you', 'just one number', 'that', 'the overall performance', 'a particular machine learning algorithm', 'You', 'the F one score', 'you', 'a class imbalance', 'you', 'the equality', 'precision', 'sensitivity', 'a higher value', 'better model performance', 'both recall', 'precision', 'you', 'the accuracy', 'your model', 'you', 'it', 'That', 'it', 'part', 'this section', 'we', 'you', 'part', 'we', 'your model']\n",
      "Video: demo1-job_name_1.1_40\n",
      "All the text from the video: Hi, welcome back. We'll continue exploring video analysis by reviewing how to evaluate and improve your model in general. You can improve the quality of your model with larger quantities of better quality data. Use training images that clearly show the object or seen. And don't include many things that you're not interested in. For bounding boxes around objects, use training images that show the object as fully visible and not hidden by other objects. Make sure that your training and test data sets match the type of images that you'll eventually run inference on for objects where you have just a few training examples like logos, you should provide bounding boxes around the logo in your test images. These images represent the scenarios you want to localize the object in reducing false positives, often results in better precision to reduce false positives. First check if increasing the confidence threshold enables you to keep the correct predictions while eliminating false positives, increasing the confidence threshold eventually results in diminishing gains because of the trade off between precision and recall for a given model. Next check to see if you need to add additional classes for training. For example, if you are detecting cats but often dogs are being flagged as cats add dog as a label to your training data set along with the images of dogs that you got the false positive on effectively. You're helping the model learn to predict dog and not cat through the new training images. You might find that the model is confused between two of your custom labels, cat and dog. The test image with label cat is predicted as having labeled dog and vice versa. In this case, first check from mislabeled images in your training and test sets. Also adding more training images that reflect this confusion will help a retrained model learn to better discriminate between cat and dog. Reducing false negatives, often results in better recall to reduce false negatives. First, lower the confidence threshold. This should improve recall also use better examples to model the variety of both the object and the images they appear in. Finally split your label into two classes that are easier to learn. For example, instead of good cookies and bad cookies, you might want good cookies, burnt cookies and broken cookies to help the model learn each unique concept better. If you're satisfied with the performance of your model, you can make it available for use by starting it from the console or by using code. After the model is running, you can perform an inference with the AWS CLI or the SDK. When you call the API you specify the Amazon resource name of the Amazon recognition custom labels model that you want to use. The Amazon resource name is also known as an A R N. You'll also specify the image you want the model to make a prediction with. You can provide an input image as an image byte array of base 64 encoded image bytes or as an S3 object. Custom labels are returned in an array of custom label objects. Each custom label represents a single object seen or concept that's found in the image. A custom label includes a label for the object scene or concept that was found in the image. It also includes a bounding box for objects that were found in the image. The bounding box coordinates show where the object is located on the source image. The coordinate values are a ratio of the overall image size. Finally, the custom label includes the confidence score. This represents how confident Amazon recognition custom labels is in the accuracy of the label and bounding box. During training, a model calculates a threshold value that determines if a prediction for a label is true. By default, the detect custom labels operation doesn't return labels with a confidence value. That's less than the model's calculated threshold value. To filter the return labels specify a value for min confidence that's greater than the model's calculated threshold. You can get the model's calculated threshold from the model's training results in the Amazon recognition custom labels console to get all the labels regardless of confidence, specify a min confidence value of zero. If you find that the confidence values returned by the detect custom labels operation are too low. Consider retraining the model, you can restrict the number of custom labels that are returned from the detect custom labels operation by specifying the max results, input parameter. The returned results are sorted from the highest confidence to the lowest confidence. Here are some key takeaways from this section of the module models must be trained for the specific domain that you want to analyze. If you're looking for turbocharger, you'll need many pictures of turbocharger to train your model. You can set custom labeling for the specific business case. We looked at the custom labeling process and some of the tools you can use. If you want objects to be detected, you need to label images and create bounding boxes for these objects. You can use Amazon Sagemaker Ground Truth to build training data sets for your models, which can also use machine learning to label your images. Thanks for watching and we'll see you in the next video.\n",
      "All the key phrases from the video: ['We', 'video analysis', 'your model', 'You', 'the quality', 'your model', 'larger quantities', 'better quality data', 'training images', 'that', 'the object', 'many things', 'that', 'you', 'boxes', 'objects', 'training images', 'that', 'the object', 'other objects', 'your training and test data sets', 'the type', 'images', 'you', 'inference', 'objects', 'you', 'just a few training examples', 'logos', 'you', 'bounding boxes', 'the logo', 'your test images', 'These images', 'the scenarios', 'you', 'the object', 'false positives', 'better precision', 'false positives', 'First check', 'the confidence threshold', 'you', 'the correct predictions', 'false positives', 'the confidence threshold', 'gains', 'the trade', 'precision', 'recall', 'a given model', 'Next check', 'you', 'additional classes', 'training', 'example', 'you', 'cats', 'dogs', 'cats', 'dog', 'a label', 'your training data', 'the images', 'dogs', 'that', 'you', 'the false positive', 'You', 'the model', 'dog', 'not cat', 'the new training images', 'You', 'the model', 'your custom labels', 'cat', 'dog', 'The test image', 'label cat', 'dog', 'this case', 'mislabeled images', 'your training and test sets', 'more training images', 'that', 'this confusion', 'a retrained model', 'cat', 'dog', 'false negatives', 'better recall', 'false negatives', 'the confidence threshold', 'This', 'recall', 'better examples', 'the variety', 'both the object', 'the images', 'they', 'your label', 'two classes', 'that', 'example', 'good cookies', 'bad cookies', 'you', 'good cookies', 'burnt cookies', 'broken cookies', 'the model', 'you', 'the performance', 'your model', 'you', 'it', 'use', 'it', 'the console', 'code', 'the model', 'you', 'an inference', 'the AWS CLI', 'the SDK', 'you', 'the API', 'you', 'the Amazon resource name', 'the Amazon recognition custom labels model', 'that', 'you', 'The Amazon resource name', 'an A R N.', 'You', 'the image', 'you', 'the model', 'a prediction', 'You', 'an input image', 'an image byte array', 'base', '64 encoded image bytes', 'an S3 object', 'Custom labels', 'an array', 'custom label objects', 'Each custom label', 'a single object', 'that', 'the image', 'A custom label', 'a label', 'the object scene', 'concept', 'that', 'the image', 'It', 'a bounding box', 'objects', 'that', 'the image', 'The bounding box coordinates', 'the object', 'the source image', 'The coordinate values', 'a ratio', 'the overall image size', 'the custom label', 'the confidence score', 'This', 'Amazon recognition custom labels', 'the accuracy', 'the label', 'bounding box', 'training', 'a model', 'a threshold value', 'that', 'a prediction', 'a label', 'default', 'the detect custom labels operation', 'labels', 'a confidence value', 'That', \"the model's calculated threshold value\", 'the return labels', 'a value', 'min confidence', 'that', \"the model's calculated threshold\", 'You', \"the model's calculated threshold\", \"the model's training results\", 'the Amazon recognition custom labels console', 'all the labels', 'confidence', 'a min confidence value', 'you', 'the confidence values', 'the detect custom labels operation', 'the model', 'you', 'the number', 'custom labels', 'that', 'the detect custom labels operation', 'the max results', 'input parameter', 'The returned results', 'the highest confidence', 'the lowest confidence', 'some key takeaways', 'this section', 'the module models', 'the specific domain', 'that', 'you', 'you', 'turbocharger', 'you', 'many pictures', 'turbocharger', 'your model', 'You', 'custom labeling', 'the specific business case', 'We', 'the custom labeling process', 'some', 'the tools', 'you', 'you', 'objects', 'you', 'images', 'bounding boxes', 'these objects', 'You', 'Amazon Sagemaker Ground Truth', 'training data sets', 'your models', 'which', 'machine learning', 'your images', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_41\n",
      "All the text from the video: It's now time to summarize some of the main points in this module. In this module, you learned how to describe the use cases for computer vision. Describe the Amazon managed machine learning services available for image and video analysis list. The steps required to prepare a custom data set for object detection. Describe how Amazon Sagemaker ground truth can be used to prepare a custom data set and use Amazon recognition to perform facial detection. That concludes this introduction to computer vision. Thanks for watching. We'll see you again in the next video.\n",
      "All the key phrases from the video: ['It', 'time', 'some', 'the main points', 'this module', 'this module', 'you', 'the use cases', 'computer vision', 'the Amazon', 'image and video analysis list', 'The steps', 'a custom data', 'object detection', 'Amazon Sagemaker', 'a custom data', 'Amazon recognition', 'facial detection', 'That', 'this introduction', 'computer vision', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_42\n",
      "All the text from the video: Hi and welcome to module six of Aws Academy Machine learning introduction to natural language processing in this module. We'll introduce natural language processing which is also known as NLP. This section includes a description of the major challenges faced by N LP and the overall development process for N LP applications. We'll then review five aws services you can use to speed up the development of N LP BASED applications. After completing this module, you should be able to describe the N LP use cases that are solved by using managed Amazon M L services and describe the managed Amazon M L services available for N LP. Let's get started.\n",
      "All the key phrases from the video: ['welcome', 'Aws Academy Machine learning introduction', 'natural language processing', 'this module', 'We', 'natural language processing', 'which', 'NLP', 'This section', 'a description', 'the major challenges', 'N LP', 'the overall development process', 'N LP applications', 'We', 'five aws services', 'you', 'the development', 'N LP BASED applications', 'this module', 'you', 'the N LP use cases', 'that', 'managed Amazon M L services', 'the managed Amazon M L services', 'N LP', \"'s\"]\n",
      "Video: demo1-job_name_1.1_43\n",
      "All the text from the video: We'll get started by reviewing what natural language processing means. Natural language processing is also known as N LP. Before we explain what N LP is, we'll consider an example of N LP. Amazon, Alexa, Alexa works by having a device such as an Amazon echo, record your words. The recording of your speech is sent to Amazon servers to be analyzed more efficiently. Amazon breaks down your phrase into individual sounds. Then it connects to a database containing the pronunciation of various words to find which words most closely correspond to the combination of individual sounds. Amazon identifies important words to make sense of the tasks and carry out corresponding functions. For instance, if Alexa notices words like outside or temperature, it will open the weather, Alexa skill. Amazon servers then send the information back to your device. And Alexa speaks N LP is a broad term for a general set of business or computational problems you can solve with machine learning or M L. However, N LP systems predate machine learning. For example, speech to text on older pre smartphone cell phones used N LP and so did screen readers. Many N LP systems now use some form of machine learning. N LP considers the hierarchical structure of language. Words are at the lowest layer in a hierarchy. A group of words make a phrase in the next level up phrases make a sentence and ultimately sentences convey ideas. NLP systems face several significant challenges. We'll look at its challenges. Next language isn't precise words can have different meanings based on the other words that surround them. This is known as context, often the same words or phrases can have multiple meanings. For example, consider the term weather. You could be under the weather which has a colloquial meaning in English that you're sick or you could say there's wonderful weather outside, which means the weather conditions outside are good. The phrase oh really could convey surprise, disagreement and many other things. It depends on the context and inflection. Here are some of the main challenges for NLP. One challenge is discovering the structure of the text. One of the first tasks of any N LP application is to break down the text into meaningful units such as words, phrases and sentences. Another challenge is labeling data. After the system converts the text to data, it must apply labels representing the various parts of speech. Every language will require a different labeling scheme to match the language's grammar. NLP also faces a challenge in representing context because word meaning can depend heavily on context. Any NLP system needs a way to represent it. This is a large challenge because there are many contexts and it's difficult to convert context into a form computers can understand. Finally, although grammar defines a structure for language, the application of grammar is indescribably large in scope handling. The variation in how language is used by humans is a major challenge for N LP systems. That's where machine learning can have a large impact. You can apply NLP to a range of problems. Some of the more common applications include search applications such as Google or Bing human machine interactions like Alexa sentiment analysis for marketing or political campaigns, social research based on media analysis and chat bots to mimic human speech and applications. You can apply the machine learning development pipeline you've seen throughout this course when developing an NLP solution, the first task is to formulate a problem then collect and label data for N LP. Collecting data consists of breaking down the text into meaningful subsets and labeling the sets. Feature engineering is a major part of N LP applications. This process gets complicated when you're dealing with highly irregular or unstructured text. For example, say you're building an application to classify documents, you'd need to be able to distinguish between the words with common terms but different meanings. Labeling data in the N LP domain is sometimes also called tagging. In the labeling process, you assign individual text strings to different parts of speech. There are specialized tools you can use for N LP. Labeling. The first task for an N LP application is to convert the text to data. So it can be analyzed, you convert text by removing words that aren't needed for analysis from the input text. In the example, the words this and is are removed to leave the phrase sample text. After removing stop words, you can normalize text by converting similar words into a common form. For example, the words run, runner, ran and running are all different forms of the word run. You can normalize all instances of these words in a block of text using the stemming and limitation processes, limitation groups, different forms of a word into a single term lemon of the versions of the word run would group all instances of those forms into a single term run. Stemming. On the other hand, removes characters that the stemming algorithm considers unnecessary stemming might not work with the run example as the form ran might not be recognized as a form of the word run after you've normalized the text. You can standardize it by removing words that aren't in the dictionary you're using for analysis. For example, you could remove acronyms slang in special characters. The natural language toolkit is also known as N L T K. Their Python library provides functions for removing stop words and normalizing text. Another first step in creating an NLP system is to convert the text into a data collection such as a data frame. All N LP libraries provide functions to assist with this process. The example shows using the word tokenize function from the N L T K library. After you've cleaned up your text and loaded it into a data frame, you can apply one of the N LP models to create features. Here are a couple of common models. The first model is known as bag of words. This is a simple model for capturing the frequency of words in a document. The model creates a key for each word. The value of the key is the number of times that word occurs in the document. The second model is term frequency and inverse document frequency which is also known as T F ID F term frequency is a count of how many times a word appears in a document. Inverse document frequency is the number of times a word occurs in a group of documents. These two values are used together to calculate a weight for the words words that frequently appear in many documents have a lower weight. There are many established models in the NLP field. The example shows a bag of words model bag of words is a vector model vector models convert each sentence or phrase into a vector which is a mathematical object that records both directionality and magnitude. In the example, a simple sentence is converted into a vector where the frequency of each word is recorded. The word is has a value of two because it appears twice in the sentence bag of words is often used to classify documents into different categories. It's also used to derive attributes that feed into N LP applications such as in sentiment analysis. There are three broad categories of text analysis. First, the classification of text is similar to other classification systems you've seen in this course, text provides the input to a process that extracts features. Then you send the features through a machine learning algorithm that interacts with a classifier model and infers the classification. There are many applications for text matching. For example, autocorrect spelling and grammar checking are based on text matching. The algorithm for edit distance. Also known as the Levenstein distance is frequently used. You can derive relationships between different words or phrases in the text using a process called co reference resolution. Several NLP systems provide Python libraries for deriving relationships. One of the biggest challenges for N LP is how to describe the context for the text. Consider this example where a user is searching for the term tablet because the word tablet has at least two distinct meanings. The search engine needs to know which meaning the user has in mind. Most search engines rely on the most commonly used context if the term isn't qualified further. For example, by adding another term like medicine or computing to the search. The process of extracting entities is known as named entity recognition or N er and any R model has the following functions. First, it can identify noun phrases using dependency charts and part of speech tagging, it can classify phrases using a classification algorithm such as word to VC. Finally, it can disambiguate entities using a knowledge graph. Here's an example of using any er to extract the entity's Titanic and North Atlantic from the text. After the named entities are extracted, you can use a knowledge graph to extract meaning a knowledge graph combines subject matter expertise with machine learning to drive. Meaning the Amazon Recommendations engine is an example of a knowledge graph. Here are the main points to remember from this section. First N LP predates machine learning. You can use the same ML workflow that you've seen in other modules for N LP. Some of the main use cases for NLP are search query analysis, human machine interaction and marketing and social research. N LP is complicated because human language lacks precision. Thanks for watching, we'll see you in the next video.\n",
      "All the key phrases from the video: ['We', 'what', 'Natural language processing', 'N LP', 'we', 'what', 'N LP', 'we', 'an example', 'N LP', 'Amazon', 'Alexa', 'a device', 'an Amazon echo', 'your words', 'The recording', 'your speech', 'Amazon servers', 'Amazon', 'your phrase', 'individual sounds', 'it', 'a database', 'the pronunciation', 'various words', 'which words', 'the combination', 'individual sounds', 'Amazon', 'important words', 'sense', 'the tasks', 'corresponding functions', 'instance', 'Alexa', 'words', 'outside', 'temperature', 'it', 'the weather', 'Alexa skill', 'Amazon servers', 'the information', 'your device', 'Alexa', 'N LP', 'a broad term', 'a general set', 'business or computational problems', 'you', 'machine learning', 'However, N LP systems predate machine learning', 'example', 'speech', 'older pre smartphone cell phones', 'N LP', 'screen readers', 'Many N LP systems', 'some form', 'machine learning', 'N LP', 'the hierarchical structure', 'language', 'Words', 'the lowest layer', 'a hierarchy', 'A group', 'words', 'a phrase', 'the next level', 'phrases', 'a sentence', 'ultimately sentences', 'ideas', 'NLP systems', 'several significant challenges', 'We', 'its challenges', 'Next language', 'precise words', 'different meanings', 'the other words', 'that', 'them', 'This', 'context', 'often the same words', 'phrases', 'multiple meanings', 'example', 'the term weather', 'You', 'the weather', 'which', 'a colloquial meaning', 'English', 'you', 'you', 'wonderful weather', 'which', 'the weather conditions', 'The phrase', 'surprise', 'disagreement', 'many other things', 'It', 'the context', 'inflection', 'some', 'the main challenges', 'NLP', 'One challenge', 'the structure', 'the text', 'the first tasks', 'any N LP application', 'the text', 'meaningful units', 'words', 'phrases', 'sentences', 'Another challenge', 'data', 'the system', 'the text', 'data', 'it', 'labels', 'the various parts', 'speech', 'Every language', 'a different labeling scheme', \"the language's grammar\", 'NLP', 'a challenge', 'context', 'word', 'context', 'Any NLP system', 'a way', 'it', 'This', 'a large challenge', 'many contexts', 'it', 'context', 'a form', 'computers', 'grammar', 'a structure', 'language', 'the application', 'grammar', 'scope handling', 'The variation', 'language', 'humans', 'a major challenge', 'N LP systems', 'That', 'machine learning', 'a large impact', 'You', 'NLP', 'a range', 'problems', 'Some', 'the more common applications', 'search applications', 'Google or Bing human machine interactions', 'Alexa sentiment analysis', 'marketing', 'political campaigns', 'social research', 'media analysis', 'bots', 'human speech', 'applications', 'You', 'the machine', 'development pipeline', 'you', 'this course', 'an NLP solution', 'the first task', 'a problem', 'N LP', 'data', 'the text', 'meaningful subsets', 'the sets', 'Feature engineering', 'a major part', 'N LP applications', 'This process', 'you', 'highly irregular or unstructured text', 'example', 'you', 'an application', 'documents', 'you', 'the words', 'common terms', 'different meanings', 'Labeling data', 'the N LP domain', 'the labeling process', 'you', 'individual text strings', 'different parts', 'speech', 'specialized tools', 'you', 'N LP', 'Labeling', 'The first task', 'an N LP application', 'the text', 'data', 'it', 'you', 'text', 'words', 'that', 'analysis', 'the input text', 'the example', 'the words', 'this', 'the phrase', 'sample text', 'stop words', 'you', 'text', 'similar words', 'a common form', 'example', 'the words', 'runner', 'running', 'different forms', 'You', 'all instances', 'these words', 'a block', 'text', 'the stemming and limitation processes', 'limitation groups', 'different forms', 'a word', 'a single term lemon', 'the versions', 'the word run', 'all instances', 'those forms', 'a single term run', 'the other hand', 'characters', 'the stemming algorithm', 'unnecessary stemming', 'the run example', 'the form', 'a form', 'the word', 'you', 'the text', 'You', 'it', 'words', 'that', 'the dictionary', 'you', 'analysis', 'example', 'you', 'acronyms slang', 'special characters', 'The natural language toolkit', 'N L T K.', 'Their Python library', 'functions', 'stop words', 'normalizing text', 'Another first step', 'an NLP system', 'the text', 'a data collection', 'a data frame', 'All N LP libraries', 'functions', 'this process', 'The example', 'the word', 'function', 'the N L T K library', 'you', 'your text', 'it', 'a data frame', 'you', 'the N LP models', 'features', 'a couple', 'common models', 'The first model', 'bag', 'words', 'This', 'a simple model', 'the frequency', 'words', 'a document', 'The model', 'a key', 'each word', 'The value', 'the key', 'the number', 'times', 'that word', 'the document', 'The second model', 'term frequency', 'inverse document frequency', 'which', 'T F ID F term frequency', 'a count', 'a word', 'a document', 'Inverse document frequency', 'the number', 'times', 'a word', 'a group', 'documents', 'These two values', 'a weight', 'the words', 'words', 'that', 'many documents', 'a lower weight', 'many established models', 'the NLP field', 'The example', 'a bag', 'words model bag', 'words', 'a vector model vector models', 'each sentence', 'phrase', 'a vector', 'which', 'a mathematical object', 'that', 'both directionality', 'magnitude', 'the example', 'a simple sentence', 'a vector', 'the frequency', 'each word', 'The word', 'a value', 'it', 'the sentence bag', 'words', 'documents', 'different categories', 'It', 'attributes', 'that', 'N LP applications', 'sentiment analysis', 'three broad categories', 'text analysis', 'the classification', 'text', 'other classification systems', 'you', 'this course', 'text', 'the input', 'a process', 'that', 'features', 'you', 'the features', 'a machine learning', 'algorithm', 'that', 'a classifier model', 'the classification', 'many applications', 'text matching', 'example', 'autocorrect spelling', 'grammar checking', 'text matching', 'The algorithm', 'edit distance', 'the Levenstein distance', 'You', 'relationships', 'different words', 'phrases', 'the text', 'a process', 'co reference resolution', 'Several NLP systems', 'Python libraries', 'relationships', 'the biggest challenges', 'N LP', 'the context', 'the text', 'this example', 'a user', 'the term tablet', 'the word tablet', 'at least two distinct meanings', 'The search engine', 'which', 'the user', 'mind', 'Most search engines', 'the most commonly used context', 'the term', 'example', 'another term', 'medicine', 'the search', 'The process', 'entities', 'entity recognition', 'any R model', 'the following functions', 'it', 'noun phrases', 'dependency charts', 'part', 'speech tagging', 'it', 'phrases', 'a classification algorithm', 'word', 'VC', 'it', 'entities', 'a knowledge graph', 'an example', \"the entity's Titanic and North Atlantic\", 'the text', 'the named entities', 'you', 'a knowledge graph', 'a knowledge', 'graph', 'subject matter expertise', 'machine learning', 'the Amazon Recommendations engine', 'an example', 'a knowledge graph', 'the main points', 'this section', 'First N LP', 'machine learning', 'You', 'you', 'other modules', 'N LP', 'Some', 'the main use cases', 'NLP', 'search query analysis', 'human machine interaction', 'marketing', 'social research', 'N LP', 'human language', 'precision', 'we', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_44\n",
      "All the text from the video: Welcome back in this section. We'll review five managed machine learning services you can use for various use cases. These services simplify the process of creating a machine learning application. We'll start by looking at Amazon transcribe. You can use Amazon transcribe to recognize speech in audio files and produce a transcription. It can recognize specific voices in an audio file and you can create a customized vocabulary for terms that are specialized for a particular domain. You can also add a transcription service to your applications by integrating with web sockets, an internet protocol you can use for two way communication between an application and Amazon transcribe. Here are some of the more common use cases for Amazon transcribe. First medical professionals can record their notes and Amazon transcribe can capture their spoken notes as text. Also video production organizations can generate subtitles automatically from video. This could also be done in real time for a live feed to add closed captioning media companies can use Amazon transcribe to capture and label content. They can then feed the content into Amazon comprehend for further analysis. Last companies can record customer service or sales calls and transcribe them. They can analyze the results for training or for strategic opportunities. Amazon Poly can convert text into lifelike speech. You can input either plain text files or a file that's formatted in speech synthesis markup language or S S M L S S M L is a markup language used to provide special instructions for how speech should sound. For example, if you want to introduce a pause in the flow of speech, you can add an S S M L tag that instructs Amazon Polly to pause between two words. You can also output speech from Amazon Polly to P three V B and PC M audio stream formats. Amazon POLY is eligible for use with certain regulated workloads. For example, it's eligible for use with the US Health insurance Portability and Accountability Act of 1996 or IP A. Amazon Polly is also eligible for use with payment card industry, data security standard or PC I DS S. Here are some of the more common use cases for Amazon Polly. As a first example, major news companies are using Amazon Polly to generate vocal content directly from the written stories. It's also been embedded in mapping APIS so developers can add voice to their geo based applications. Language training companies have used Amazon Polly to create systems for learning a new language. Finally, animators have used it to add voices to their characters with Amazon translate, you can create multi language experiences in your applications, you can create systems that read documents in one language and then render or store them in another language. You can also use it as part of a document analysis system. Amazon translate is fully integrated with other machine learning services such as Amazon comprehend, Amazon transcribe and Amazon Poly. With this integration, you can extract named entities, sentiment and key phrases by integrating it with Amazon comprehend create Multilingual subtitles with Amazon transcribe and speak translated content with Amazon poly. Here are some of the more common use cases for Amazon translate. The first use case is building international websites. You can use Amazon translate to quickly globalize your websites. Amazon translate can also be used to develop Multilingual chat bots. Chat bots are used to create a more human like interface to applications with Amazon translate. You can create a chat bot that speaks multiple languages. Another use case is software localization localization is a major cost for all software aimed at a global audience. Amazon translate can decrease software development time and significantly reduce costs for localizing software. The final example use case is international media management companies that manage media for a global audience have used Amazon translate to reduce their costs for localization. Amazon comprehend implements many of the NLP techniques that we reviewed earlier in this module. You can extract key entities perform sentiment analysis and tag words with parts of speech. Here are some of the more common use cases for Amazon comprehend. The first example is analyzing legal and medical documents. Legal insurance and medical organizations have used Amazon comprehend to perform many of the N LP functions we reviewed in this module. Another use is for large scale mobile app analysis. Mobile app developers use Amazon comprehend to look for patterns of usage with their apps so they can design improvements. Financial fraud detection is another use case for Amazon comprehend, banking, financial and other institutions have used it to examine very large data sets of financial transactions to uncover fraud and look for patterns of illegal transactions. Finally, it can be used for content management, media and other content companies can use Amazon comprehend to tag content for analysis and management. With Amazon Lex, you can add a human language front end to your applications. Amazon Lex lets you use the same conversational engine that powers Amazon Alexa. You can automatically increase capacity for your Amazon Lex solution by creating Aws lambda functions that scale on demand. You can also store log files of the conversations for further analysis. Here are some of the more common use cases for Amazon Lex. The first use case is building front end interfaces for inventory management and sales. Voice interfaces are becoming more common. Companies have used Amazon Lex to add chat bots to their inventory and sales applications. Another use for Amazon Lex is creating customer service interfaces. Human like voice applications are quickly becoming the standard for many customer service applications. Amazon Lex can reduce the time it takes to develop these chat bots and increase their quality. Amazon Lex can also be used to develop interactive assistance by combining Amazon Lex with other ML services. Customers are creating more sophisticated assistance for many different industries. The final example use case is querying databases with a human like language. Amazon Lex has been combined with other AWS database services to create sophisticated data analysis applications with a human like language interface. Here are some of the main points you should take away from this module. First, Amazon transcribe can automatically convert spoken language to text. Amazon Polly can convert written text to spoken language. Amazon translate can create real time translation between languages. Amazon comprehend automates many of the NLP use cases reviewed in this module. And finally, Amazon Lex can create a human like interface to your applications. Thanks for watching. We'll see you in the next video.\n",
      "All the key phrases from the video: ['this section', 'We', 'five managed machine learning services', 'you', 'various use cases', 'These services', 'the process', 'a machine learning application', 'We', 'Amazon transcribe', 'You', 'Amazon', 'transcribe', 'speech', 'audio files', 'a transcription', 'It', 'specific voices', 'an audio file', 'you', 'a customized vocabulary', 'terms', 'that', 'a particular domain', 'You', 'a transcription service', 'your applications', 'web sockets', 'an internet protocol', 'you', 'two way communication', 'an application', 'some', 'the more common use cases', 'Amazon transcribe', 'First medical professionals', 'their notes', 'Amazon transcribe', 'their spoken notes', 'text', 'video production organizations', 'subtitles', 'video', 'This', 'real time', 'a live feed', 'closed captioning media companies', 'Amazon', 'transcribe', 'They', 'the content', 'Amazon', 'further analysis', 'Last companies', 'customer service', 'sales calls', 'them', 'They', 'the results', 'training', 'strategic opportunities', 'Amazon Poly', 'text', 'lifelike speech', 'You', 'either plain text files', 'a file', 'that', 'speech synthesis', 'markup language', 'S S M L S S M L', 'a markup language', 'special instructions', 'speech', 'example', 'you', 'a pause', 'the flow', 'speech', 'you', 'an S S M L tag', 'that', 'Amazon Polly', 'two words', 'You', 'speech', 'Amazon Polly', 'P three V B and PC M audio stream formats', 'Amazon POLY', 'use', 'certain regulated workloads', 'example', 'it', 'use', 'the US Health insurance Portability and Accountability Act', 'IP A. Amazon Polly', 'use', 'payment card industry', 'data security standard', 'PC', 'I', 'DS S.', 'some', 'the more common use cases', 'Amazon Polly', 'a first example', 'major news companies', 'Amazon Polly', 'vocal content', 'the written stories', 'It', 'mapping APIS', 'developers', 'voice', 'their geo based applications', 'Language training companies', 'Amazon Polly', 'systems', 'a new language', 'animators', 'it', 'voices', 'their characters', 'you', 'multi language experiences', 'your applications', 'you', 'systems', 'that', 'documents', 'one language', 'them', 'another language', 'You', 'it', 'part', 'a document analysis system', 'other machine learning services', 'Amazon', 'Amazon', 'Amazon Poly', 'this integration', 'you', 'named entities', 'sentiment', 'key phrases', 'it', 'Amazon', 'Multilingual subtitles', 'Amazon', 'translated content', 'some', 'the more common use cases', 'The first use case', 'international websites', 'You', 'Amazon', 'your websites', 'Amazon', 'Multilingual chat bots', 'Chat bots', 'interface', 'applications', 'You', 'a chat bot', 'that', 'multiple languages', 'Another use case', 'software localization localization', 'a major cost', 'all software', 'a global audience', 'software development time', 'costs', 'software', 'The final example use case', 'international media management companies', 'that', 'media', 'a global audience', 'Amazon', 'translate', 'their costs', 'localization', 'Amazon', 'the NLP techniques', 'that', 'we', 'this module', 'You', 'key entities', 'sentiment analysis', 'tag', 'words', 'parts', 'speech', 'some', 'the more common use cases', 'Amazon', 'The first example', 'legal and medical documents', 'Legal insurance', 'medical organizations', 'Amazon', 'the N LP functions', 'we', 'this module', 'Another use', 'large scale mobile app analysis', 'Mobile app developers', 'Amazon', 'patterns', 'usage', 'their apps', 'they', 'improvements', 'Financial fraud detection', 'another use case', 'banking, financial and other institutions', 'it', 'very large data sets', 'financial transactions', 'fraud', 'patterns', 'illegal transactions', 'it', 'content management', 'media', 'other content companies', 'Amazon', 'content', 'analysis', 'management', 'Amazon Lex', 'you', 'a human language', 'front end', 'your applications', 'Amazon Lex', 'you', 'the same conversational engine', 'that', 'Amazon Alexa', 'You', 'capacity', 'your Amazon Lex solution', 'Aws lambda functions', 'that', 'demand', 'You', 'log files', 'the conversations', 'further analysis', 'some', 'the more common use cases', 'Amazon Lex', 'The first use case', 'front end interfaces', 'inventory management', 'sales', 'Voice interfaces', 'Companies', 'Amazon Lex', 'bots', 'their inventory and sales applications', 'Another use', 'Amazon Lex', 'customer service interfaces', 'Human like voice applications', 'the standard', 'many customer service applications', 'Amazon Lex', 'the time', 'it', 'these chat bots', 'their quality', 'Amazon Lex', 'interactive assistance', 'Amazon Lex', 'other ML services', 'Customers', 'more sophisticated assistance', 'many different industries', 'The final example use case', 'databases', 'a human', 'language', 'Amazon Lex', 'other AWS database services', 'sophisticated data analysis applications', 'a human like language interface', 'some', 'the main points', 'you', 'this module', 'Amazon transcribe', 'spoken language', 'Amazon Polly', 'written text', 'spoken language', 'Amazon', 'real time translation', 'languages', 'Amazon', 'the NLP use cases', 'this module', 'Amazon Lex', 'a human like interface', 'your applications', 'Thanks', 'We', 'you', 'the next video']\n",
      "Video: demo1-job_name_1.1_45\n",
      "All the text from the video: Welcome back. It's now time to review the module and wrap it up. In summary. In this module, you learn how to describe the NLP use cases that are solved by using managed Amazon ML services and describe the managed M L services available for N LP. Good job. Thanks for watching. We'll see you in the next module.\n",
      "All the key phrases from the video: ['It', 'time', 'the module', 'it', 'summary', 'this module', 'you', 'the NLP use cases', 'that', 'managed Amazon ML services', 'the managed M L services', 'N LP', 'Good job', 'Thanks', 'We', 'you', 'the next module']\n",
      "Video: demo1-job_name_1.1_46\n",
      "All the text from the video: Welcome to module seven course. Wrap up. Congratulations on completing the Aws Academy Machine Learning course. We'll take a few minutes to review what you've learned and where you can go from here. We're going to start with a review of what you've learned in this course. You learned how to describe machine learning, implement a machine learning pipeline and use Amazon machine learning services for forecasting computer vision and natural language processing well done. Although this course isn't designed to prepare you to become certified for the Aws certified machine learning specialty. We'll review how you can continue to work towards that certification. Aws certification helps you build credibility and confidence by validating your cloud expertise with an industry recognized credential. It also helps organizations identify skilled professionals who can lead cloud initiatives by using Aws. You must earn a passing score by taking a proctored exam to earn an Aws certification. After receiving a passing score, you'll receive your certification credentials. Aws certification doesn't publish a list of all services or features that are covered in a certification exam. However, there's an exam guide for each exam and it lists the current topic areas and objectives covered in the exam exam guides can be found on the prepare for your Aws Certification exam web page. You'll be required to update your certification or recertify every three years. View the Aws Certification re certification page. For more details, the information on this slide is current as of June 2020. However, exams are frequently updated. Also the details regarding which exams are available and what topics are tested by each exam are subject to change. The Aws certified machine learning specialty means you can select and justify the appropriate machine learning approach for a given business problem. You can also identify appropriate Aws services to implement machine learning solutions. And finally, you can design and implement scalable cost, optimized, reliable and secure machine learning solutions. Before sitting for the Aws Certified Machine learning specialty exam, we recommend that you have the following knowledge and experience. First, you should have 1 to 2 years of experience developing architect or running M L or deep learning workloads on the Aws cloud. Your experience should include performing basic hyper parameter optimization and working with machine learning and deep learning frameworks. You should also be able to express the intuition behind basic ML algorithms. Finally, you should be able to follow best practices for model training in addition to best practices for deployment and operations. Thanks for watching and congratulations on completing the Aws Academy machine learning course.\n",
      "All the key phrases from the video: ['seven course', 'Congratulations', 'the Aws Academy Machine Learning course', 'We', 'a few minutes', 'what', 'you', 'you', 'We', 'a review', 'what', 'you', 'this course', 'You', 'machine learning', 'a machine learning pipeline', 'Amazon machine learning services', 'computer vision', 'natural language processing', 'this course', 'you', 'the Aws certified machine learning specialty', 'We', 'you', 'that certification', 'Aws certification', 'you', 'credibility', 'confidence', 'your cloud expertise', 'an industry', 'credential', 'It', 'organizations', 'skilled professionals', 'who', 'cloud initiatives', 'Aws', 'You', 'a passing score', 'a proctored exam', 'an Aws certification', 'a passing score', 'you', 'your certification credentials', 'Aws certification', 'a list', 'all services', 'features', 'that', 'a certification exam', 'an exam guide', 'each exam', 'it', 'the current topic areas', 'objectives', 'the exam exam guides', 'the prepare', 'your Aws Certification exam web page', 'You', 'your certification', 'every three years', 'the Aws Certification', 'certification page', 'more details', 'the information', 'this slide', 'June', 'exams', 'the details', 'which exams', 'what', 'topics', 'each exam', 'change', 'The Aws certified machine learning specialty', 'you', 'the appropriate machine learning approach', 'a given business problem', 'You', 'appropriate Aws services', 'machine learning solutions', 'you', 'scalable cost', 'optimized, reliable and secure machine learning solutions', 'the Aws Certified Machine', 'specialty exam', 'we', 'you', 'the following knowledge', 'experience', 'you', '1 to 2 years', 'architect', 'M L', 'deep learning workloads', 'the Aws cloud', 'Your experience', 'basic hyper parameter optimization', 'machine learning', 'deep learning frameworks', 'You', 'the intuition', 'basic ML algorithms', 'you', 'best practices', 'model training', 'addition', 'best practices', 'deployment', 'operations', 'Thanks', 'congratulations', 'the Aws Academy machine learning course']\n",
      "All the key phrases saved to all_keyphrases.json\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import json\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "all_keyphrases = {}\n",
    "with open('./combined_output.json') as f:\n",
    "    data = json.load(f)\n",
    "for video_id, video_text in data.items():\n",
    "    doc = nlp(video_text)\n",
    "    k_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
    "    print('Video:', video_id)\n",
    "    print('All the text from the video:', video_text)\n",
    "    print('All the key phrases from the video:', k_phrases)\n",
    "    all_keyphrases[video_id] = k_phrases\n",
    "with open('all_keyphrases.json', 'w') as f:\n",
    "    json.dump(all_keyphrases, f)\n",
    "print('All the key phrases saved to all_keyphrases.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "75922172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combine output saved to combine_output_1.json\n"
     ]
    }
   ],
   "source": [
    "combine_output_dict = {}\n",
    "with open('./combined_output.json', 'r') as f:\n",
    "    for line in f:\n",
    "        line_dict = json.loads(line)\n",
    "        combine_output_dict.update(line_dict)\n",
    "with open('combine_output_1.json', 'w') as f:\n",
    "    json.dump(combine_output_dict, f)\n",
    "print(f\"combine output saved to combine_output_1.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "64c1405c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/ec2-user/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dedb3ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1184bd8959748c39c25baa8b3b1f8d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='', description='Search:', layout=Layout(width='40%'), placeholder='Enter your searc"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "import string\n",
    "import pandas as pd\n",
    "from ipywidgets import widgets\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import clear_output\n",
    "def text_normalize(t):\n",
    "    t = t.lower()\n",
    "    t = t.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = t.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    t = \" \".join(filtered_tokens)\n",
    "    return t\n",
    "def keywords_match(query_words, transcript_words):\n",
    "    match_words = query_words.intersection(transcript_words)\n",
    "    s = len(match_words) / len(query_words)\n",
    "    return s\n",
    "def seek_videos(button):\n",
    "    clear_output(wait=True)\n",
    "    query_Str = inputbox.value\n",
    "    l = l_dropdown.value.lower()\n",
    "    minimum_score = score_slider.value\n",
    "    query = text_normalize(query_Str)\n",
    "    querywords = set(extract_key_phrases(query))\n",
    "    print(\"Query keywords are : {}\".format(querywords))\n",
    "    if len(querywords) == 0:\n",
    "        with output_box:\n",
    "            print(\"sorry!there are no any videos: {}\".format(query_Str))\n",
    "    else :\n",
    "        df = pd.read_csv(\"Intermediate_res.csv\")\n",
    "        df['transcription_key_tags'] = df['transcription_key_tags'].apply(lambda x: set(x.split()))\n",
    "        df['matching_score'] = df['transcription_key_tags'].apply(lambda x: keywords_match(querywords, x))\n",
    "        df = df.sort_values(by='matching_score', ascending=False)\n",
    "        videos = df[df['matching_score'] >= minimum_score].head(10)\n",
    "        with output_box:\n",
    "            display(videos[['path']])\n",
    "\n",
    "inputbox = widgets.Text(\n",
    "    placeholder='Enter your search query',\n",
    "    description='Search:',\n",
    "    layout=widgets.Layout(width='40%')\n",
    ")\n",
    "inputbox.style.background = '#F8F8F8'\n",
    "l_dropdown = widgets.Dropdown(\n",
    "    options=['English', 'Spanish', 'French','Gujarati','Hindi'],\n",
    "    value='English',\n",
    "    description='Language:',\n",
    "    layout=widgets.Layout(width='25%')\n",
    ")\n",
    "l_dropdown.style.background = '#F8F8F8'\n",
    "score_slider = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0,\n",
    "    max=1,\n",
    "    step=0.1,\n",
    "    description='Min Score:',\n",
    "    layout=widgets.Layout(width='50%')\n",
    ")\n",
    "score_slider.style.background = '#F8F8F8'\n",
    "search_button = widgets.Button(description='Search')\n",
    "search_button.style.button_color = 'green'\n",
    "output_box = widgets.Output()\n",
    "search_button.on_click(seek_videos)\n",
    "display(widgets.VBox([inputbox, l_dropdown, score_slider, search_button, output_box]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc19aaf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
